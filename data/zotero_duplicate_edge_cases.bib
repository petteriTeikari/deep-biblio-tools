@inproceedings{acemoglu2011,
  title = {Skills, Tasks and Technologies: {{Implications}} for Employment and Earnings},
  booktitle = {Handbook of {{Labor Economics}}},
  author = {Acemoglu, Daron and Autor, David},
  year = 2011,
  series = {Handbook of {{Labor Economics}}},
  pages = {1043--1171},
  doi = {10.1016/S0169-7218(11)02410-5},
  urldate = {2025-08-02}
}

@incollection{acemogluSkillsTasksTechnologies2011,
  title = {Skills, {{Tasks}} and {{Technologies}}: {{Implications}} for {{Employment}} and {{Earnings}}*},
  shorttitle = {Skills, {{Tasks}} and {{Technologies}}},
  booktitle = {Handbook of {{Labor Economics}}},
  author = {Acemoglu, Daron and Autor, David},
  editor = {Card, David and Ashenfelter, Orley},
  year = 2011,
  month = jan,
  volume = {4},
  pages = {1043--1171},
  publisher = {Elsevier},
  doi = {10.1016/S0169-7218(11)02410-5},
  urldate = {2025-07-30},
  abstract = {A central organizing framework of the voluminous recent literature studying changes in the returns to skills and the evolution of earnings inequality is what we refer to as the canonical model, which elegantly and powerfully operationalizes the supply and demand for skills by assuming two distinct skill groups that perform two different and imperfectly substitutable tasks or produce two imperfectly substitutable goods. Technology is assumed to take a factor-augmenting form, which, by complementing either high or low skill workers, can generate skill biased demand shifts. In this paper, we argue that despite its notable successes, the canonical model is largely silent on a number of central empirical developments of the last three decades, including: (1) significant declines in real wages of low skill workers, particularly low skill males; (2) non-monotone changes in wages at different parts of the earnings distribution during different decades; (3) broad-based increases in employment in high skill and low skill occupations relative to middle skilled occupations (i.e., job ``polarization''); (4) rapid diffusion of new technologies that directly substitute capital for labor in tasks previously performed by moderately skilled workers; and (5) expanding offshoring in opportunities, enabled by technology, which allow foreign labor to substitute for domestic workers specific tasks. Motivated by these patterns, we argue that it is valuable to consider a richer framework for analyzing how recent changes in the earnings and employment distribution in the United States and other advanced economies are shaped by the interactions among worker skills, job tasks, evolving technologies, and shifting trading opportunities. We propose a tractable task-based model in which the assignment of skills to tasks is endogenous and technical change may involve the substitution of machines for certain tasks previously performed by labor. We further consider how the evolution of technology in this task-based setting may be endogenized. We show how such a framework can be used to interpret several central recent trends, and we also suggest further directions for empirical exploration.},
  keywords = {College premium,Directed technical change,Earnings inequality,Occupations,Returns to schooling,Skill biased technical change,Skill premium,Tasks,Wage inequality}
}

@article{acsr2025,
  title = {Recon-{{3D}} Measurement Accuracy Study for Small Scenes},
  author = {{ACSR}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{acsrRecon3DMeasurementAccuracy2025,
  title = {Recon-{{3D}} Measurement Accuracy Study for Small Scenes},
  author = {{ACSR}},
  year = 2025,
  urldate = {2025-07-31}
}

@incollection{adebayoSanityChecksSaliency2018,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = 2018,
  pages = {9525--9536},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-22},
  file = {/home/petteri/Zotero/storage/5X6PG45C/Adebayo et al. - 2018 - Sanity Checks for Saliency Maps.pdf}
}

@article{adebayoSanityChecksSaliency2018a,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  year = 2018,
  month = oct,
  journal = {arXiv:1810.03292 [cs, stat]},
  eprint = {1810.03292},
  primaryclass = {cs, stat},
  urldate = {2020-07-30},
  abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@techreport{alexandrov2023,
  title = {Reengineering the {{Appraisal Process Better Leveraging Both Automated Valuation Models}} and {{Manual Appraisals}}},
  author = {Alexandrov, Alexei and Goodman, Laurie and Neal, Michael},
  year = 2023,
  month = jan,
  institution = {Urban Institute},
  urldate = {2025-08-02}
}

@misc{alexandrovReengineeringAppraisalProcess2023,
  title = {Reengineering the {{Appraisal Process Better Leveraging Both Automated Valuation Models}} and {{Manual Appraisals}}},
  author = {Alexandrov, Alexei and Goodman, Laurie and Neal, Michael},
  year = 2023,
  urldate = {2025-07-31}
}

@article{amershi2019,
  title = {Guidelines for Human-{{AI}} Interaction},
  author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and {Kikin-Gil}, Ruth and Horvitz, Eric},
  year = 2019,
  journal = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  series = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  pages = {1--13},
  doi = {10.1145/3290605.3300233},
  urldate = {2025-08-02}
}

@inproceedings{amershiGuidelinesHumanAIInteraction2019,
  title = {Guidelines for Human-{{AI}} Interaction},
  author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and {Kikin-Gil}, Ruth and Horvitz, Eric},
  year = 2019,
  series = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  pages = {1--13},
  doi = {10.1145/3290605.3300233},
  urldate = {2025-07-31}
}

@misc{andriushchenkoWhyWeNeed2023,
  title = {Why {{Do We Need Weight Decay}} in {{Modern Deep Learning}}?},
  author = {Andriushchenko, Maksym and D'Angelo, Francesco and Varre, Aditya and Flammarion, Nicolas},
  year = 2023,
  month = oct,
  journal = {arXiv.org},
  urldate = {2024-10-06},
  abstract = {Weight decay is a broadly used technique for training state-of-the-art deep networks, including large language models. Despite its widespread usage, its role remains poorly understood. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For overparameterized deep networks, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for underparameterized large language models trained with nearly online SGD, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss. Moreover, we show that weight decay also prevents sudden loss divergences for bfloat16 mixed-precision training which is a crucial tool for LLM training. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. Our code is available at https://github.com/tml-epfl/why-weight-decay.},
  howpublished = {https://arxiv.org/abs/2310.04415v1},
  langid = {english}
}

@misc{asdrubali2015,
  title = {Thermal and Optical Characterization of Natural and Artificial Marble for Roof and External Floor Installations},
  author = {Asdrubali, Francesco and others},
  year = 2015,
  urldate = {2025-08-02}
}

@article{asdrubaliThermalOpticalCharacterization2015,
  title = {Thermal and Optical Characterization of Natural and Artificial Marble for Roof and External Floor Installations},
  author = {Asdrubali, F and Baldinelli, G and Bianchi, F and Presciutti, A and Rossi, F and Schiavoni, S},
  year = 2015,
  month = oct,
  journal = {Journal of Physics: Conference Series},
  volume = {655},
  number = {1},
  pages = {012017},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/655/1/012017},
  urldate = {2025-08-03},
  abstract = {Some types of buildings need to use certain materials for aesthetic reasons, like churches or mosques. Marble is one of the most common materials usually installed on roofs and floors. The measurement of the thermal and optical characteristics can be useful to understand its behaviour when it is subjected to thermal loads such as solar radiation or high temperature winds. The paper shows a comparison study between natural and artificial types of marble, to investigate the thermal characteristics both in steady-state and transient conditions. Optical properties and surface emissivity were evaluated, in order to calculate the Solar Reflectance Index (SRI); the specific heat, the thermal conductivity and the density were measured to define the thermophysical properties useful for the dynamic analysis. Finally, a test bench was created to check the marble behaviour under known artificial irradiation.},
  langid = {english}
}

@book{batool2025,
  title = {{{ImpedanceGPT}}: {{VLM-driven Impedance Control}} of {{Swarm}} of {{Mini-drones}} for {{Intelligent Navigation}} in {{Dynamic Environment}}},
  shorttitle = {{{ImpedanceGPT}}},
  author = {Batool, Faryal and Zafar, Malaika and Yaqoot, Yasheerah and Khan, Roohan Ahmed and Khan, Muhammad Haris and Fedoseev, Aleksey and Tsetserukou, Dzmitry},
  year = 2025,
  month = mar,
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.02723},
  urldate = {2025-08-01},
  abstract = {Swarm robotics plays a crucial role in enabling autonomous operations in dynamic and unpredictable environments. However, a major challenge remains ensuring safe and efficient navigation in environments filled with both dynamic alive (e.g., humans) and dynamic inanimate (e.g., non-living objects) obstacles. In this paper, we propose ImpedanceGPT, a novel system that combines a Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to enable real-time reasoning for adaptive navigation of mini-drone swarms in complex environments. The key innovation of ImpedanceGPT lies in the integration of VLM and RAG, which provides the drones with enhanced semantic understanding of their surroundings. This enables the system to dynamically adjust impedance control parameters in response to obstacle types and environmental conditions. Our approach not only ensures safe and precise navigation but also improves coordination between drones in the swarm. Experimental evaluations demonstrate the effectiveness of the system. The VLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 \% under optimal lighting. In static environments, drones navigated dynamic inanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation around humans. In dynamic environments, speed adjusted to 1.0 m/s near hard obstacles, while reducing to 0.6 m/s with higher deflection to safely avoid moving humans.},
  keywords = {Computer Science - Robotics}
}

@misc{batoolImpedanceGPTVLMdrivenImpedance2025,
  title = {{{ImpedanceGPT}}: {{VLM-driven Impedance Control}} of {{Swarm}} of {{Mini-drones}} for {{Intelligent Navigation}} in {{Dynamic Environment}}},
  shorttitle = {{{ImpedanceGPT}}},
  author = {Batool, Faryal and Zafar, Malaika and Yaqoot, Yasheerah and Khan, Roohan Ahmed and Khan, Muhammad Haris and Fedoseev, Aleksey and Tsetserukou, Dzmitry},
  year = 2025,
  month = mar,
  number = {arXiv:2503.02723},
  eprint = {2503.02723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.02723},
  urldate = {2025-08-01},
  abstract = {Swarm robotics plays a crucial role in enabling autonomous operations in dynamic and unpredictable environments. However, a major challenge remains ensuring safe and efficient navigation in environments filled with both dynamic alive (e.g., humans) and dynamic inanimate (e.g., non-living objects) obstacles. In this paper, we propose ImpedanceGPT, a novel system that combines a Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to enable real-time reasoning for adaptive navigation of mini-drone swarms in complex environments. The key innovation of ImpedanceGPT lies in the integration of VLM and RAG, which provides the drones with enhanced semantic understanding of their surroundings. This enables the system to dynamically adjust impedance control parameters in response to obstacle types and environmental conditions. Our approach not only ensures safe and precise navigation but also improves coordination between drones in the swarm. Experimental evaluations demonstrate the effectiveness of the system. The VLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 \% under optimal lighting. In static environments, drones navigated dynamic inanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation around humans. In dynamic environments, speed adjusted to 1.0 m/s near hard obstacles, while reducing to 0.6 m/s with higher deflection to safely avoid moving humans.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics}
}

@article{bechler-speicher2024,
  title = {The Intelligible and Effective Graph Neural Additive Networks},
  author = {{Bechler-Speicher}, Maya and Globerson, Amir and {Gilad-Bachrach}, Ran},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{bechler-speicherIntelligibleEffectiveGraph2024,
  title = {The Intelligible and Effective Graph Neural Additive Networks},
  author = {{Bechler-Speicher}, Maya and Globerson, Amir and {Gilad-Bachrach}, Ran},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{bechlerspeicher2024,
  title = {The Intelligible and Effective Graph Neural Additive Networks},
  author = {{Maya Bechler-Speicher}, Amir Globerson and {Gilad-Bachrach}, Ran},
  year = 2024,
  eprint = {2406.01317},
  archiveprefix = {arXiv},
  arxivid = {2406.01317}
}

@misc{behrouzChimeraEffectivelyModeling2024,
  title = {Chimera: {{Effectively Modeling Multivariate Time Series}} with 2-{{Dimensional State Space Models}}},
  shorttitle = {Chimera},
  author = {Behrouz, Ali and Santacatterina, Michele and Zabih, Ramin},
  year = 2024,
  month = jun,
  number = {arXiv:2406.04320},
  eprint = {2406.04320},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04320},
  urldate = {2024-06-28},
  abstract = {Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets. Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their simplicity and expressive power to represent linear dependencies. They, however, have fundamentally limited expressive power to capture non-linear dependencies, are slow in practice, and fail to model the inter-variate information flow. Despite recent attempts to improve the expressive power of SSMs by using deep structured SSMs, the existing methods are either limited to univariate time series, fail to model complex patterns (e.g., seasonal patterns), fail to dynamically model the dependencies of variate and time dimensions, and/or are input-independent. We present Chimera that uses two input-dependent 2-D SSM heads with different discretization processes to learn long-term progression and seasonal patterns. To improve the efficiency of complex 2D recurrence, we present a fast training using a new 2-dimensional parallel selective scan. We further present and discuss 2-dimensional Mamba and Mamba-2 as the spacial cases of our 2D SSM. Our experimental evaluation shows the superior performance of Chimera on extensive and diverse benchmarks, including ECG and speech time series classification, long-term and short-term time series forecasting, and time series anomaly detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{behrouzChimeraEffectivelyModeling2024a,
  title = {Chimera: {{Effectively Modeling Multivariate Time Series}} with 2-{{Dimensional State Space Models}}},
  shorttitle = {Chimera},
  booktitle = {The {{Thirty-eighth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Behrouz, Ali and Santacatterina, Michele and Zabih, Ramin},
  year = 2024,
  month = nov,
  urldate = {2024-12-09},
  abstract = {Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets. It, however, is challenging as it requires methods to (1) have high expressive power of representing complicated dependencies along the time axis to capture both long-term progression and seasonal patterns, (2) capture the inter-variate dependencies when it is informative, (3) dynamically model the dependencies of variate and time dimensions, and (4) have efficient training and inference for very long sequences. Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their simplicity and expressive power to represent linear dependencies. They, however, have fundamentally limited expressive power to capture non-linear dependencies, are slow in practice, and fail to model the inter-variate information flow. Despite recent attempts to improve the expressive power of SSMs by using deep structured SSMs, the existing methods are either limited to univariate time series, fail to model complex patterns (e.g., seasonal patterns), fail to dynamically model the dependencies of variate and time dimensions, and/or are input-independent. We present Chimera, an expressive variation of the 2-dimensional SSMs with careful design of parameters to maintain high expressive power while keeping the training complexity linear. Using two SSM heads with different discretization processes and input-dependent parameters, Chimera is provably able to learn long-term progression, seasonal patterns, and desirable dynamic autoregressive processes. To improve the efficiency of complex 2D recurrence, we present a fast training using a new 2-dimensional parallel selective scan. Our experimental evaluation shows the superior performance of Chimera on extensive and diverse benchmarks, including ECG and speech time series classification, long-term and short-term time series forecasting, and time series anomaly detection.},
  langid = {english}
}

@article{brynjolfssonProductivityJCurveHow2021,
  title = {The {{Productivity J-Curve}}: {{How Intangibles Complement General Purpose Technologies}}},
  shorttitle = {The {{Productivity J-Curve}}},
  author = {Brynjolfsson, Erik and Rock, Daniel and Syverson, Chad},
  year = 2021,
  month = jan,
  journal = {American Economic Journal: Macroeconomics},
  volume = {13},
  number = {1},
  pages = {333--372},
  issn = {1945-7707},
  doi = {10.1257/mac.20180386},
  urldate = {2025-08-03},
  abstract = {General purpose technologies (GPTs) like AI enable and require significant complementary investments. These investments are often intangible and poorly measured in national accounts. We develop a model that shows how this can lead to underestimation of productivity growth in a new GPTs early years and, later, when the benefits of intangible investments are harvested, productivity growth overestimation. We call this phenomenon the Productivity J-curve. We apply our method to US data and find that adjusting for intangibles related to computer hardware and software yields a TFP level that is 15.9 percent higher than official measures by the end of 2017.},
  langid = {english},
  keywords = {Capacity Macroeconomics: Production Capital Budgeting,Capacity Microelectronics,Capital,Capital Budgeting; Fixed Investment and Inventory Studies; Capacity,Communications Equipment Information and Internet Services,Computer Software,Computers,Fixed Investment and Inventory Studies,Information and Internet Services; Computer Software,Intangible Capital,Investment,Investment; Capital; Intangible Capital; Capacity,Macroeconomics: Production,Microelectronics; Computers; Communications Equipment}
}

@misc{brynjolfssonProductivityJCurveHow2021,
  title = {The {{Productivity J-Curve}}: {{How Intangibles Complement General Purpose Technologies}}},
  shorttitle = {The {{Productivity J-Curve}}},
  author = {Brynjolfsson, Erik and Rock, Daniel and Syverson, Chad},
  year = 2021,
  month = jan,
  volume = {13},
  number = {1},
  pages = {333--372},
  issn = {1945-7707},
  doi = {10.1257/mac.20180386},
  urldate = {2025-08-03},
  abstract = {General purpose technologies (GPTs) like AI enable and require significant complementary investments. These investments are often intangible and poorly measured in national accounts. We develop a model that shows how this can lead to underestimation of productivity growth in a new GPTs early years and, later, when the benefits of intangible investments are harvested, productivity growth overestimation. We call this phenomenon the Productivity J-curve. We apply our method to US data and find that adjusting for intangibles related to computer hardware and software yields a TFP level that is 15.9 percent higher than official measures by the end of 2017.},
  langid = {english},
  keywords = {Capacity,Capacity Macroeconomics: Production Capital Budgeting,Capacity Microelectronics,Capital,Capital Budgeting,Communications Equipment,Communications Equipment Information and Internet Services,Computer Software,Computers,Fixed Investment and Inventory Studies,Information and Internet Services,Intangible Capital,Investment,Macroeconomics: Production,Microelectronics}
}

@misc{c2025,
  title = {{{FHFA}} Expands Appraisal Alternatives for New Purchases in 2025},
  author = {(c), MtgeFi},
  year = 2025,
  urldate = {2025-07-31}
}

@article{c2025,
  title = {{{FHFA}} Expands Appraisal Alternatives for New Purchases in 2025},
  author = {(c), MtgeFi},
  year = 2025,
  month = oct,
  journal = {Web page},
  urldate = {2025-08-02},
  abstract = {Inspection based waivers become the first appraisal alternatives to be eligible for all CLTVs, up to the GSE program limits.},
  langid = {american}
}

@article{canvas2025,
  title = {How Canvas Works: {{Scan-to-CAD}} in Minutes with {{iOS}} Devices},
  author = {{Canvas}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{canvasHowCanvasWorks2025,
  title = {How Canvas Works: {{Scan-to-CAD}} in Minutes with {{iOS}} Devices},
  author = {{Canvas}},
  year = 2025,
  urldate = {2025-07-31}
}

@misc{cfpb2022,
  title = {{{CFPB}} Approves Rule to Ensure Accuracy and Accountability in the Use of {{AI}} and Algorithms in Home Appraisals {\textbar} {{Consumer Financial Protection Bureau}}},
  author = {{CFPB}},
  year = 2022,
  journal = {Blog post},
  urldate = {2025-08-02}
}

@misc{cfpbCFPBApprovesRule2022,
  title = {{{CFPB}} Approves Rule to Ensure Accuracy and Accountability in the Use of {{AI}} and Algorithms in Home Appraisals {\textbar} {{Consumer Financial Protection Bureau}}},
  author = {{CFPB}},
  year = 2022,
  urldate = {2025-07-31}
}

@misc{chenUCRTimeSeries2015,
  title = {The {{UCR Time Series Classification Archive}}},
  author = {Chen, Yanping and Keogh, Eamonn and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo},
  year = 2015,
  month = jul,
  howpublished = {http://www.cs.ucr.edu/{\textasciitilde}eamonn/time\_series\_data/}
}

@article{chenUcrTimeSeries2015a,
  title = {The Ucr Time Series Classification Archive},
  author = {Chen, Yanping and Keogh, Eamonn and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo},
  year = 2015,
  journal = {URL www. cs. ucr. edu/{\textbackslash}textasciitilde eamonn/time\_series\_data}
}

@article{conti2025,
  title = {{{ToF-splatting}}: {{Dense SLAM}} Using Sparse Time-of-Flight Depth and Multi-Frame Integration},
  author = {Conti, Andrea and Poggi, Matteo and Cambareri, Valerio and Oswald, Martin R. and Mattoccia, Stefano},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@misc{contiToFSplattingDenseSLAM2025,
  title = {{{ToF-Splatting}}: {{Dense SLAM}} Using {{Sparse Time-of-Flight Depth}} and {{Multi-Frame Integration}}},
  shorttitle = {{{ToF-Splatting}}},
  author = {Conti, Andrea and Poggi, Matteo and Cambareri, Valerio and Oswald, Martin R. and Mattoccia, Stefano},
  year = 2025,
  month = apr,
  number = {arXiv:2504.16545},
  eprint = {2504.16545},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16545},
  urldate = {2025-08-03},
  abstract = {Time-of-Flight (ToF) sensors provide efficient active depth sensing at relatively low power budgets; among such designs, only very sparse measurements from low-resolution sensors are considered to meet the increasingly limited power constraints of mobile and AR/VR devices. However, such extreme sparsity levels limit the seamless usage of ToF depth in SLAM. In this work, we propose ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for using effectively very sparse ToF input data. Our approach improves upon the state of the art by introducing a multi-frame integration module, which produces dense depth maps by merging cues from extremely sparse ToF depth, monocular color, and multi-view geometry. Extensive experiments on both synthetic and real sparse ToF datasets demonstrate the viability of our approach, as it achieves state-of-the-art tracking and mapping performances on reference datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{cubicasa2025,
  title = {{{CubiCasa}} {\textbar} {{Create}} a Floor Plan in under 5 Minutes},
  author = {{CubiCasa}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{cubicasaCubiCasaCreateFloor2025,
  title = {{{CubiCasa}} {\textbar} {{Create}} a Floor Plan in under 5 Minutes},
  author = {{CubiCasa}},
  year = 2025,
  urldate = {2025-07-31}
}

@misc{dangeloWhyWeNeed2024,
  title = {Why {{Do We Need Weight Decay}} in {{Modern Deep Learning}}?},
  author = {D'Angelo, Francesco and Andriushchenko, Maksym and Varre, Aditya and Flammarion, Nicolas},
  year = 2024,
  month = nov,
  number = {arXiv:2310.04415},
  eprint = {2310.04415},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.04415},
  urldate = {2024-11-21},
  abstract = {Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. The code is available at https://github.com/tml-epfl/why-weight-decay},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{docusketch2025,
  title = {How to Write an Xactimate Estimate {\textbar} Introductory Guide {\textbar} Docusketch},
  author = {{DocuSketch}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{docusketchHowWriteXactimate2025,
  title = {How to Write an Xactimate Estimate {\textbar} Introductory Guide {\textbar} Docusketch},
  author = {{DocuSketch}},
  year = 2025,
  urldate = {2025-07-31}
}

@misc{durante2024,
  title = {An Interactive Agent Foundation Model},
  author = {Zane Durante, Bidipta Sarkar, Ran Gong and Huang, Qiuyuan},
  year = 2024,
  eprint = {2402.05929},
  archiveprefix = {arXiv},
  arxivid = {2402.05929}
}

@article{durante2024,
  title = {An Interactive Agent Foundation Model},
  author = {Durante, Zane and Sarkar, Bidipta and Gong, Ran and Taori, Rohan and Noda, Yusuke and Tang, Paul and Adeli, Ehsan and Lakshmikanth, Shrinidhi Kowshika and Schulman, Kevin and Milstein, Arnold and Terzopoulos, Demetri and Famoti, Ade and Kuno, Noboru and Llorens, Ashley and Vo, Hoi and Ikeuchi, Katsu and {Fei-Fei}, Li and Gao, Jianfeng and Wake, Naoki and Huang, Qiuyuan},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{duranteInteractiveAgentFoundation2024,
  title = {An Interactive Agent Foundation Model},
  author = {Durante, Zane and Sarkar, Bidipta and Gong, Ran and Taori, Rohan and Noda, Yusuke and Tang, Paul and Adeli, Ehsan and Lakshmikanth, Shrinidhi Kowshika and Schulman, Kevin and Milstein, Arnold and Terzopoulos, Demetri and Famoti, Ade and Kuno, Noboru and Llorens, Ashley and Vo, Hoi and Ikeuchi, Katsu and {Fei-Fei}, Li and Gao, Jianfeng and Wake, Naoki and Huang, Qiuyuan},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@article{dwork2012,
  title = {Fairness through Awareness},
  author = {Cynthia Dwork, Moritz Hardt, Toniann Pitassi and Zemel, Richard},
  year = 2012,
  journal = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  pages = {214--226},
  publisher = {ACM},
  doi = {10.1145/2090236.2090255}
}

@article{dwork2012,
  title = {Fairness through Awareness},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = 2012,
  journal = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  series = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  pages = {214--226},
  doi = {10.1145/2090236.2090255},
  urldate = {2025-08-02}
}

@inproceedings{dworkFairnessAwareness2012,
  title = {Fairness through Awareness},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = 2012,
  series = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  pages = {214--226},
  doi = {10.1145/2090236.2090255},
  urldate = {2025-07-31}
}

@misc{executivestudentappraiserstateMLSConsolidationResources2021,
  title = {{{MLS}} Consolidation Resources},
  author = {ExecutiveStudentAppraiserState, RoleBrokerAssociation and Sales, SpecialtyCommercialGlobalSenior MarketShort and ExecutiveStudentAppraiserState, RoleBrokerAssociation},
  year = 2021,
  urldate = {2025-07-31}
}

@article{faigle2024,
  title = {Polynomial Representation of {{TU-games}}},
  author = {Faigle, Ulrich and Grabisch, Michel},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@misc{faiglePolynomialRepresentationTUgames2024,
  title = {Polynomial Representation of {{TU-games}}},
  author = {Faigle, Ulrich and Grabisch, Michel},
  year = 2024,
  eprint = {2401.12741},
  archiveprefix = {arXiv},
  arxivid = {2401.12741}
}

@misc{fan2025,
  title = {{{InstantSplat}}: {{Sparse-view}} Gaussian Splatting in Seconds},
  author = {Zhiwen Fan, Wenyan Cong, Kairun Wen and Wang, Yue},
  year = 2025,
  eprint = {2403.20309},
  archiveprefix = {arXiv},
  arxivid = {2403.20309}
}

@article{fan2025,
  title = {{{InstantSplat}}: {{Sparse-view}} Gaussian Splatting in Seconds},
  author = {Fan, Zhiwen and Cong, Wenyan and Wen, Kairun and Wang, Kevin and Zhang, Jian and Ding, Xinghao and Xu, Danfei and Ivanovic, Boris and Pavone, Marco and Pavlakos, Georgios and Wang, Zhangyang and Wang, Yue},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{fanInstantSplatSparseviewGaussian2025,
  title = {{{InstantSplat}}: {{Sparse-view}} Gaussian Splatting in Seconds},
  author = {Fan, Zhiwen and Cong, Wenyan and Wen, Kairun and Wang, Kevin and Zhang, Jian and Ding, Xinghao and Xu, Danfei and Ivanovic, Boris and Pavone, Marco and Pavlakos, Georgios and Wang, Zhangyang and Wang, Yue},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@article{feiginDetectingMuscleActivation2019,
  title = {Detecting Muscle Activation Using Ultrasound Speed of Sound Inversion with Deep Learning},
  author = {Feigin, Micha and Zwecker, Manuel and Freedman, Daniel and Anthony, Brian W.},
  year = 2019,
  month = oct,
  journal = {arXiv:1910.09046 [physics, q-bio]},
  eprint = {1910.09046},
  primaryclass = {physics, q-bio},
  urldate = {2021-01-28},
  abstract = {Functional muscle imaging is essential for diagnostics of a multitude of musculoskeletal afflictions such as degenerative muscle diseases, muscle injuries, muscle atrophy, and neurological related issues such as spasticity. However, there is currently no solution, imaging or otherwise, capable of providing a map of active muscles over a large field of view in dynamic scenarios. In this work, we look at the feasibility of longitudinal sound speed measurements to the task of dynamic muscle imaging of contraction or activation. We perform the assessment using a deep learning network applied to pre-beamformed ultrasound channel data for sound speed inversion. Preliminary results show that dynamic muscle contraction can be detected in the calf and that this contraction can be positively assigned to the operating muscles. Potential frame rates in the hundreds to thousands of frames per second are necessary to accomplish this.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Physics - Medical Physics,Quantitative Biology - Tissues and Organs}
}

@inproceedings{feiginDetectingMuscleActivation2020,
  title = {Detecting Muscle Activation Using Ultrasound Speed of Sound Inversion with Deep Learning},
  booktitle = {2020 42nd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine Biology Society}} ({{EMBC}})},
  author = {Feigin, Micha and Zwecker, Manuel and Freedman, Daniel and Anthony, Brian W.},
  year = 2020,
  month = jul,
  pages = {2092--2095},
  issn = {2694-0604},
  doi = {10.1109/EMBC44109.2020.9175237},
  abstract = {Functional muscle imaging is essential for diagnostics of a multitude of musculoskeletal afflictions such as degenerative muscle diseases, muscle injuries, muscle atrophy, and neurological related issues such as spasticity. However, there is currently no solution, imaging or otherwise, capable of providing a map of active muscles over a large field of view in dynamic scenarios.In this work, we look at the feasibility of applying longitudinal sound speed measurements to the task of dynamic muscle imaging of contraction or activation. We perform the assessment using a deep learning network applied to prebeam formed ultrasound channel data for sound speed inversion.Preliminary results show that dynamic muscle contraction can be detected in the calf and that this contraction can be positively assigned to the operating muscles. Potential frame rates in the hundreds to thousands of frames per second are necessary to accomplish this.},
  keywords = {Dynamics,Force,Imaging,Machine learning,Muscles,Training,Ultrasonic imaging},
  file = {/home/petteri/Zotero/storage/B6Q6MPLY/9175237.html}
}

@article{fhfa2024,
  title = {Standardizing Mortgage Data through the Uniform Mortgage Data Program {\textbar} {{FHFA}}},
  author = {{FHFA}},
  year = 2024,
  journal = {Blog post},
  urldate = {2025-08-02}
}

@misc{fhfaStandardizingMortgageData2024,
  title = {Standardizing Mortgage Data through the Uniform Mortgage Data Program {\textbar} {{FHFA}}},
  author = {{FHFA}},
  year = 2024,
  urldate = {2025-07-31}
}

@article{foundation2024,
  title = {The Appraisal Foundation},
  author = {Foundation, Appraisal},
  year = 2024,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{foundationAppraisalFoundation2024,
  title = {The Appraisal Foundation},
  author = {Foundation, Appraisal},
  year = 2024,
  urldate = {2025-07-31}
}

@article{gal2016,
  title = {Dropout as a Bayesian Approximation: {{Representing}} Model Uncertainty in Deep Learning},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = 2016,
  month = oct,
  journal = {arXiv},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02142},
  urldate = {2025-08-02},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{galDropoutBayesianApproximation2015,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = 2015,
  month = jun,
  journal = {arXiv:1506.02142 [cs, stat]},
  eprint = {1506.02142},
  primaryclass = {cs, stat},
  urldate = {2016-01-05},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Learning,Statistics - Machine Learning}
}

@misc{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = 2016,
  month = oct,
  number = {arXiv:1506.02142},
  eprint = {1506.02142},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02142},
  urldate = {2025-08-01},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@techreport{geifman2017,
  title = {Selective Classification for Deep Neural Networks},
  author = {{Geifman} and {El-Yaniv}},
  year = 2017,
  urldate = {2025-07-31}
}

@misc{geifman2017,
  title = {Selective Classification for Deep Neural Networks},
  author = {{Geifman} and {El-Yaniv}},
  year = 2017,
  urldate = {2025-08-02}
}

@article{geifmanSelectiveClassificationDeep2017,
  title = {Selective {{Classification}} for {{Deep Neural Networks}}},
  author = {Geifman, Yonatan and {El-Yaniv}, Ran},
  year = 2017,
  month = may,
  journal = {arXiv:1705.08500 [cs.LG]},
  eprint = {1705.08500},
  primaryclass = {cs.LG},
  urldate = {2019-03-13},
  abstract = {Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2\% error in top-5 ImageNet classification can be guaranteed with probability 99.9\%, and almost 60\% test coverage.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/petteri/Zotero/storage/E8M46VPP/Geifman and El-Yaniv - 2017 - Selective Classification for Deep Neural Networks.pdf}
}

@misc{geifmanSelectiveClassificationDeep2017a,
  title = {Selective {{Classification}} for {{Deep Neural Networks}}},
  author = {Geifman, Yonatan and {El-Yaniv}, Ran},
  year = 2017,
  month = jun,
  number = {arXiv:1705.08500},
  eprint = {1705.08500},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.08500},
  urldate = {2025-08-04},
  abstract = {Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2\% error in top-5 ImageNet classification can be guaranteed with probability 99.9\%, and almost 60\% test coverage.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{gumeliROCARobustCAD,
  title = {{{ROCA}}: {{Robust CAD Model Retrieval}} and {{Alignment}} from a {{Single Image}}},
  shorttitle = {{{ROCA}}},
  author = {G{\"u}meli, Can and Dai, Angela},
  urldate = {2025-10-09}
}

@inproceedings{gumeliROCARobustCAD2022,
  title = {{{ROCA}}: {{Robust CAD Model Retrieval}} and {{Alignment}} from a {{Single Image}}},
  shorttitle = {{{ROCA}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {G{\"u}meli, Can and Dai, Angela and Nie{\ss}ner, Matthias},
  year = 2022,
  month = jun,
  eprint = {2112.01988},
  primaryclass = {cs},
  pages = {4012--4021},
  doi = {10.1109/CVPR52688.2022.00399},
  urldate = {2025-10-26},
  abstract = {We present ROCA, a novel end-to-end approach that retrieves and aligns 3D CAD models from a shape database to a single input image. This enables 3D perception of an observed scene from a 2D RGB observation, characterized as a lightweight, compact, clean CAD representation. Core to our approach is our differentiable alignment optimization based on dense 2D-3D object correspondences and Procrustes alignment. ROCA can thus provide a robust CAD alignment while simultaneously informing CAD retrieval by leveraging the 2D-3D correspondences to learn geometrically similar CAD models. Experiments on challenging, real-world imagery from ScanNet show that ROCA significantly improves on state of the art, from 9.5\% to 17.6\% in retrieval-aware CAD alignment accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/petteri/Zotero/storage/QJ8H5BPY/GÃ¼meli et al. - 2022 - ROCA Robust CAD Model Retrieval and Alignment from a Single Image.pdf;/home/petteri/Zotero/storage/W6JR9ZWR/2112.html}
}

@article{gupta2018,
  title = {Compressive Light Field Reconstructions Using Deep Learning},
  author = {Gupta, Mayank and Jauhari, Arjun and Kulkarni, Kuldeep and Jayasuriya, Suren and Molnar, Alyosha and Turaga, Pavan},
  year = 2018,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@inproceedings{guptaCompressiveLightField2017,
  title = {Compressive {{Light Field Reconstructions Using Deep Learning}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Gupta, M. and Jauhari, A. and Kulkarni, K. and Jayasuriya, S. and Molnar, A. and Turaga, P.},
  year = 2017,
  month = jul,
  pages = {1277--1286},
  doi = {10.1109/CVPRW.2017.168},
  abstract = {Light field imaging is limited in its computational processing demands of high sampling for both spatial and angular dimensions. Single-shot light field cameras sacrifice spatial resolution to sample angular viewpoints, typically by multiplexing incoming rays onto a 2D sensor array. While this resolution can be recovered using compressive sensing, these iterative solutions are slow in processing a light field. We present a deep learning approach using a new, two branch network architecture, consisting jointly of an autoencoder and a 4D CNN, to recover a high resolution 4D light field from a single coded 2D image. This network decreases reconstruction time significantly while achieving average PSNR values of 26-32 dB on a variety of light fields. In particular, reconstruction time is decreased from 35 minutes to 6.7 minutes as compared to the dictionary method for equivalent visual quality. These reconstructions are performed at small sampling/compression ratios as low as 8\%, allowing for cheaper coded light field cameras. We test our network reconstructions on synthetic light fields, simulated coded measurements of real light fields captured from a Lytro Illum camera, and real coded images from a custom CMOS diffractive light field camera. The combination of compressive light field capture with deep learning allows the potential for real-time light field video acquisition systems in the future.},
  keywords = {2D sensor array,4D CNN,angular dimensions,autoencoder,average PSNR values,cameras,Cameras,CMOS diffractive light field camera,coded images,coded light field cameras,compressed sensing,compressive light field capture,compressive light field reconstruction,compressive sensing,deep learning,high resolution 4D light field,image capture,image coding,image reconstruction,Image reconstruction,image resolution,image sampling,incoming rays,learning (artificial intelligence),light field imaging,Lytro Illum camera,Machine learning,Network architecture,network reconstructions,real-time light field video acquisition systems,reconstruction time,sample angular viewpoints,sampling-compression ratio,simulated coded measurements,single coded 2D image,single-shot light field cameras,spatial dimensions,spatial resolution,Spatial resolution,synthetic light fields,Two dimensional displays,two-branch network architecture,video signal processing},
  file = {/home/petteri/Zotero/storage/UB23UYVT/8014902.html}
}

@misc{hassanRethinkingSoftwareEngineering2024,
  title = {Rethinking {{Software Engineering}} in the {{Era}} of {{Foundation Models}}: {{A Curated Catalogue}} of {{Challenges}} in the {{Development}} of {{Trustworthy FMware}}},
  shorttitle = {Rethinking {{Software Engineering}} in the {{Era}} of {{Foundation Models}}},
  author = {Hassan, Ahmed E. and Lin, Dayi and Rajbahadur, Gopi Krishnan and Gallaba, Keheliya and Cogo, Filipe R. and Chen, Boyuan and Zhang, Haoxiang and Thangarajah, Kishanthan and Oliva, Gustavo Ansaldi and Lin, Jiahuei and Abdullah, Wali Mohammad and Jiang, Zhen Ming},
  year = 2024,
  month = feb,
  number = {arXiv:2402.15943},
  eprint = {2402.15943},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified 10 key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. In this paper, we discuss these challenges in detail and state the path for innovation that we envision. Next, we present FMArts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we (i) show how the unique properties of FMArts enabled us to design and develop a complex FMware for a large customer in a timely manner and (ii) discuss the lessons that we learned in doing so. We hope that the disclosure of the aforementioned challenges and our associated efforts to tackle them will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions across the software engineering discipline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering}
}

@book{hassanRethinkingSoftwareEngineering2024a,
  title = {Rethinking {{Software Engineering}} in the {{Era}} of {{Foundation Models}}: {{A Curated Catalogue}} of {{Challenges}} in the {{Development}} of {{Trustworthy FMware}}},
  shorttitle = {Rethinking {{Software Engineering}} in the {{Era}} of {{Foundation Models}}},
  author = {Hassan, Ahmed E. and Lin, Dayi and Rajbahadur, Gopi Krishnan and Gallaba, Keheliya and Roseiro C{\^o}go, Filipe and Chen, Boyuan and Zhang, Haoxiang and Thangarajah, Kishanthan and Oliva, Gustavo and Lin, Jiahuei and Abdullah, Wali and Jiang, Zhen},
  year = 2024,
  month = jul,
  pages = {305},
  doi = {10.1145/3663529.3663849},
  abstract = {Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified ten key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. For each of those challenges, we state the path for innovation that we envision. We hope that the disclosure of the challenges will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions.}
}

@article{hjort2024a,
  title = {Benchmarking Uncertainty Disentanglement: {{Specialized}} Uncertainties for Specialized Tasks},
  author = {Mucs{\'a}nyi, B{\'a}lint and Kirchhof, Michael and Oh, Seong Joon},
  year = 2024,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{hosta2025,
  title = {Homepage {\textbar} Hosta a.i.},
  author = {{Hosta}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{hosta2025,
  title = {Homepage {\textbar} Hosta a.i.},
  author = {{Hosta}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@article{hover2025,
  title = {The Ultimate App for Measurements, Takeoffs, and Design - Hover},
  author = {{Hover}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{hoverUltimateAppMeasurements2025,
  title = {The Ultimate App for Measurements, Takeoffs, and Design - Hover},
  author = {{Hover}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{ibisworld2025,
  title = {Real Estate Appraisal in the {{US}} Market Size Statistics {\textbar} Ibisworld},
  author = {{IBISWorld}},
  year = 2025,
  journal = {Government report},
  urldate = {2025-08-02}
}

@techreport{ibisworldRealEstateAppraisal2025,
  title = {Real Estate Appraisal in the {{US}} Market Size Statistics {\textbar} Ibisworld},
  author = {{IBISWorld}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{iguide2025,
  title = {Affordable Precision: {{iGUIDE}} Floor Plans {\textbar} {{iGUIDE}}},
  author = {{iGUIDE}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{iguideAffordablePrecisionIGUIDE2025,
  title = {Affordable Precision: {{iGUIDE}} Floor Plans {\textbar} {{iGUIDE}}},
  author = {{iGUIDE}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{imagespace2025,
  title = {Large Scale Accurate {{3D}} Scanning on Your Phone},
  author = {{ImageSpace}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{imagespaceLargeScaleAccurate2025,
  title = {Large Scale Accurate {{3D}} Scanning on Your Phone},
  author = {{ImageSpace}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{institute2020,
  title = {Appraisal Institute - Home Page},
  author = {Institute, Appraisal},
  year = 2020,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{instituteAppraisalInstituteHome2020,
  title = {Appraisal Institute - Home Page},
  author = {Institute, Appraisal},
  year = 2020,
  urldate = {2025-07-31}
}

@article{jaro2025,
  title = {Jaro Platform - Blog - Understanding the Difference between Property Data Collection and Appraisals},
  author = {{Jaro}},
  year = 2025,
  journal = {Blog post},
  urldate = {2025-08-02}
}

@misc{jaroJaroPlatformBlog2025,
  title = {Jaro Platform - Blog - Understanding the Difference between Property Data Collection and Appraisals},
  author = {{Jaro}},
  year = 2025,
  urldate = {2025-07-31}
}

@misc{jvm2025,
  title = {Inspection-Based Appraisal Waivers Explained},
  author = {Lending, {\relax JVM}},
  year = 2025
}

@incollection{kendallWhatUncertaintiesWe2017,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Kendall, Alex and Gal, Yarin},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = 2017,
  pages = {5574--5584},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-05-22}
}

@misc{kendallWhatUncertaintiesWe2017a,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer}}   {{Vision}}?},
  author = {Kendall, Alex and Gal, Yarin},
  year = 2017,
  number = {1703.04977},
  eprint = {1703.04977},
  publisher = {arXiv},
  archiveprefix = {arXiv}
}

@inproceedings{kim2023,
  title = {Neural Spectro-Polarimetric Fields},
  author = {Kim, Youngchan and Jin, Wonjoon and Cho, Sunghyun and Baek, Seung-Hwan},
  year = 2023,
  series = {{{SIGGRAPH Asia}} 2023 {{Conference Papers}}},
  pages = {1--11},
  doi = {10.1145/3610548.3618172},
  urldate = {2025-08-02}
}

@article{kim2023a,
  title = {Neural Spectro-Polarimetric Fields},
  author = {Kim, Youngchan and Jin, Wonjoon and Cho, Sunghyun and Baek, Seung-Hwan},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@misc{kirchhof2025a,
  title = {Revisiting Uncertainty Estimation and Calibration of Large Language Models},
  author = {Tao, Linwei and Yeh, Yi-Fan and Dong, Minjing and Huang, Tao and Torr, Philip and Xu, Chang},
  year = 2025,
  urldate = {2025-08-02}
}

@misc{kusner2018,
  title = {Counterfactual Fairness},
  author = {Matt J. Kusner, Joshua R. Loftus, Chris Russell and Silva, Ricardo},
  year = 2018,
  eprint = {1703.06856},
  archiveprefix = {arXiv},
  arxivid = {1703.06856}
}

@article{kusner2018,
  title = {Counterfactual Fairness},
  author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
  year = 2018,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@inproceedings{kusnerCounterfactualFairness2017,
  title = {Counterfactual Fairness},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year = 2017,
  month = dec,
  series = {{{NIPS}}'17},
  pages = {4069--4079},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-08-03},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
  isbn = {978-1-5108-6096-4}
}

@article{kusnerCounterfactualFairness2018,
  title = {Counterfactual Fairness},
  author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
  year = 2018,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{langGaussianLIC2LiDARInertialCameraGaussian2025,
  title = {Gaussian-{{LIC2}}: {{LiDAR-Inertial-Camera Gaussian Splatting SLAM}}},
  shorttitle = {Gaussian-{{LIC2}}},
  author = {Lang, Xiaolei and Lv, Jiajun and Tang, Kai and Li, Laijian and Huang, Jianxin and Liu, Lina and Liu, Yong and Zuo, Xingxing},
  year = 2025,
  month = jul,
  number = {arXiv:2507.04004},
  eprint = {2507.04004},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.04004},
  urldate = {2025-09-21},
  abstract = {This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian Splatting SLAM system that simultaneously addresses visual quality, geometric accuracy, and real-time performance. The proposed method performs robust and accurate pose estimation within a continuous-time trajectory optimization framework, while incrementally reconstructing a 3D Gaussian map using camera and LiDAR data, all in real time. The resulting map enables high-quality, real-time novel view rendering of both RGB images and depth maps. To effectively address under-reconstruction in regions not covered by the LiDAR, we employ a lightweight zero-shot depth model that synergistically combines RGB appearance cues with sparse LiDAR measurements to generate dense depth maps. The depth completion enables reliable Gaussian initialization in LiDAR-blind areas, significantly improving system applicability for sparse LiDAR sensors. To enhance geometric accuracy, we use sparse but precise LiDAR depths to supervise Gaussian map optimization and accelerate it with carefully designed CUDA-accelerated strategies. Furthermore, we explore how the incrementally reconstructed Gaussian map can improve the robustness of odometry. By tightly incorporating photometric constraints from the Gaussian map into the continuous-time factor graph optimization, we demonstrate improved pose estimation under LiDAR degradation scenarios. We also showcase downstream applications via extending our elaborate system, including video frame interpolation and fast 3D mesh extraction. To support rigorous evaluation, we construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth poses, depth maps, and extrapolated trajectories for assessing out-of-sequence novel view synthesis. Both the dataset and code will be made publicly available on project page https://xingxingzuo.github.io/gaussian\_lic2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/home/petteri/Zotero/storage/E4EGE7YR/Lang et al. - 2025 - Gaussian-LIC2 LiDAR-Inertial-Camera Gaussian Splatting SLAM.pdf;/home/petteri/Zotero/storage/HERZI433/2507.html}
}

@misc{lasheras-hernandezSingleShotMetricDepth2024,
  title = {Single-{{Shot Metric Depth}} from {{Focused Plenoptic Cameras}}},
  author = {{Lasheras-Hernandez}, Blanca and Strobl, Klaus H. and Izquierdo, Sergio and Bodenm{\"u}ller, Tim and Triebel, Rudolph and Civera, Javier},
  year = 2024,
  month = dec,
  number = {arXiv:2412.02386},
  eprint = {2412.02386},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.02386},
  urldate = {2025-08-03},
  abstract = {Metric depth estimation from visual sensors is crucial for robots to perceive, navigate, and interact with their environment. Traditional range imaging setups, such as stereo or structured light cameras, face hassles including calibration, occlusions, and hardware demands, with accuracy limited by the baseline between cameras. Single- and multi-view monocular depth offers a more compact alternative, but is constrained by the unobservability of the metric scale. Light field imaging provides a promising solution for estimating metric depth by using a unique lens configuration through a single device. However, its application to single-view dense metric depth is under-addressed mainly due to the technology's high cost, the lack of public benchmarks, and proprietary geometrical models and software. Our work explores the potential of focused plenoptic cameras for dense metric depth. We propose a novel pipeline that predicts metric depth from a single plenoptic camera shot by first generating a sparse metric point cloud using machine learning, which is then used to scale and align a dense relative depth map regressed by a foundation depth model, resulting in dense metric depth. To validate it, we curated the Light Field \& Stereo Image Dataset (LFS) of real-world light field images with stereo depth labels, filling a current gap in existing resources. Experimental results show that our pipeline produces accurate metric depth predictions, laying a solid groundwork for future research in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{lending2025,
  title = {Inspection-Based Appraisal Waivers Explained},
  author = {Lending, {\relax JVM}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{lending2025,
  title = {Inspection-Based Appraisal Waivers Explained},
  author = {Lending, {\relax JVM}},
  year = 2025,
  journal = {Blog post},
  urldate = {2025-08-02}
}

@misc{lengerich2023,
  title = {{{LLMs}} Understand Glass-Box Models, Discover Surprises, and Suggest Repairs},
  author = {Benjamin J. Lengerich, Sebastian Bordt, Harsha Nori and Caruana, Rich},
  year = 2023,
  eprint = {2308.01157},
  archiveprefix = {arXiv},
  arxivid = {2308.01157}
}

@article{lengerich2023,
  title = {{{LLMs}} Understand Glass-Box Models, Discover Surprises, and Suggest Repairs},
  author = {Lengerich, Benjamin J. and Bordt, Sebastian and Nori, Harsha and Nunnally, Mark E. and Aphinyanaphongs, Yin and Kellis, Manolis and Caruana, Rich},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{lengerichLLMsUnderstandGlassbox2023,
  title = {{{LLMs}} Understand Glass-Box Models, Discover Surprises, and Suggest Repairs},
  author = {Lengerich, Benjamin J. and Bordt, Sebastian and Nori, Harsha and Nunnally, Mark E. and Aphinyanaphongs, Yin and Kellis, Manolis and Caruana, Rich},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{lengerichLLMsUnderstandGlassBox2023a,
  title = {{{LLMs Understand Glass-Box Models}}, {{Discover Surprises}}, and {{Suggest Repairs}}},
  author = {Lengerich, Benjamin J. and Bordt, Sebastian and Nori, Harsha and Nunnally, Mark E. and Aphinyanaphongs, Yin and Kellis, Manolis and Caruana, Rich},
  year = 2023,
  month = aug,
  number = {arXiv:2308.01157},
  eprint = {2308.01157},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.01157},
  urldate = {2025-08-04},
  abstract = {We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package \${\textbackslash}texttt\{TalkToEBM\}\$ as an open-source LLM-GAM interface.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{lewis2020,
  title = {Retrieval-Augmented Generation for Knowledge-Intensive {{NLP}} Tasks},
  author = {Patrick Lewis, Ethan Perez, Aleksandra Piktus and Kiela, Douwe},
  year = 2020,
  eprint = {2005.11401},
  archiveprefix = {arXiv},
  arxivid = {2005.11401}
}

@article{lewis2020,
  title = {Retrieval-Augmented Generation for Knowledge-Intensive {{NLP}} Tasks},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = 2020,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{lewisRetrievalaugmentedGenerationKnowledgeintensive2020,
  title = {Retrieval-Augmented Generation for Knowledge-Intensive {{NLP}} Tasks},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = 2020,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{li2025,
  title = {{{TSGS}}: {{Improving}} Gaussian Splatting for Transparent Surface Reconstruction via Normal and de-Lighting Priors},
  author = {Mingwei Li, Pu Pang, Hehe Fan and Yang, Yi},
  year = 2025,
  eprint = {2504.12799},
  archiveprefix = {arXiv},
  arxivid = {2504.12799}
}

@article{li2025,
  title = {{{TSGS}}: {{Improving}} Gaussian Splatting for Transparent Surface Reconstruction via Normal and de-Lighting Priors},
  author = {Li, Mingwei and Pang, Pu and Fan, Hehe and Huang, Hua and Yang, Yi},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{li2025c,
  title = {Gaussian-{{LIC2}}: {{LiDAR-inertial-camera}} Gaussian Splatting {{SLAM}}},
  author = {Lang, Xiaolei and Lv, Jiajun and Tang, Kai and Li, Laijian and Huang, Jianxin and Liu, Lina and Liu, Yong and Zuo, Xingxing},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{liTSGSImprovingGaussian2025,
  title = {{{TSGS}}: {{Improving}} Gaussian Splatting for Transparent Surface Reconstruction via Normal and de-Lighting Priors},
  author = {Li, Mingwei and Pang, Pu and Fan, Hehe and Huang, Hua and Yang, Yi},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@article{liu2025,
  title = {Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey},
  author = {Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{liuUncertaintyQuantificationConfidence2025,
  title = {Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey},
  author = {Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{liuUncertaintyQuantificationConfidence2025a,
  title = {Uncertainty {{Quantification}} and {{Confidence Calibration}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Uncertainty {{Quantification}} and {{Confidence Calibration}} in {{Large Language Models}}},
  author = {Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},
  year = 2025,
  month = jun,
  number = {arXiv:2503.15850},
  eprint = {2503.15850},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.15850},
  urldate = {2025-10-22},
  abstract = {Large Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/petteri/Zotero/storage/AR2VERMX/Liu et al. - 2025 - Uncertainty Quantification and Confidence Calibration in Large Language Models A Survey.pdf;/home/petteri/Zotero/storage/7TWAJGF8/2503.html}
}

@article{locometric2025,
  title = {{{RoomScan}} pro {{LiDAR}} --- Locometric{\textregistered}},
  author = {{Locometric}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{locometricRoomScanProLiDAR2025,
  title = {{{RoomScan}} pro {{LiDAR}} --- Locometric{\textregistered}},
  author = {{Locometric}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{lundberg2017,
  title = {A Unified Approach to Interpreting Model Predictions},
  author = {Lundberg, Scott and Lee, Su-In},
  year = 2017,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@incollection{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lundberg, Scott M and Lee, Su-In},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = 2017,
  pages = {4765--4774},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-07-27}
}

@misc{lundbergUnifiedApproachInterpreting2017a,
  title = {A Unified Approach to Interpreting Model Predictions},
  author = {Lundberg, Scott and Lee, Su-In},
  year = 2017,
  eprint = {1705.07874},
  archiveprefix = {arXiv},
  arxivid = {1705.07874}
}

@book{lusht1997,
  type = {{{SSRN Scholarly Paper}}},
  title = {A {{Comparison}} of {{Prices Brought}} by {{English Auctions}} and {{Private Negotiations}}},
  author = {Lusht, Kenneth M.},
  year = 1997,
  month = jan,
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  urldate = {2025-08-01},
  abstract = {A comparison of house prices brought by English auctions and private negotiations produced evidence that the pricing mechanism matters. In an active market for middle- to high- priced houses, auctions extracted higher prices than private negotiations.},
  langid = {english},
  keywords = {A Comparison of Prices Brought by English Auctions and Private Negotiations,Kenneth M. Lusht,SSRN}
}

@misc{lushtComparisonPricesBrought1997,
  type = {{{SSRN Scholarly Paper}}},
  title = {A {{Comparison}} of {{Prices Brought}} by {{English Auctions}} and {{Private Negotiations}}},
  author = {Lusht, Kenneth M.},
  year = 1997,
  month = jan,
  number = {9212},
  eprint = {9212},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  urldate = {2025-08-01},
  abstract = {A comparison of house prices brought by English auctions and private negotiations produced evidence that the pricing mechanism matters. In an active market for middle- to high- priced houses, auctions extracted higher prices than private negotiations.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {A Comparison of Prices Brought by English Auctions and Private Negotiations,Kenneth M. Lusht,SSRN}
}

@article{mac2024,
  title = {Uniform Appraisal Dataset ({{UAD}}) 3.6 {{FAQ}}},
  author = {Mac, Freddie},
  year = 2024,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{macUniformAppraisalDataset2024,
  title = {Uniform Appraisal Dataset ({{UAD}}) 3.6 {{FAQ}}},
  author = {Mac, Freddie},
  year = 2024,
  urldate = {2025-07-31}
}

@article{mae2024,
  title = {Appraiser Update {\textbar} Fannie Mae},
  author = {Mae, Fannie},
  year = 2024,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{mae2024a,
  title = {Uniform Appraisal Dataset {\textbar} Fannie Mae},
  author = {Mae, Fannie},
  year = 2024,
  urldate = {2025-07-31}
}

@article{mae2024a,
  title = {Uniform Appraisal Dataset {\textbar} Fannie Mae},
  author = {Mae, Fannie},
  year = 2024,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{mae2025,
  title = {Uniform Appraisal Dataset {\textbar} Fannie Mae},
  author = {Mae, Fannie},
  year = 2025,
  urldate = {2025-08-02}
}

@article{mae2025,
  title = {Uniform Property Dataset {\textbar} Fannie Mae},
  author = {Mae, Fannie},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{maeAppraiserUpdateFannie2024,
  title = {Appraiser Update {\textbar} Fannie Mae},
  author = {Mae, Fannie},
  year = 2024,
  urldate = {2025-07-31}
}

@misc{maeUniformPropertyDataset2025,
  title = {Uniform Property Dataset {\textbar} Fannie Mae},
  author = {Mae, Fannie},
  year = 2025,
  urldate = {2025-07-31}
}

@article{magicplan2025,
  title = {Magicplan {\textbar} Construction \& Floor Plan App for Contractors},
  author = {{magicplan}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{magicplanMagicplanConstructionFloor2025,
  title = {Magicplan {\textbar} Construction \& Floor Plan App for Contractors},
  author = {{magicplan}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{matterport2025,
  title = {Digital Twins for Design \& Construction Management},
  author = {{Matterport}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{matterportDigitalTwinsDesign2025,
  title = {Digital Twins for Design \& Construction Management},
  author = {{Matterport}},
  year = 2025,
  urldate = {2025-07-31}
}

@inproceedings{mitchellModelCardsModel2019,
  title = {Model {{Cards}} for {{Model Reporting}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year = 2019,
  month = jan,
  eprint = {1810.03993},
  primaryclass = {cs},
  pages = {220--229},
  doi = {10.1145/3287560.3287596},
  urldate = {2023-10-21},
  abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{mitchellModelCardsModel2019a,
  title = {Model {{Cards}} for {{Model Reporting}}},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year = 2019,
  journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages = {220--229},
  doi = {10.1145/3287560.3287596}
}

@article{mtgefi2025,
  title = {Inspection Based Appraisal Waivers in {{March}} - up 57},
  author = {{MtgeFi}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{mtgefiInspectionBasedAppraisal2025,
  title = {Inspection Based Appraisal Waivers in {{March}} - up 57},
  author = {MtgeFi},
  year = 2025,
  urldate = {2025-07-31}
}

@misc{mucsanyiBenchmarkingUncertaintyDisentanglement2024,
  title = {Benchmarking {{Uncertainty Disentanglement}}: {{Specialized Uncertainties}} for {{Specialized Tasks}}},
  shorttitle = {Benchmarking {{Uncertainty Disentanglement}}},
  author = {Mucs{\'a}nyi, B{\'a}lint and Kirchhof, Michael and Oh, Seong Joon},
  year = 2024,
  month = nov,
  number = {arXiv:2402.19460},
  eprint = {2402.19460},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.19460},
  urldate = {2024-12-03},
  abstract = {Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one source of uncertainty. This paper presents the first benchmark of uncertainty disentanglement. We reimplement and evaluate a comprehensive range of uncertainty estimators, from Bayesian over evidential to deterministic ones, across a diverse range of uncertainty tasks on ImageNet. We find that, despite recent theoretical endeavors, no existing approach provides pairs of disentangled uncertainty estimators in practice. We further find that specialized uncertainty tasks are harder than predictive uncertainty tasks, where we observe saturating performance. Our results provide both practical advice for which uncertainty estimators to use for which specific task, and reveal opportunities for future research toward task-centric and disentangled uncertainties. All our reimplementations and Weights \& Biases logs are available at https://github.com/bmucsanyi/untangle.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/petteri/Zotero/storage/WZHJ6L4L/MucsÃ¡nyi et al. - 2024 - Benchmarking Uncertainty Disentanglement Specialized Uncertainties for Specialized Tasks.pdf;/home/petteri/Zotero/storage/PN8AF3YA/2402.html}
}

@misc{mucsanyiBenchmarkingUncertaintyDisentanglement2024a,
  title = {Benchmarking Uncertainty Disentanglement: {{Specialized}} Uncertainties for Specialized Tasks},
  author = {Mucs{\'a}nyi, B{\'a}lint and Kirchhof, Michael and Oh, Seong Joon},
  year = 2024,
  urldate = {2025-07-31}
}

@article{nar2021,
  title = {{{MLS}} Consolidation Resources},
  author = {ExecutiveStudentAppraiserState, RoleBrokerAssociation and Sales, SpecialtyCommercialGlobalSenior MarketShort and ExecutiveStudentAppraiserState, RoleBrokerAssociation},
  year = 2021,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{oquab2024,
  title = {{{DINOv2}}: {{Learning}} Robust Visual Features without Supervision},
  author = {Maxime Oquab, Timoth{\'e}e Darcet, Th{\'e}o Moutakanni and Bojanowski, Piotr},
  year = 2024,
  eprint = {2304.07193},
  archiveprefix = {arXiv},
  arxivid = {2304.07193}
}

@article{oquab2024,
  title = {{{DINOv2}}: {{Learning}} Robust Visual Features without Supervision},
  author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{oquabDINOv2LearningRobust2024,
  title = {{{DINOv2}}: {{Learning}} Robust Visual Features without Supervision},
  author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{peng2023,
  title = {{{OpenScene}}: {{3D}} Scene Understanding with Open Vocabularies},
  author = {Songyou Peng, Kyle Genova, Chiyu Max Jiang and Funkhouser, Thomas},
  year = 2023,
  eprint = {2211.15654},
  archiveprefix = {arXiv},
  arxivid = {2211.15654}
}

@article{peng2023,
  title = {{{OpenScene}}: {{3D}} Scene Understanding with Open Vocabularies},
  author = {Peng, Songyou and Genova, Kyle and Jiang, Chiyu "Max" and Tagliasacchi, Andrea and Pollefeys, Marc and Funkhouser, Thomas},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{pengOpenScene3DScene2023,
  title = {{{OpenScene}}: {{3D}} Scene Understanding with Open Vocabularies},
  author = {Peng, Songyou and Genova, Kyle and Jiang, Chiyu ``Max`` and Tagliasacchi, Andrea and Pollefeys, Marc and Funkhouser, Thomas},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@article{plnar2025,
  title = {Plnar Spatial - Plnar -- Property Intelligence, Powered by {{AI}} {\textbar} Fueled by Smartphone Imagery},
  author = {{Plnar}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{plnarPlnarSpatialPlnar2025,
  title = {Plnar Spatial - Plnar -- Property Intelligence, Powered by {{AI}} {\textbar} Fueled by Smartphone Imagery},
  author = {{Plnar}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{polycam2025,
  title = {{{3D}} Spatial Capture - {{LiDAR 3D}} Scanner {\textbar} Polycam},
  author = {{Polycam}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{polycam3DSpatialCapture2025,
  title = {{{3D}} Spatial Capture - {{LiDAR 3D}} Scanner {\textbar} Polycam},
  author = {{Polycam}},
  year = 2025,
  urldate = {2025-07-31}
}

@inproceedings{pushkarnaDataCardsPurposeful2022,
  title = {Data {{Cards}}: {{Purposeful}} and {{Transparent Dataset Documentation}} for {{Responsible AI}}},
  shorttitle = {Data {{Cards}}},
  booktitle = {Proceedings of the 2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
  year = 2022,
  month = jun,
  series = {{{FAccT}} '22},
  pages = {1776--1826},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3531146.3533231},
  urldate = {2023-10-22},
  abstract = {As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset's origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset's lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models---such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.x},
  isbn = {978-1-4503-9352-2},
  keywords = {data cards,dataset documentation,datasheets,model cards,responsible AI,transparency}
}

@article{pushkarnaDataCardsPurposeful2022a,
  title = {Data {{Cards}}: {{Purposeful}} and {{Transparent Dataset Documentation}} for {{Responsible AI}}},
  author = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
  year = 2022,
  journal = {2022 ACM Conference on Fairness Accountability and Transparency},
  pages = {1776--1826},
  doi = {10.1145/3531146.3533231}
}

@misc{qin2023,
  title = {{{ToolLLM}}: {{Facilitating}} Large Language Models to Master 16000+ Real-World Apis},
  author = {Yujia Qin, Shihao Liang, Yining Ye and Sun, Maosong},
  year = 2023,
  eprint = {2307.16789},
  archiveprefix = {arXiv},
  arxivid = {2307.16789}
}

@article{qin2023,
  title = {{{ToolLLM}}: {{Facilitating}} Large Language Models to Master 16000+ Real-World Apis},
  author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{qinToolLLMFacilitatingLarge2023,
  title = {{{ToolLLM}}: {{Facilitating}} Large Language Models to Master 16000+ Real-World Apis},
  author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{radford2021,
  title = {Learning Transferable Visual Models from Natural Language Supervision},
  author = {Alec Radford, Jong Wook Kim, Chris Hallacy and Sutskever, Ilya},
  year = 2021,
  eprint = {2103.00020},
  archiveprefix = {arXiv},
  arxivid = {2103.00020}
}

@article{radford2021,
  title = {Learning Transferable Visual Models from Natural Language Supervision},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = 2021,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{radfordLearningTransferableVisual2021,
  title = {Learning Transferable Visual Models from Natural Language Supervision},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = 2021,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{recon-3dRecon3DIPhoneLidar2025,
  title = {Recon-{{3D}} {\textbar} {{iPhone}} Lidar for Forensics},
  author = {{Recon-3D}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{recond2025,
  title = {Recon-{{3D}} {\textbar} {{iPhone}} Lidar for Forensics},
  author = {{Recon-3D}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@article{reggora2023,
  title = {The Path to Appraisal Modernization},
  author = {{Reggora}},
  year = 2023,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{reggoraPathAppraisalModernization2023,
  title = {The Path to Appraisal Modernization},
  author = {{Reggora}},
  year = 2023,
  urldate = {2025-07-31}
}

@article{reinholdValidatingUncertaintyMedical2020,
  title = {Validating Uncertainty in Medical Image Translation},
  author = {Reinhold, Jacob C. and He, Yufan and Han, Shizhong and Chen, Yunqiang and Gao, Dashan and Lee, Junghoon and Prince, Jerry L. and Carass, Aaron},
  year = 2020,
  month = feb,
  journal = {arXiv:2002.04639 [cs, eess]},
  eprint = {2002.04639},
  primaryclass = {cs, eess},
  urldate = {2020-07-28},
  abstract = {Medical images are increasingly used as input to deep neural networks to produce quantitative values that aid researchers and clinicians. However, standard deep neural networks do not provide a reliable measure of uncertainty in those quantitative values. Recent work has shown that using dropout during training and testing can provide estimates of uncertainty. In this work, we investigate using dropout to estimate epistemic and aleatoric uncertainty in a CT-to-MR image translation task. We show that both types of uncertainty are captured, as defined, providing confidence in the output uncertainty estimates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@inproceedings{reinholdValidatingUncertaintyMedical2020a,
  title = {Validating {{Uncertainty}} in {{Medical Image Translation}}},
  booktitle = {2020 {{IEEE}} 17th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Reinhold, Jacob C. and He, Yufan and Han, Shizhong and Chen, Yunqiang and Gao, Dashan and Lee, Junghoon and Prince, Jerry L. and Carass, Aaron},
  year = 2020,
  month = apr,
  pages = {95--98},
  issn = {1945-8452},
  doi = {10.1109/ISBI45749.2020.9098543},
  abstract = {Medical images are increasingly used as input to deep neural networks to produce quantitative values that aid researchers and clinicians. However, standard deep neural networks do not provide a reliable measure of uncertainty in those quantitative values. Recent work has shown that using dropout during training and testing can provide estimates of uncertainty. In this work, we investigate using dropout to estimate epistemic and aleatoric uncertainty in a CT-to-MR image translation task. We show that both types of uncertainty are captured, as defined, providing confidence in the output uncertainty estimates.},
  keywords = {aleatoric uncertainty,Biomedical imaging,biomedical MRI,Computed tomography,computerised tomography,CT-to-MR image translation task,dropout,epistemic uncertainty,Image translation,Measurement uncertainty,medical image processing,medical image translation,neural nets,Neural networks,output uncertainty estimates,quantitative values,standard deep neural networks,Standards,Task analysis,Uncertainty,uncertainty estimation},
  file = {/home/petteri/Zotero/storage/BJ5Z85ZD/9098543.html}
}

@misc{sapkota2025,
  title = {{{AI}} Agents vs. Agentic {{AI}}: A Conceptual Taxonomy, Applications and Challenges},
  author = {Ranjan Sapkota, Konstantinos I. Roumeliotis and Karkee, Manoj},
  year = 2025,
  eprint = {2505.10468},
  archiveprefix = {arXiv},
  arxivid = {2505.10468}
}

@article{sapkotaAIAgentsVs2026,
  title = {{{AI Agents}} vs. {{Agentic AI}}: {{A Conceptual Taxonomy}}, {{Applications}} and {{Challenges}}},
  shorttitle = {{{AI Agents}} vs. {{Agentic AI}}},
  author = {Sapkota, Ranjan and Roumeliotis, Konstantinos I. and Karkee, Manoj},
  year = 2026,
  month = feb,
  journal = {Information Fusion},
  volume = {126},
  eprint = {2505.10468},
  primaryclass = {cs},
  pages = {103599},
  issn = {15662535},
  doi = {10.1016/j.inffus.2025.103599},
  urldate = {2025-10-23},
  abstract = {This review critically distinguishes between AI Agents and Agentic AI, offering a structured, conceptual taxonomy, application mapping, and analysis of opportunities and challenges to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven and enabled by LLMs and LIMs for task-specific automation. Generative AI is positioned as a precursor providing the foundation, with AI agents advancing through tool integration, prompt engineering, and reasoning enhancements. We then characterize Agentic AI systems, which, in contrast to AI Agents, represent a paradigm shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and coordinated autonomy. Through a chronological evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both AI agents and agentic AI paradigms. Application domains enabled by AI Agents such as customer support, scheduling, and data summarization are then contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure, and propose targeted solutions such as ReAct loops, retrieval-augmented generation (RAG), automation coordination layers, and causal modeling. This work aims to provide a roadmap for developing robust, scalable, and explainable AI-driven systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/petteri/Zotero/storage/ADIXQNEL/Sapkota et al. - 2026 - AI Agents vs. Agentic AI A Conceptual Taxonomy, Applications and Challenges.pdf;/home/petteri/Zotero/storage/LDG824GP/2505.html}
}

@misc{schwabeMETRICframeworkAssessingData2024,
  title = {The {{METRIC-framework}} for Assessing Data Quality for Trustworthy {{AI}} in Medicine: A Systematic Review},
  shorttitle = {The {{METRIC-framework}} for Assessing Data Quality for Trustworthy {{AI}} in Medicine},
  author = {Schwabe, Daniel and Becker, Katinka and Seyferth, Martin and Kla{\ss}, Andreas and Sch{\"a}ffter, Tobias},
  year = 2024,
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-26},
  abstract = {The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset. This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine. Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards.},
  howpublished = {https://arxiv.org/abs/2402.13635v1},
  langid = {english}
}

@misc{schwabeMETRICframeworkAssessingData2024a,
  title = {The {{METRIC-framework}} for Assessing Data Quality for Trustworthy {{AI}} in Medicine: A Systematic Review},
  shorttitle = {The {{METRIC-framework}} for Assessing Data Quality for Trustworthy {{AI}} in Medicine},
  author = {Schwabe, Daniel and Becker, Katinka and Seyferth, Martin and Kla{\ss}, Andreas and Sch{\"a}ffter, Tobias},
  year = 2024,
  month = feb,
  number = {arXiv:2402.13635},
  eprint = {2402.13635},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.13635},
  urldate = {2024-08-13},
  abstract = {The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset. This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine. Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@book{serra2025,
  title = {How to {{Leverage Predictive Uncertainty Estimates}} for {{Reducing Catastrophic Forgetting}} in {{Online Continual Learning}}},
  author = {Serra, Giuseppe and Werner, Ben and Buettner, Florian},
  year = 2025,
  month = jul,
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.07668},
  urldate = {2025-08-01},
  abstract = {Many real-world applications require machine-learning models to be able to deal with non-stationary data distributions and thus learn autonomously over an extended period of time, often in an online setting. One of the main challenges in this scenario is the so-called catastrophic forgetting (CF) for which the learning model tends to focus on the most recent tasks while experiencing predictive degradation on older ones. In the online setting, the most effective solutions employ a fixed-size memory buffer to store old samples used for replay when training on new tasks. Many approaches have been presented to tackle this problem. However, it is not clear how predictive uncertainty information for memory management can be leveraged in the most effective manner and conflicting strategies are proposed to populate the memory. Are the easiest-to-forget or the easiest-to-remember samples more effective in combating CF? Starting from the intuition that predictive uncertainty provides an idea of the samples' location in the decision space, this work presents an in-depth analysis of different uncertainty estimates and strategies for populating the memory. The investigation provides a better understanding of the characteristics data points should have for alleviating CF. Then, we propose an alternative method for estimating predictive uncertainty via the generalised variance induced by the negative log-likelihood. Finally, we demonstrate that the use of predictive uncertainty measures helps in reducing CF in different settings.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{serraHowLeveragePredictive2025,
  title = {How to {{Leverage Predictive Uncertainty Estimates}} for {{Reducing Catastrophic Forgetting}} in {{Online Continual Learning}}},
  author = {Serra, Giuseppe and Werner, Ben and Buettner, Florian},
  year = 2025,
  month = jul,
  number = {arXiv:2407.07668},
  eprint = {2407.07668},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.07668},
  urldate = {2025-08-01},
  abstract = {Many real-world applications require machine-learning models to be able to deal with non-stationary data distributions and thus learn autonomously over an extended period of time, often in an online setting. One of the main challenges in this scenario is the so-called catastrophic forgetting (CF) for which the learning model tends to focus on the most recent tasks while experiencing predictive degradation on older ones. In the online setting, the most effective solutions employ a fixed-size memory buffer to store old samples used for replay when training on new tasks. Many approaches have been presented to tackle this problem. However, it is not clear how predictive uncertainty information for memory management can be leveraged in the most effective manner and conflicting strategies are proposed to populate the memory. Are the easiest-to-forget or the easiest-to-remember samples more effective in combating CF? Starting from the intuition that predictive uncertainty provides an idea of the samples' location in the decision space, this work presents an in-depth analysis of different uncertainty estimates and strategies for populating the memory. The investigation provides a better understanding of the characteristics data points should have for alleviating CF. Then, we propose an alternative method for estimating predictive uncertainty via the generalised variance induced by the negative log-likelihood. Finally, we demonstrate that the use of predictive uncertainty measures helps in reducing CF in different settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{shankarWeHaveNo2024,
  title = {``{{We}} Have No Idea How Models Will Behave in Production until Production'': {{How}} Engineers Operationalize Machine Learning},
  booktitle = {{{CSCW}}, 2024},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M and Parameswaran, Aditya G},
  year = 2024,
  volume = {37},
  abstract = {Organizations rely on machine learning engineers (MLEs) to deploy models and maintain ML pipelines in production. Due to models' extensive reliance on fresh data, the operationalization of machine learning, or MLOps, requires MLEs to have proficiency in data science and engineering. When considered holistically, the job seems staggering---how do MLEs do MLOps, and what are their unaddressed challenges? To address these questions, we conducted semi-structured ethnographic interviews with 18 MLEs working on various applications, including chatbots, autonomous vehicles, and finance. We find that MLEs engage in a workflow of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and (iv) continual monitoring and response. Throughout this workflow, MLEs collaborate extensively with data scientists, product stakeholders, and one another, supplementing routine verbal exchanges with communication tools ranging from Slack to organization-wide ticketing and reporting systems. We introduce the 3Vs of MLOps: velocity, visibility, and versioning---three virtues of successful ML deployments that MLEs learn to balance and grow as they mature. Finally, we discuss design implications and opportunities for future work.},
  langid = {english},
  file = {/home/petteri/Zotero/storage/DX7RIN4V/Shankar et al. - ``We have no idea how models will behave in produc.pdf}
}

@article{shankarWeHaveNo2024a,
  title = {"{{We Have No Idea How Models}} Will {{Behave}} in {{Production}} until {{Production}}": {{How Engineers Operationalize Machine Learning}}},
  shorttitle = {"{{We Have No Idea How Models}} Will {{Behave}} in {{Production}} until {{Production}}"},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
  year = 2024,
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {8},
  number = {CSCW1},
  pages = {206:1--206:34},
  doi = {10.1145/3653697},
  urldate = {2024-05-21},
  abstract = {Organizations rely on machine learning engineers (MLEs) to deploy models and maintain ML pipelines in production. Due to models' extensive reliance on fresh data, the operationalization of machine learning, or MLOps, requires MLEs to have proficiency in data science and engineering. When considered holistically, the job seems staggering---how do MLEs do MLOps, and what are their unaddressed challenges? To address these questions, we conducted semi-structured ethnographic interviews with 18 MLEs working on various applications, including chatbots, autonomous vehicles, and finance. We find that MLEs engage in a workflow of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and (iv) continual monitoring and response. Throughout this workflow, MLEs collaborate extensively with data scientists, product stakeholders, and one another, supplementing routine verbal exchanges with communication tools ranging from Slack to organization-wide ticketing and reporting systems. We introduce the 3Vs of MLOps: velocity, visibility, and versioning --- three virtues of successful ML deployments that MLEs learn to balance and grow as they mature. Finally, we discuss design implications and opportunities for future work.},
  keywords = {interview study,mlops}
}

@article{sitzmann2021,
  title = {Light Field Networks: {{Neural}} Scene Representations with Single-Evaluation Rendering},
  author = {Sitzmann, Vincent and Rezchikov, Semon and Freeman, William T. and Tenenbaum, Joshua B. and Durand, Fredo},
  year = 2021,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@inproceedings{sitzmannLightFieldNetworks2021,
  title = {Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering},
  shorttitle = {Light Field Networks},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Sitzmann, Vincent and Rezchikov, Semon and Freeman, William T. and Tenenbaum, Joshua B. and Durand, Fr{\'e}do},
  year = 2021,
  month = dec,
  series = {{{NIPS}} '21},
  pages = {19313--19325},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-08-02},
  abstract = {Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural network. Rendering a ray from an LFN requires only a single network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.},
  isbn = {978-1-7138-4539-3}
}

@article{smith2024,
  title = {Rethinking Aleatoric and Epistemic Uncertainty},
  author = {Smith, Freddie Bickford and Kossen, Jannik and Trollope, Eleanor and van der Wilk, Mark and Foster, Adam and Rainforth, Tom},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{smithRethinkingAleatoricEpistemic2024,
  title = {Rethinking Aleatoric and Epistemic Uncertainty},
  author = {Smith, Freddie Bickford and Kossen, Jannik and Trollope, Eleanor and {van der Wilk}, Mark and Foster, Adam and Rainforth, Tom},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{smithRethinkingAleatoricEpistemic2025,
  title = {Rethinking {{Aleatoric}} and {{Epistemic Uncertainty}}},
  author = {Smith, Freddie Bickford and Kossen, Jannik and Trollope, Eleanor and van der Wilk, Mark and Foster, Adam and Rainforth, Tom},
  year = 2025,
  month = jun,
  number = {arXiv:2412.20892},
  eprint = {2412.20892},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.20892},
  urldate = {2025-08-04},
  abstract = {The ideas of aleatoric and epistemic uncertainty are widely used to reason about the probabilistic predictions of machine-learning models. We identify incoherence in existing discussions of these ideas and suggest this stems from the aleatoric-epistemic view being insufficiently expressive to capture all the distinct quantities that researchers are interested in. To address this we present a decision-theoretic perspective that relates rigorous notions of uncertainty, predictive performance and statistical dispersion in data. This serves to support clearer thinking as the field moves forward. Additionally we provide insights into popular information-theoretic quantities, showing they can be poor estimators of what they are often purported to measure, while also explaining how they can still be useful in guiding data acquisition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{studio2025,
  title = {Client Challenge},
  author = {Studio, Invision},
  year = 2025,
  journal = {Government report},
  urldate = {2025-08-02}
}

@techreport{studioClientChallenge2025,
  title = {Client Challenge},
  author = {Studio, Invision},
  year = 2025,
  urldate = {2025-07-31}
}

@article{tan2024,
  title = {{{PlanarSplatting}}: {{Accurate}} Planar Surface Reconstruction in 3 Minutes},
  author = {Tan, Bin and Yu, Rui and Shen, Yujun and Xue, Nan},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{tanPlanarSplattingAccuratePlanar2024,
  title = {{{PlanarSplatting}}: {{Accurate}} Planar Surface Reconstruction in 3 Minutes},
  author = {Tan, Bin and Yu, Rui and Shen, Yujun and Xue, Nan},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{tanPlanarSplattingAccuratePlanar2024a,
  title = {{{PlanarSplatting}}: {{Accurate Planar Surface Reconstruction}} in 3 {{Minutes}}},
  shorttitle = {{{PlanarSplatting}}},
  author = {Tan, Bin and Yu, Rui and Shen, Yujun and Xue, Nan},
  year = 2024,
  month = dec,
  number = {arXiv:2412.03451},
  eprint = {2412.03451},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.03451},
  urldate = {2025-08-04},
  abstract = {This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images. We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps. As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the dependencies on 2D/3D plane detection and plane matching and tracking for planar surface reconstruction. Furthermore, the essential merits of plane-based representation plus CUDA-based implementation of planar splatting functions, PlanarSplatting reconstructs an indoor scene in 3 minutes while having significantly better geometric accuracy. Thanks to our ultra-fast reconstruction speed, the largest quantitative evaluation on the ScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the advantages of our method. We believe that our accurate and ultrafast planar surface reconstruction method will be applied in the structured data curation for surface reconstruction in the future. The code of our CUDA implementation will be publicly available. Project page: https://icetttb.github.io/PlanarSplatting/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{tao2025,
  title = {Revisiting {{Uncertainty Estimation}} and {{Calibration}} of {{Large Language Models}}},
  author = {Tao, Linwei and Yeh, Yi-Fan and Dong, Minjing and Huang, Tao and Torr, Philip and Xu, Chang},
  year = 2025,
  month = may,
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.23854},
  urldate = {2025-08-01},
  abstract = {As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{taoRevisitingUncertaintyEstimation2025,
  title = {Revisiting {{Uncertainty Estimation}} and {{Calibration}} of {{Large Language Models}}},
  author = {Tao, Linwei and Yeh, Yi-Fan and Dong, Minjing and Huang, Tao and Torr, Philip and Xu, Chang},
  year = 2025,
  month = may,
  number = {arXiv:2505.23854},
  eprint = {2505.23854},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.23854},
  urldate = {2025-08-01},
  abstract = {As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{taxonomy2025,
  title = {Bloom's Taxonomy - Wikipedia},
  author = {{taxonomy}, Bloom's},
  year = 2025,
  urldate = {2025-07-31}
}

@article{taxonomy2025,
  title = {Bloom's Taxonomy - Wikipedia},
  author = {{taxonomy}, Bloom's},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@article{thomason2024,
  title = {Single-Shot Metric Depth from Focused Plenoptic Cameras},
  author = {{Lasheras-Hernandez}, Blanca and Strobl, Klaus H. and Izquierdo, Sergio and Bodenm{\"u}ller, Tim and Triebel, Rudolph and Civera, Javier},
  year = 2024,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{valuation2024,
  title = {Preparing for {{UAD}} 3.6: {{What}} Lenders Need to Know},
  author = {Valuation, Class},
  year = 2024,
  journal = {Blog post},
  urldate = {2025-08-02}
}

@misc{valuation2025,
  title = {{{GSE}} Hybrid Appraisals: A Game-Changer for Mortgage Lenders in 2025 - Class Valuation},
  author = {Valuation, Class},
  year = 2025,
  urldate = {2025-07-31}
}

@article{valuation2025,
  title = {{{GSE}} Hybrid Appraisals: A Game-Changer for Mortgage Lenders in 2025 - Class Valuation},
  author = {Valuation, Class},
  year = 2025,
  journal = {Blog post},
  urldate = {2025-08-02}
}

@misc{valuationPreparingUAD362024,
  title = {Preparing for {{UAD}} 3.6: {{What}} Lenders Need to Know},
  author = {Valuation, Class},
  year = 2024,
  urldate = {2025-07-31}
}

@misc{wachter2017,
  title = {Counterfactual Explanations without Opening the Black Box: {{Automated}} Decisions and the {{GDPR}}},
  author = {Sandra Wachter, Brent Mittelstadt and Russell, Chris},
  year = 2017,
  eprint = {1711.00399},
  archiveprefix = {arXiv},
  arxivid = {1711.00399}
}

@article{wachter2017,
  title = {Counterfactual Explanations without Opening the Black Box: {{Automated}} Decisions and the {{GDPR}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = 2017,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{wachterCounterfactualExplanationsOpening2017,
  title = {Counterfactual Explanations without Opening the Black Box: {{Automated}} Decisions and the {{GDPR}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = 2017,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@misc{wang2025,
  title = {From Aleatoric to Epistemic: {{Exploring}} Uncertainty Quantification Techniques in Artificial Intelligence},
  author = {Tianyang Wang, Yunze Wang, Jun Zhou and Yan, Lawrence KQ},
  year = 2025,
  eprint = {2501.03282},
  archiveprefix = {arXiv},
  arxivid = {2501.03282}
}

@article{wang2025,
  title = {From Aleatoric to Epistemic: {{Exploring}} Uncertainty Quantification Techniques in Artificial Intelligence},
  author = {Wang, Tianyang and Wang, Yunze and Zhou, Jun and Peng, Benji and Song, Xinyuan and Zhang, Charles and Sun, Xintian and Niu, Qian and Liu, Junyu and Chen, Silin and Chen, Keyu and Li, Ming and Feng, Pohsun and Bi, Ziqian and Liu, Ming and Zhang, Yichao and Fei, Cheng and Yin, Caitlyn Heqi and Yan, Lawrence KQ},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{wangAleatoricEpistemicExploring2025,
  title = {From Aleatoric to Epistemic: {{Exploring}} Uncertainty Quantification Techniques in Artificial Intelligence},
  author = {Wang, Tianyang and Wang, Yunze and Zhou, Jun and Peng, Benji and Song, Xinyuan and Zhang, Charles and Sun, Xintian and Niu, Qian and Liu, Junyu and Chen, Silin and Chen, Keyu and Li, Ming and Feng, Pohsun and Bi, Ziqian and Liu, Ming and Zhang, Yichao and Fei, Cheng and Yin, Caitlyn Heqi and Yan, Lawrence KQ},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@article{wescan2025,
  title = {{{weScan}}},
  author = {{weScan}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{wescanWeScan2025,
  title = {{{weScan}}},
  author = {{weScan}},
  year = 2025,
  urldate = {2025-07-31}
}

@book{williamson1975a,
  type = {{{SSRN Scholarly Paper}}},
  title = {Markets and {{Hierarchies}}: {{Analysis}} and {{Antitrust Implications}}: {{A Study}} in the {{Economics}} of {{Internal Organization}}},
  shorttitle = {Markets and {{Hierarchies}}},
  author = {Williamson, Oliver E.},
  year = 1975,
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  urldate = {2025-08-01},
  abstract = {This study analyzes organization of economic activity within and between markets and hierarchies. It considers the transaction to be the ultimate unit of microeconomic analysis, and defines hierarchical transactions as ones for which a single administrative entity spans both sides of the transaction, some form of subordination prevails and, typically, consolidated ownership obtains. Discusses the advantages of the transactional approach by examining three issues: price discrimination, insurance, and vertical integration. Develops the concept of the organizational failure framework, and demonstrates why it is always the combination of human with environmental factors, not either taken by itself, that causes transactional problems. The study also describes each of the transactional relations of interest, and presents the advantages of internal organization with respect to the transactional condition. The analysis explains why primary work groups of the peer group and simple hierarchy types arise. The same transactional factor which impede autonomous contracting between individuals also impede market exchange between technologically separable work groups. Peer groups can be understood as an internal organizational response to the frictions of intermediate product markets, while conglomerate organization can be seen as a response to failures in the capital market. In both contexts, the same human factors, such as bounded rationality and opportunism, occur. Examines the reasons for and properties of the employment relation, which is commonly associated with voluntary subordination. The analysis attempts better to assess the employment relation in circumstances where workers acquire, during the course of the employment, significant job-specific skills and knowledge. The study compares alternative labor-contracting modes and demonstrates that collective organization is helpful in enhancing the acquisition of idiosyncratic knowledge and skills by the work force. The study then examines more complex structures -- the movement from simple hierarchies to the vertical integration of firms, then multidivisional structures, conglomerates, monopolies and oligopolies. Discusses the market structure in relation to technical and organizational innovation. The study proposes a systems approach to the innovation process. Its purpose is to permit the realization of the distinctive advantages of both small and large firms which apply at different stages of the innovation process. The analysis also examines the relation of organizational innovation to technological innovation. (AT)},
  langid = {english},
  keywords = {Environment,Financial markets,Innovation process,Labor markets,Market structures,Monopolies,Oligopolies,Organizational structures,Peer groups,Public policies,Technology innovation,Transaction costs,Uncertainty,Vertical integration}
}

@misc{williamsonMarketsHierarchiesAnalysis1975,
  type = {{{SSRN Scholarly Paper}}},
  title = {Markets and {{Hierarchies}}: {{Analysis}} and {{Antitrust Implications}}: {{A Study}} in the {{Economics}} of {{Internal Organization}}},
  shorttitle = {Markets and {{Hierarchies}}},
  author = {Williamson, Oliver E.},
  year = 1975,
  number = {1496220},
  eprint = {1496220},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  urldate = {2025-08-01},
  abstract = {This study analyzes organization of economic activity within and between markets and hierarchies. It considers the transaction to be the ultimate unit of microeconomic analysis, and defines hierarchical transactions as ones for which a single administrative entity spans both sides of the transaction, some form of subordination prevails and, typically, consolidated ownership obtains.        Discusses the advantages of the transactional approach by examining three issues: price discrimination, insurance, and vertical integration.          Develops the concept of the organizational failure framework, and demonstrates why it is always the combination of human with environmental factors, not either taken by itself, that causes transactional problems. The study also describes each of the transactional relations of interest, and presents the advantages of internal organization with respect to the transactional condition.                  The analysis explains why primary work groups of the peer group and simple hierarchy types arise. The same transactional factor which impede autonomous contracting between individuals also impede market exchange between technologically separable work groups. Peer groups can be understood as an internal organizational response to the frictions of intermediate product markets, while conglomerate organization can be seen as a response to failures in the capital market. In both contexts, the same human factors, such  as bounded rationality and opportunism, occur.                 Examines the reasons for and properties of the employment relation, which is commonly associated with voluntary subordination. The analysis attempts better to assess the employment relation in circumstances where workers acquire, during the course of the employment, significant job-specific skills and knowledge. The study compares alternative labor-contracting modes and demonstrates that collective organization is helpful in enhancing the acquisition of idiosyncratic knowledge and skills by the work force.                 The study then examines more complex structures -- the movement from simple hierarchies to the vertical integration of firms, then multidivisional structures, conglomerates, monopolies and oligopolies.                 Discusses the market structure in relation to technical and organizational innovation.  The study proposes a systems approach to the innovation process. Its purpose is to permit the realization of  the distinctive advantages of both small and large firms which apply at different stages of the innovation process. The analysis also examines the relation of organizational innovation to technological innovation.  (AT)},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Environment,Financial markets,Innovation process,Labor markets,Market structures,Monopolies,Oligopolies,Organizational structures,Peer groups,Public policies,Technology innovation,Transaction costs,Uncertainty,Vertical integration}
}

@misc{williamsonMarketsHierarchiesAnalysis1975a,
  type = {{{SSRN Scholarly Paper}}},
  title = {Markets and {{Hierarchies}}: {{Analysis}} and {{Antitrust Implications}}: {{A Study}} in the {{Economics}} of {{Internal Organization}}},
  shorttitle = {Markets and {{Hierarchies}}},
  author = {Williamson, Oliver E.},
  year = 1975,
  number = {1496220},
  eprint = {1496220},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  urldate = {2025-08-04},
  abstract = {This study analyzes organization of economic activity within and between markets and hierarchies. It considers the transaction to be the ultimate unit of microeconomic analysis, and defines hierarchical transactions as ones for which a single administrative entity spans both sides of the transaction, some form of subordination prevails and, typically, consolidated ownership obtains.        Discusses the advantages of the transactional approach by examining three issues: price discrimination, insurance, and vertical integration.          Develops the concept of the organizational failure framework, and demonstrates why it is always the combination of human with environmental factors, not either taken by itself, that causes transactional problems. The study also describes each of the transactional relations of interest, and presents the advantages of internal organization with respect to the transactional condition.                  The analysis explains why primary work groups of the peer group and simple hierarchy types arise. The same transactional factor which impede autonomous contracting between individuals also impede market exchange between technologically separable work groups. Peer groups can be understood as an internal organizational response to the frictions of intermediate product markets, while conglomerate organization can be seen as a response to failures in the capital market. In both contexts, the same human factors, such  as bounded rationality and opportunism, occur.                 Examines the reasons for and properties of the employment relation, which is commonly associated with voluntary subordination. The analysis attempts better to assess the employment relation in circumstances where workers acquire, during the course of the employment, significant job-specific skills and knowledge. The study compares alternative labor-contracting modes and demonstrates that collective organization is helpful in enhancing the acquisition of idiosyncratic knowledge and skills by the work force.                 The study then examines more complex structures -- the movement from simple hierarchies to the vertical integration of firms, then multidivisional structures, conglomerates, monopolies and oligopolies.                 Discusses the market structure in relation to technical and organizational innovation.  The study proposes a systems approach to the innovation process. Its purpose is to permit the realization of  the distinctive advantages of both small and large firms which apply at different stages of the innovation process. The analysis also examines the relation of organizational innovation to technological innovation.  (AT)},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Environment,Financial markets,Innovation process,Labor markets,Market structures,Monopolies,Oligopolies,Organizational structures,Peer groups,Public policies,Technology innovation,Transaction costs,Uncertainty,Vertical integration}
}

@article{xactimate2025,
  title = {Xactimate},
  author = {{Xactimate}},
  year = 2025,
  journal = {Web page},
  urldate = {2025-08-02}
}

@misc{xactimateXactimate2025,
  title = {Xactimate},
  author = {{Xactimate}},
  year = 2025,
  urldate = {2025-07-31}
}

@article{xu2025b,
  title = {Towards Depth Foundation Model: {{Recent}} Trends in Vision-Based Depth Estimation},
  author = {Xu, Zhen and Zhou, Hongyu and Peng, Sida and Lin, Haotong and Guo, Haoyu and Shao, Jiahao and Yang, Peishan and Yang, Qinglin and Miao, Sheng and He, Xingyi and Wang, Yifan and Wang, Yue and Hu, Ruizhen and Liao, Yiyi and Zhou, Xiaowei and Bao, Hujun},
  year = 2025,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@misc{xuDepthFoundationModel2025,
  title = {Towards {{Depth Foundation Model}}: {{Recent Trends}} in {{Vision-Based Depth Estimation}}},
  shorttitle = {Towards {{Depth Foundation Model}}},
  author = {Xu, Zhen and Zhou, Hongyu and Peng, Sida and Lin, Haotong and Guo, Haoyu and Shao, Jiahao and Yang, Peishan and Yang, Qinglin and Miao, Sheng and He, Xingyi and Wang, Yifan and Wang, Yue and Hu, Ruizhen and Liao, Yiyi and Zhou, Xiaowei and Bao, Hujun},
  year = 2025,
  month = jul,
  number = {arXiv:2507.11540},
  eprint = {2507.11540},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.11540},
  urldate = {2025-09-21},
  abstract = {Depth estimation is a fundamental task in 3D computer vision, crucial for applications such as 3D reconstruction, free-viewpoint rendering, robotics, autonomous driving, and AR/VR technologies. Traditional methods relying on hardware sensors like LiDAR are often limited by high costs, low resolution, and environmental sensitivity, limiting their applicability in real-world scenarios. Recent advances in vision-based methods offer a promising alternative, yet they face challenges in generalization and stability due to either the low-capacity model architectures or the reliance on domain-specific and small-scale datasets. The emergence of scaling laws and foundation models in other domains has inspired the development of "depth foundation models": deep neural networks trained on large datasets with strong zero-shot generalization capabilities. This paper surveys the evolution of deep learning architectures and paradigms for depth estimation across the monocular, stereo, multi-view, and monocular video settings. We explore the potential of these models to address existing challenges and provide a comprehensive overview of large-scale datasets that can facilitate their development. By identifying key architectures and training strategies, we aim to highlight the path towards robust depth foundation models, offering insights into their future research and applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/petteri/Zotero/storage/5JPYVVAZ/Xu et al. - 2025 - Towards Depth Foundation Model Recent Trends in Vision-Based Depth Estimation.pdf;/home/petteri/Zotero/storage/MMLR5XXP/2507.html}
}

@misc{zhang2018,
  title = {Mitigating Unwanted Biases with Adversarial Learning},
  author = {Brian Hu Zhang, Blake Lemoine and Mitchell, Margaret},
  year = 2018,
  eprint = {1801.07593},
  archiveprefix = {arXiv},
  arxivid = {1801.07593}
}

@article{zhang2018,
  title = {Mitigating Unwanted Biases with Adversarial Learning},
  author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  year = 2018,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@misc{zhangMitigatingUnwantedBiases2018,
  title = {Mitigating {{Unwanted Biases}} with {{Adversarial Learning}}},
  author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  year = 2018,
  month = jan,
  number = {arXiv:1801.07593},
  eprint = {1801.07593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.07593},
  urldate = {2025-08-01},
  abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@article{zhangMitigatingUnwantedBiases2018a,
  title = {Mitigating Unwanted Biases with Adversarial Learning},
  author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  year = 2018,
  journal = {arXiv},
  urldate = {2025-07-31}
}

@inproceedings{zhangOpenVocabularyFunctional3D2025,
  title = {Open-{{Vocabulary Functional 3D Scene Graphs}} for {{Real-World Indoor Spaces}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Chenyangguang and Delitzas, Alexandros and Wang, Fangjinhua and Zhang, Ruida and Ji, Xiangyang and Pollefeys, Marc and Engelmann, Francis},
  year = 2025,
  pages = {19401--19413},
  urldate = {2025-08-20},
  langid = {english}
}

@misc{zhangOpenVocabularyFunctional3D2025a,
  title = {Open-{{Vocabulary Functional 3D Scene Graphs}} for {{Real-World Indoor Spaces}}},
  author = {Zhang, Chenyangguang and Delitzas, Alexandros and Wang, Fangjinhua and Zhang, Ruida and Ji, Xiangyang and Pollefeys, Marc and Engelmann, Francis},
  year = 2025,
  month = mar,
  number = {arXiv:2503.19199},
  eprint = {2503.19199},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.19199},
  urldate = {2025-08-20},
  abstract = {We introduce the task of predicting functional 3D scene graphs for real-world indoor environments from posed RGB-D images. Unlike traditional 3D scene graphs that focus on spatial relationships of objects, functional 3D scene graphs capture objects, interactive elements, and their functional relationships. Due to the lack of training data, we leverage foundation models, including visual language models (VLMs) and large language models (LLMs), to encode functional knowledge. We evaluate our approach on an extended SceneFun3D dataset and a newly collected dataset, FunGraph3D, both annotated with functional 3D scene graphs. Our method significantly outperforms adapted baselines, including Open3DSG and ConceptGraph, demonstrating its effectiveness in modeling complex scene functionalities. We also demonstrate downstream applications such as 3D question answering and robotic manipulation using functional 3D scene graphs. See our project page at https://openfungraph.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{zhou2023,
  title = {Don't Make Your {{LLM}} an Evaluation Benchmark Cheater},
  author = {Kun Zhou, Yutao Zhu, Zhipeng Chen and Han, Jiawei},
  year = 2023,
  eprint = {2311.01964},
  archiveprefix = {arXiv},
  arxivid = {2311.01964}
}

@article{zhou2023,
  title = {Don't Make Your {{LLM}} an Evaluation Benchmark Cheater},
  author = {Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-08-02}
}

@article{zhouDontMakeYour2023,
  title = {Don't Make Your {{LLM}} an Evaluation Benchmark Cheater},
  author = {Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  year = 2023,
  journal = {arXiv},
  urldate = {2025-07-31}
}
