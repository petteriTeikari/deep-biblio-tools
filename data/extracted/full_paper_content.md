# Knowledge-driven inference for automatic reconstruction of indoor detailed as-built BIMs from laser scanning data

**Journal:** Automation in Construction
**Year:** 2023
**Volume:** 156
**DOI:** 10.1016/j.autcon.2023.105097


## Automation in Construction
Volume 156 , December 2023 , 105097
# Knowledge-driven inference for automatic reconstruction of indoor detailed as-built BIMs from laser scanning data
Author links open overlay panel Biao Xiong a , Yusheng Jin a , Fashuai Li b c , Yuwei Chen c , Yiquan Zou d , Zhize Zhou e Show more Outline Add to Mendeley Share Cite https://doi.org/10.1016/j.autcon.2023.105097 Get rights and content Full text access
## Highlights
- •A room segmentation approach for complex indoor scenes.
- •Topological relations to preserve the high fidelity of reconstructed BIMs.
- •An automatic BIM reconstruction framework achieving high performance.
- •A method combining segmentation optimization and geometric regularization.
- •An inference transforming prior knowledge to generic constraints and optimization.
## Abstract
In spite of recent advances, a significant disparity between automatically reconstructed building information models (BIMs) and real scenes persists, particularly within complex indoor environments . With this objective in mind, a knowledge-driven as-built BIM reconstruction method is proposed, which employs laser scanning point clouds and panorama images. Initiation of the method involves the segmentation of 3D data into individual rooms through the application of wall constraints. Subsequently, geometric regularization of the rooms is performed, accompanied by the establishment of topological relations through the maintenance of rigid consistency among rooms. Finally, building frames and components embedded within walls are reconstructed and assembled to formulate comprehensive BIMs. The method was evaluated using indoor point cloud datasets (ISPRS and WUT), showing that it outperforms state-of-the-art techniques with room segmentation accuracy and completeness of 0.98 and 0.88, respectively, and achieving millimetre level accuracy on average. - Previousarticlein issue
- Nextarticlein issue
## Keywords
Laser point cloud Indoor BIM reconstruction Wall-beam centreline Geometry regularization Topological relation
## 1. Introduction
Over the past few decades, Building Information Model (BIM) has been widely used in the AECO (Architecture, Engineering, Construction and Operation) industry. BIM is a digital representation of a building that encapsulates essential information throughout its lifecycle from construction to demolition, encompassing 3D design sketches , material properties, and so forth. More than just a basic 3D model, BIM is structured in a hierarchical manner, with components organized from the building level down to specific elements like walls, windows, and doors. Currently, BIM can be generally categorized into two types: as-designed BIM, which is created during the design stage, and as-built BIM, which reflects the current state of the building during construction. As-designed BIM represents the ideal case, requiring that all elements are known, modelled, and semantically complete before construction. In reality, however, the perpetual renewal of buildings throughout their lifecycle emphasizes the significance of as-built BIM. Creating as-built BIMs offers substantial benefits for existing buildings. As-built BIMs facilitate comprehensive modelling, structural analysis, and well-informed decision-making throughout the lifecycle of structures. By capturing the geometric and architectural details of buildings, engineers and architects can identify structural deformations, irregularities, and potential weaknesses in the buildings' construction, enabling them to make informed choices about repairs, reinforcements, or retrofits . Moreover, by comparing the as-built structure to the original design plans, contractors can swiftly detect any discrepancies, deviations, or errors in the construction work. In this way, as-built BIMs can help to reduce construction costs and errors, and accelerate construction processes [ 1 , 2 ]. Since most old buildings lack as-built BIMs for their maintenance and renovation, the 3D reconstruction of interior scenes of existing buildings to generate as-built BIMs is in great demand [ 1 , 3 ]. In recent years, many studies, referred to as Scan-to-BIM [ 1 ], have focused on the automatic reconstruction of as-built BIM from laser scanning point cloud data due to its high precision and efficiency, and some great progresses have already been made in this area. However, several challenges still exist in Scan-to-BIM. Firstly, most Scan-to-BIM work relies heavily on manual labelling [ 1 , 4 ], which can be laborious and time-consuming, especially in complex indoor environments with lots of small components such as sockets and switches. Besides, inconsistent modelling standards among different sketchers, even skilled ones, lead to significant variations in BIM quality [ 2 , 5 ]. Secondly, there is a lack of consistency between models and their corresponding buildings. Different components in BIM are rarely globally aligned to maintain geometric consistency [ 3 , [6] , [7] , [8] , [9] ]. Connections between building components are not strictly constrained, resulting in inconsistent topology [ [8] , [9] , [10] , [11] ]. Furthermore, important building components such as sockets and switches are often missing from the model, leading to semantic inconsistencies [ 8 , [10] , [11] , [12] , [13] , [14] ]. Sockets and switches play an important role in BIMs, as they facilitate the precise planning of electrical systems within a building. These inconsistencies significantly impact the accuracy of reconstructed BIMs. To address these challenges, this paper proposes an automated indoor as-built BIM reconstruction method based on structural knowledge of buildings by reasoning more stringent sets of constraints at three levels: geometry, topology, and semantics. The proposed method focuses on single-story indoor environments under Manhattan assumption, and the accuracy of reconstructed BIMs can reach millimetre level. In this method, point clouds are segmented into rooms as an essential step before modelling. Then main frames of the rooms are modelled including walls, ceilings, etc., and the openings (e.g., windows and doors) and in-wall embeddings (e.g., sockets and switches) are detected afterwards. The contributions of this research includes: - (1)A novel roomsegmentation methodthat combines structural constraints and global optimization to segment complex rooms and long corridors, even in the presence of occlusions and without prior scanner position information.
- (2)The incorporation of geometry and construct topological relations to preserve the consistency between extracted structures and realistic scenes, significantly enhancing the reliability of model reconstruction.
- (3)A novel BIM reconstruction framework for indoor scenes, which achieves higher accuracy and reconstructs models with more details, including beams and switches, surpassing existing methods.
## 2. Related work
In this section, researches on Scan-to-BIM are reviewed from two perspectives, room segmentation and model reconstruction.
### 2.1. Room segmentation
Room segmentation involves dividing an interior space into subspaces with spatial semantic properties [ 15 ] to provide prior information for BIM modelling. Bormann et al. [ 16 ] utilized a morphology operator for room segmentation, using an erode operator applied iteratively on a binary grid map to separate connected areas, marking each separated area as a single room until all occupied building areas were labelled. This approach is prone to under-segmentation when doors are relatively wide. Magri and Fusiello [ 17 ] extracted wall lines from point clouds and constructed cell complexes, dividing rooms by K-medoid clustering according to the visibility of surface centroids , which could lead to over-segmentation. Elseicy et al. [ 18 ] used trajectory data from mobile laser scanners to assist in point cloud segmentation, and employed multiple filters to optimize the labelling result. This method is limited due to its reliance on trajectory data. Mura et al. [ 10 ] extracted wall lines from point clouds to construct a cell complex, weighting its edges according to the possibility of real walls, and carrying out a diffusion process to segment rooms. Ochmann et al. [ 12 ] proposed a segmentation method based on Bayesian theory, calculating the quasi-conditional probability based on the mutual visibility of points and iteratively updating the allocation of points in a room. This method requires prior knowledge of the scanning positions. Ambrus et al. [ 19 ] used a Voronoi graph to calculate the viewpoints of a group of rooms to obtain the initial labels of rooms, then refining the segmentation result through a two-step energy minimization approach. This method heavily relies on the accuracy of wall and ceiling detection.
### 2.2. Model reconstruction
Model reconstruction is implemented after room segmentation in Scan-to-BIM to generate simplified representations of the 3D shapes of building components. Model reconstruction methods can be either parametric or non-parametric [ 1 , 20 ], and only parametric methods are to be reviewed as BIM is mainly represented by parametric primitives. Besides, these methods can be categorized into surface-based and volume-based, as detailed below.
#### 2.2.1. Surface-based methods
Surface-based methods are more commonly adopted for automatic indoor reconstruction compared to volume-based approaches [ 19 , 21 ]. Previtali et al. [ 22 ] tackled high occlusion by transforming indoor reconstruction into a problem of marking planar structural units based on energy minimization, and used a graph-cut algorithm to solve it. Shi et al. [ 23 ] used a hybrid algorithm for point cloud segmentation, identifying walls based on probability distribution functions and reconstructing openings to enrich wall details. Cai et al. [ 7 ] proposed a planar reconstruction method combining point density and normal direction to obtain the rough external contour of the interior building. Mura et al. [ 10 ] used an occlusion perception algorithm and heat diffusion process to automatically extract each room. Wang et al. [ 24 ] used semantic labels to extract line structures, which were regularly adjusted to obtain a line frame model conforming to the building's architectural structure . Cai and Fan [ 25 ] extracted wall lines using a new grammar rule and stretching them based on prior knowledge of room layout. This method is sensitive to noise and occlusion. However, surface-based modelling methods struggle to express the semantic information of the models compared to volume-based methods. As a result, most surface modelling methods focus solely on walls, overlooking essential building elements like beams, columns, and more. Additionally, these methods fail to account for the topological relationship between walls, such as connection or embedding, as walls are represented as independent planes.
#### 2.2.2. Volume-based methods
Volume-based modelling methods can more effectively depict the topological relationships between walls, and thus improve the topological consistency of the model by merge inner walls into volume walls. These methods are particularly suitable for BIM reconstruction, as BIM components are typically represented as volume primitives rather than planar primitives, and volume modelling enables the reconstruction of a wide range of architectural structures, ensuring the semantic consistency and richness of the model. The following review discusses a range of volume-based methods, including traditional and deep learning-based methods.
##### 2.2.2.1. Traditional reconstruction methods
Ochmann et al. [ 12 ] utilized room prior layout to transform the indoor reconstruction task into an energy minimization problem and reconstructed a topologically consistent volumetric wall model based on contextual relations. Similarly, Cui et al. [ 8 ] proposed a wall-line-based method to reconstruct indoor structures. Lim and Doh [ 26 ] introduced an energy-based method to reconstruct multi-story indoor models from point clouds and trajectories. Nikoohemat et al. [ 9 ] constructed a volumetric wall model based on the topological relationship between wall patches and provided a topological map of indoor spaces. For complex indoor environments with multiple rooms, Jung et al. [ 13 ] extracted wall lines and obtained the wall volume model by distinguishing the inner and outer walls. Tran and Khoshelham [ 27 ] decomposed the interior space by extracting planes and completed the reconstruction using grammatical rules. This method reconstructs only walls and floors. Han et al. [ 28 ] cast the reconstruction as a globally optimal combination problem of a series of 2D segments or units to reconstruct indoor models. Ochmann et al. [ 6 ] reconstructed indoor volumetric models based on integer linear optimization. Bassier and Vergauwen [ 14 ] proposed a connection evaluation framework to reconstruct indoor building models by calculating the best-matching topological relationship between walls. Du et al. [ 29 ] introduced a hierarchical optimization, which globally regularizes linear primitives and performs topological connections, to reconstruct building models from large-scale airborne point cloud scenes. Hübner et al. [ 11 ] proposed a fully automatic voxel-based indoor reconstruction method, which can derive semantically rich and geometrically complete voxel-represented indoor models from unstructured triangular meshes. This method can handle sloped floors as well as curved walls.
##### 2.2.2.2. Deep learning-based reconstruction methods
Pintore et al. [ 30 ] employed a deep learning approach to generate mosaic-boundary 3D surfaces from a single 360° image in real-time to recover the 3D shape of a room-boundary permanent surface. Yang et al. [ 31 ] presented a semantic-guided method for reconstructing indoor navigation elements from RGB-D data. They utilized a graph convolutional network-based architectural structure recognition method to infer long-range interactions between primitives and map indoor space features directly from coloured 3D points to the Indoor GM-encoded navigation model. Tang et al. [ 3 ] applied a deep learning method to interpret point cloud scenes into multiple structures and combined room prior layouts to perform semantically complete volume reconstruction of indoor environments. Pan et al. [ 32 ] proposed a high-precision indoor unbounded RGB-D dense point cloud reconstruction algorithm to address problems such as low precision, inaccurate pose estimation, and limited data set shooting. Guo et al. [ 33 ] incorporated planar constraints into depth map estimation by utilizing MLP networks to represent signed distance functions as scene geometry. The method regularized floors and walls predicted by a 2D semantic segmentation network with planar constraints, addressing the challenges of reconstructing 3D indoor scenes from multi-view images, such as handling ground-textured planar regions. Tang et al. [ 34 ] presented a fast, automatic method for reconstructing semantically rich indoor 3D building models from low-quality RGB-D sequences to handle situations with line-of-sight occlusion or complex spatial structures. Overall, volume-based modelling methods tend to reconstruct models more consistent with as-built BIMs compared to surface-based methods. Volume-based modelling represents each architectural component as a geometric element corresponding to the actual part at the geometric level, as well as a more complex semantic component at the semantic level . Moreover, models reconstructed by volume-based methods aligns more accurately with the actual building at the topological level, effectively presenting the topological information of the building itself.
## 3. Methodology for as-built BIM reconstruction
The proposed as-built BIM reconstruction framework is shown in Fig. 1 . The process comprises three main stages: room segmentation, extraction of topological relations, and model reconstruction. Initially, the original point cloud is segmented into individual rooms. Next, the topological relationships between building components including walls and beams are extracted. Finally, BIM is constructed by reconstructing the building frames and in-wall components.
### 3.1. Room segmentation
A room segmentation method is proposed to partition an original point cloud into individual rooms, based on the concept that a room is an enclosed space surrounded by the main structure of a building. In this method, a point cloud is first semantically segmented into different components, including walls, beams, floors, ceilings, and clutters. Then, wall-beam centrelines are extracted from walls and beams. Utilizing these centrelines, the point cloud is initially segmented into different rooms, and the results are further refined using an optimization method. This approach differs from previous methods as it does not require any prior knowledge, such as the positions of laser scanners , etc.
#### 3.1.1. Semantic segmentation
In this step, main building structures are semantically segmented from original point clouds, as illustrated in Fig. 2 . Initially, point clouds are clustered into different segments using a surface growing algorithm [ 35 ] with a default neighbourhood radius of 1.0 and number of neighbours for KD-tree set at 20. Subsequently, a series of features for each segment are extracted, including 2D area, normal direction, size and height. Next, segments are classified into five categories, ceilings, walls, floors, beams and clutters, by implementing a set of rules based on the features. For instance, a segment will be classified as a floor if its normal direction is vertical, its 2D area is large than a certain value, and its height is lower than the mean z-value of a point cloud, while it will be classified as a wall if its normal direction is horizontal and its area, width and length surpass predetermined thresholds (refer to Section 4.2.1 for detailed threshold settings). Objects not categorized as ceilings, walls, floors, or beams are classified as clutters, and some of them such as furniture are subsequently filtered out from the original point cloud.
#### 3.1.2. Wall-beam centreline extraction
Wall-beam centrelines, instead of only walls considered in most existing methods, are extracted to improve the reliability of room segmentation. Prior to extraction, each classified segment, including floor, ceil, wall and beam, is fitted to a plane using least squares . The fitted floor planes are then intersected with wall planes to obtain intersection lines. Two intersecting lines are assumed to be parallel if the angle between them is sufficiently small, and are merged as a single line if the lines are also close enough to each other, addressing the over-segmentation issue caused by occlusions from indoor furniture. A wall centreline is simply the midline of two parallel intersection lines belonging to the same wall (i.e., their distance is within the interior wall width d Wi ), as depicted by the red line in the upper left corner of Fig. 3 . However, as the outside of a building is typically not scanned by a laser scanner, some wall lines will not have corresponding counterparts. Consequently, walls with two lines are categorized as interior walls that separate different rooms within the building, while the others are categorized as exterior walls that demarcate the inner space and outer space of a building, as shown in Fig. 3 . To calculate the centreline of an exterior wall, the number of points is counted from both sides of the wall plane, and the outer direction of this exterior wall plane is determined as the normal direction pointing towards the side with a smaller number of points. Then the centreline is located d We / 2 (half of the exterior wall width) away from the wall line along the outer direction, as illustrated by the red line in the right crop box of Fig. 3 . As-built BIMs typically pertain to buildings that have yet to be furnished or decorated, in which case exposed beams constitute a significant aspect of the structural framework and offer important insights for dividing rooms. Similar to interior wall centreline extraction, the ceiling planes are intersected with the vertical planes of the beams to obtain the beam lines, and the beam centrelines are then extracted from the two parallel beam lines, as shown by the red line in the bottom left box of Fig. 3 . Moreover, due to the occlusion caused by indoor furniture or walls, there is a high probability that the wall-beam centrelines will not form enclosed regions. To resolve this issue, each endpoint of these centrelines is extended to reach its nearest intersection line. The final extracted wall-beam centrelines are shown by red lines in Fig. 4 .
#### 3.1.3. Initial room segmentation
Next, the building layout space is segmented into different rooms based on the extracted wall-beam centrelines. To accomplish this task, only points above a certain height (0.5 m below the ceilings) are selected to mitigate the impact of noisy points. The points of walls and beams are then removed from the selected points based on the wall-beam lines. The points are subsequently projected into 2D grids ( Fig. 5 (a) ) to generate a binary image ( Fig. 5 (b)). In addition, a segment at the bottom left in Fig. 5 (a) is filtered out as an awning . The empty grids where no points hit in are coloured black, while the remaining grids are white. A watershed operation is then performed to segment the 2D building space into multiple regions ( Fig. 5 (c)). Finally, these regions are back-projected to the point cloud to complete the initial room segmentation ( Fig. 5 (d)).
#### 3.1.4. Segmentation optimization
In indoor scenes, point clouds could be over-segmented due to missing points caused by reflective lights on ceilings and occlusions. To address this issue, an optimization method is proposed utilizing wall-beam centrelines instead of directly using wall lines. In this method, wall-beam centrelines are extended to intersect with one another, constructing a complex graph G ’ (black lines in Fig. 6 (a)). This graph divides the 2D region of a building into a set of cells (1st to 35th patch in Fig. 6 (a)). The dual graph of G ’ , denoted as G V E , consists of nodes V representing cells, and edges E connecting these nodes. Then the optimization of the initial room segmentation transforms into a multi-label (i.e., room or not) optimization problem. A graph cut algorithm [ 36 ] is used for the optimization, with the energy function E l defined by an unary data term U v l v and a pairwise term B v , w l v l w on graph G V E . Here, l v , l w ∈ L o L c L 1 … L m are the labels of cells v and w , with possible labels including L o (area without points), L c (clutter), and L 1 , … , L m (each room in the initial segmentation). The minimization of energy cost is formulated as Eq. (1) : (1) E l = ∑ U v l v + ∑ B v , w l v l w → min
##### 3.1.4.1. Unary data term
Unary data term is designed to assess whether a specific label is assigned a particular cell. To calculate the unary data term, a cell v is divided into small grids with diverse labels ( Fig. 6 (b)). The ratio of the number of grids ∑ g l v with a certain label l v to the total number of grids ∑ g is then calculated as the probability of the cell belonging to that label. The unary data term is thereby formulated as Eq. (2) : (2) U v l v = − α ∙ area v ∙ ln ∑ g l v ∑ g where α is the weighting factor, area v is the area of a cell v for quantifying the importance of the cell.
##### 3.1.4.2. Pairwise smooth term
The pairwise term is designed to encourage each pair of two neighbouring cells to merge or split, based on the observation that two adjacent cells are likely to share the same label if there is no wall-beam centreline between them, and vice versa. For example, in Fig. 6 (c), cell 9 and 14 are inclined to possess the same label since there is no wall-beam centreline separating them, whereas cell 9 and 10 are expected to bear different labels due to the presence of a wall-beam centreline (red line) between them. Consequently, the energy cost for two adjacent cells without a wall-beam centreline between them should be high if they are assigned distinct labels, and low if they share the same label. The pairwise term is subsequently formulated as Eq. (3) : (3) B v , w l v l w = 0 , l v = l w β · e vw · 1 − e vw ∩ c b e vw , l v ≠ l w where β is the weighting factor, c b is wall-beam centerline , e vw is the separation edge of cell v and w , and e is the length of e which is used to penalize the effect of edge length. A graph cut algorithm [ 37 ] is implemented on G to minimize the energy function in Eq. (1) for the segmentation optimization. Rooms with insufficient walls (<4) are further filtered out. Each node in graph G is then assigned its corresponding label to complete the segmentation optimization. As depicted in Fig. 7 , the initial segmentation result (left side) exhibits over-segmentation (black circle) and under-segmentation (blue circle) due to occlusions and reflective lights on ceilings. The right side of Fig. 7 presents the optimized result, where the over- and under-segmentations are resolved, and the room boundaries are noticeably smoother.
### 3.2. Topological relation extraction
To preserve the consistency of walls and beams for model reconstruction, topological relations are extracted based on stringent geometric and topological rules inherent to indoor scenes, utilizing prior knowledge. As demonstrated in Fig. 8 , geometric regularization is first conducted, followed by the establishment of topological relationships between walls and beams.
#### 3.2.1. Geometric regularization
To mitigate the impact of noisy points and small deviations during the surface growing process in Section 3.1.2 , geometric regulations are introduced in this section to refine wall-beam lines, ensuring high consistency between the reconstructed model and its indoor scene. In indoor scenes, walls and beams typically adhere to specific structural rules, such as adjacent walls being orthogonal or collinear, and non-adjacent walls being parallel, orthogonal or collinear (beams likewise). These relationships can be mathematically formulated as Eq. (4) : (4) l i l j ∈ ort col , if l i ⊕ l j l i l j ∈ ort par col , if l i ⊖ l j where l i and l j are two wall-beam lines, *ort* , *par* and *col* are the geometric constraint relationships between them, which represent orthogonal, parallel and collinear respectively. ⊕ and ⊖ denote that the two lines are adjacent and non-adjacent respectively. Here, all wall-beam lines are represented by three parameters a i b i c i since a line in 2D space can be formulated as a i x + b i y + c i = 0 . Additionally, the angle between two lines is denoted as ang , and the distance as dist . Two lines are supposed to be orthogonal if ang − π / 2 < ε , parallel if ang < ε and dist > d , and collinear if ang < ε , and dist < d , where ε and d represent the predefined angle and distance thresholds. To perform geometric regularization, the longest wall line is selected as the reference l ref . Next, relations between l ref and all the other wall-beam lines are calculated. Subsequently, all other wall-beam lines are aligned to the reference line based on the geometric rule in Eq. (4) to refine ( a i , b i ) of each wall-beam line. Following this, the parameter c i of each wall-beam line is refined based on the least squares of plane fitting. Lastly, collinear lines are merged. Fig. 9 shows the comparison of wall-beam lines before and after geometric regulation. The black and red lines illustrate wall-beam lines before and after geometry regularization respectively.
#### 3.2.2. Topological relation construction
To precisely depict the topological relationships between different wall-beams, it is essential to obtain more accurate wall-beam centrelines than those extracted in Section 3.1.2 . Consequently, the process from Section 3.1.2 is repeated to extract more reliable wall-beam centrelines from the updated wall-beam lines. If two centrelines are considered collinear (i.e., their distance is less than the collinear distance threshold), the shorter centreline is aligned to the longer one. Subsequently, as common knowledge , a wall can be divided into two parts by an intersecting wall, but not by an intersecting beam, since a beam only intersects with the upper part of a wall. In contrast, a beam can be divided into two parts by either a wall or a beam. Based on this principle, the wall-beam centrelines are updated as shown in Fig. 10 (a), and the topological relations are constructed correspondingly, as shown in Fig. 10 (b). As shown in Fig. 10 (a), some centrelines do not intersect with other centrelines, resulting in missing connection relationships in Fig. 10 (b). For instance, wall centrelines 1, 11 and 14 are completely isolated from other centrelines, and 7 and 9 only partially intersect. To tackle this problem, a patch-growing method is introduced to find the corners of these centrelines to generate accurate topological relationships. Initially, each wall-beam centreline is extended if it separates multiple rooms, and its extension hits within one of its separated rooms. Additionally, the four boundary lines of each room are also extended to intersect with each other. Fig. 11 (a) illustrates the extended centrelines. Then, a Delaunay algorithm is employed to triangulate the eight endpoints and all the intersection points of wall-beam centrelines to obtain triangular patches as illustrated in Fig. 11 (b). Subsequently, labels are assigned to these patches based on the room segmentation result. As shown in Fig. 11 (c), each patch is given the label which covers the highest ratio of points. Next, a patch-growing algorithm is performed to merge different triangular patches based on the labels as depicted in Fig. 11 (d), and the result is shown in Fig. 11 (e). Compared to the centrelines extracted in room segmentation, wall-beam centrelines here are more precise, and the corners of wall-beam centrelines are all enclosed. After patch growing, the enclosed wall-beam centrelines and topological relations are generated as shown in Fig. 12 . In contrast to the initial topological relation in Fig. 10 (b), wall-beam centrelines are now connected to one another based on their topological relationships, reflecting a more accurate representation of real-world scenes in both geometry and topology.
### 3.3. Model reconstruction
The elements of interest for BIM reconstruction can be divided into two parts, building main frames including walls, beams, floors and ceilings, and components embedded in walls including doors, windows, and electrical panels (e.g., switches and sockets). In this section, models of building frames are first reconstructed based on the extracted topological relations. Subsequently, models of the components embedded in walls are reconstructed after they are detected from the recognized walls. Finally, the complete BIM of the building is obtained by merging all the individual reconstructed models.
#### 3.3.1. Main frame reconstruction
Building main frames are reconstructed in the order of walls, ceilings, floors, and beams, since the footprints of ceilings and floors are determined by the respective walls, and beams are attached to ceilings and walls. A wall can be defined by Eq. (5) , where x EW and x EW ′ are the centerline endpoints, and h W and t W represent the height and thickness of the wall, respectively. (5) P wall = x EW x EW ′ t W h W A floor can be defined by Eq. (6) as a sequence of footprints, x F 1 , x F 2 , … , x Fn and its thickness t f . Similarly, a ceiling can be defined by Eq. (7) as a sequence of vertices, x C 1 , x C 2 , … , x Cn and its thickness t C . The footprints of floors and the vertices of ceilings are obtained by intersecting the inner side of exterior walls. (6) P floor = x F 1 x F 2 … x Fn t f (7) P ceiling = x C 1 x C 2 … x Cn t C A beam can be defined similar to a wall by Eq. (8) , where x EB and x EB ′ are the centreline endpoints, and h B and t B represent the height and thickness of the beam, respectively. (8) P beam = x EB x EB ′ t B h B 3D models of building main frames can then be reconstructed following the above equations, as shown in Fig. 13 . The ceiling is removed for better visualization of the interior space.
#### 3.3.2. Reconstruction of components embedded in walls
##### 3.3.2.1. Detection
The components embedded in walls, including door and window openings, as well as panel openings for switches and sockets, need to be detected from the recognized wall point clouds in Section 3.1.1 before they are reconstructed. Doors and windows are detected based on their salient features in point clouds, as shown in Fig. 14 . An α-shape algorithm [ 37 ] is implemented to detect openings ( Fig. 14 (b)) from point clouds of walls ( Fig. 14 (a)). Subsequently, the openings are examined for the presence of frames ( Fig. 14 (c)). In other words, a valid door or window should be surrounded by segments with a number of points larger than 50, and the principal direction of a segment should be orthogonal to the principal direction of its neighbouring segment. Then, the openings are determined to be doors or windows based on manually designed rules (see Section 4.2.2 for details). Finally, these openings are fitted to bounding boxes to get their lengths and widths ( Fig. 14 (d)). A deep neural network is employed for extracting electrical panels (sockets and switches) from images, since it is challenging to extract them directly from point clouds due to their small sizes. Panorama images are taken from rooms and mapped to point clouds based on the intrinsic and extrinsic parameters of the camera and the scanner. Subsequently, the areas of interest corresponding to the walls are cropped from the images. Electrical panels are detected from these areas by leveraging a RetinaNet [ 38 ] with ResNet50 + FPN as the backbone. Fig. 15 illustrates the detection result of electrical panels in a panorama image. The 2D positions of electrical panels in the images are subsequently back-projected into the point clouds of walls. The pixel coordinates of the panorama u v are first converted into longitude and latitude coordinates θ φ (Eq. (9) ). Then, they are transformed into Cartesian coordinates x y z by considering the depth information r (Eq. (10) ). Finally, the scanner's pose information is utilized to map the coordinates into the world coordinate system X Y Z (Eq. (11) ). In these equations, w and h represent the width and height of the panorama image, t x t y t z is the scan position, and R − 1 denotes the rotation matrix to the scan coordinate system. (9) θ = u w ∗ 2 π − π φ = v h ∗ π − π 2 (10) x y z = r ∗ cos φ ∗ cos θ r ∗ cos φ ∗ sin θ r ∗ sin φ (11) X Y Z = R − 1 x − t x y − t y z − t z Fig. 16 shows the projected results. The length, height and centre point of each electrical panel are calculated based on the cropped electrical panels. Fig. 17 shows the detection result of in-wall components, in which red rectangles are doors, blue rectangles are windows, and purple rectangles are electrical panels.
##### 3.3.2.2. Reconstruction
Doors, windows and electrical panels are then represented using parametric models for reconstruction after they are detected. A door is represented by its centre point x CD , thickness t D , height h D and length l D , as shown in Eq. (12) . Similarly, a window is represented by its centre point x CW , thickness t W , height h W and length l W as shown in Eq. (13) . An electrical panel is represented using its centre point x CP , thickness t P , height h P and length l P as shown in Eq. (14) . t D , t W and t P are all defined as half the width of their embedded wall. Then Boolean operations are conducted between walls and in-wall components to form a complete, detailed model. (12) P door = x CD t D h D l D (13) P window = x CW t W h W l W (14) P panel = x CP t P h P l P In the final stage, all the component parameters in the model are exported to an XML file. To facilitate further manipulation and editing, a plugin has been developed for Autodesk Revit, which automatically constructs a BIM from the XML file. The resulting BIM, constructed within Autodesk Revit 2018, is shown in Fig. 18 . The default material properties of walls, beams, ceiling, floor and in-wall components are used since this study focuses on geometry modelling. The generated BIM can be employed for various applications, such as construction progress monitoring and energy simulation.
## 4. Experiment
The method is implemented in C++ using OpenCV [ 40 ] and the CGAL library [ 41 ], and tested on two datasets, ISPRS and WUT, which were collected with different building styles and point densities in two countries.
### 4.1. Datasets and evaluation metrics
#### 4.1.1. ISPRS datasets
The ISPRS dataset serves as a benchmark for indoor reconstruction tasks [ 42 ]. The dataset was collected by a Viametris iMS3D system in buildings of TUBraunschweig, Germany . Three point clouds, TUB1, TUB2 and TUB3, are used to evaluate our framework ( Fig. 19 ). TUB1 comprises 11 rooms enclosed by walls with different thicknesses on the same floor containing 7 windows and 23 doors. TUB2 contains 16 rooms, 8 windows and 23 doors. TUB3 consists of 9 rooms, 13 windows and 28 doors. Windows and doors in these three indoor scenes can be either open or closed. There are no panoramic images in the ISPRS dataset due to the lack of scanning information.
#### 4.1.2. WUT datasets
The WUT dataset was collected by a FARO Focus laser scanner, which consists of four scenes, WUT1, WUT2, WUT3 and WUT4, located at Wuhan University of Technology ( Fig. 20 ). WUT1 contains 6 rooms, 6 windows, 4 doors and 5 panels. WUT2 comprises 5 rooms, 6 windows and 3 doors. WUT3 consists of 6 rooms, 4 windows, 3 doors and 10 panels. WUT4 contains 4 rooms, 3 windows, 7 doors and 9 panels. All windows and doors are open. Compared to ISPRS, the WUT dataset features a smaller area and more complex indoor environments, including beam structures and electrical panel components. Additionally, each scene in WUT contains several panorama images exported from FARO SCENE software. The attributes of the ISPRS dataset and the WUT dataset are detailed in Table 1 . Table 1. The attributes oftest datasets.
#### 4.1.3. Evaluation metrics
The performance of room segmentation and BIM reconstruction are evaluated separately by using two groups of metrics which indicate the difference between the ground truth and test results, as detailed below.
##### 4.1.3.1. Room segmentation
Four metrics, IoU (intersection over union), precision, recall and F1 score, are adopted to evaluate the performance of room segmentation. The IoU, defined in Eq. (15) , measures the ratio of the intersection area between the predicted room and the ground truth to their union area. The precision, recall and F1 score are defined by Eqs. (16) , (17) , (18) . (15) IoU = Area of Intersection Area o f Union (16) precision = TP TP + FP (17) recall = TP TP + FN (18) F 1 score = TP TP + FP where TP is the number of points located in the intersection area between a predicted room and its corresponding ground truth. FP refers to the number of points within the predicted room but not present in its ground truth. FN is the number of points in the ground truth but not falling inside its predicted room area.
##### 4.1.3.2. Model reconstruction
Four metrics, RMSE (Root Mean Square Error), completeness, correctness and median distance, are used to evaluate the performance of model reconstruction. RMSE, as outlined in Eq. (19) , quantifies the quality of the reconstructed model by measuring the distance between a point and its nearest constructed surface. Completeness M CP , as defined in Eq. (20) , measures the extent to which the geometric elements within the ground truth model are reconstructed in the reconstructed model. Correctness M CR , as defined in Eq. (21) , measures the extent to which the geometric elements within the reconstructed model are present in the ground truth model. Median distance M D is calculated based on the geometric distance between the reconstructed surfaces and their ground truth model surfaces, as detailed in Eq. (22) . (19) RMSE = 1 m ∑ i = 1 m y i − f y i 2 (20) M CP R G b = ∑ i = 1 n r P R i G i b ∑ i = 1 n g A G i (21) M CR R G b = ∑ i = 1 n r P R i G i b ∑ i = 1 n r A R i (22) M D R G t r = Med D y i p j , ifD y i p j ≤ t r where m is the number of points, x i is a point, and f x i is the nearest point in x i ’s nearest plane of a reconstructed model. R i and G i are the surfaces of reconstructed model R and ground truth model G , respectively, n r and n g are the number of surfaces of R and G, respectively. P R i G i b means the overlapping area of the orthogonal projection of R i and its corresponding reference surface G i , of which distance is less than b . A G i means the area of surface G i , and A R i denotes the area of surface R i . t r is the cut-off distance, and D y i p j is the orthogonal distance between the vertex of the reconstructed model y i and its corresponding reference plane p j .
### 4.2. Experimental results
#### 4.2.1. Room segmentation
In the experiments, several empirical parameters are used for room segmentation and subsequent topological relation extraction. For semantic segmentation of the original point cloud, the 2D area of a floor is set to be larger than 2.0m 2 , and the area, width and length of a wall are set to be larger than 3.0m 2 , 2.5 m and 1.5 m, respectively. A segment is recognized as an awning and filtered out if its average height is at least 0.3 m lower than the ceiling height, and its 2D area is smaller than 10m 2 . For the extraction of wall-beam centrelines, wall-beam lines are regarded as parallel if the angle between them is <3°, and they will be merged into a single line if the distance between them is <0.025 m. Additionally, the interior and exterior wall widths are both set to 0.3 m based on the datasets. For room segmentation, the dimension of 2D grids for point projection is set to 0.03 m × 0.03 m. For geometric regularization, ε and d are set to 10° and 0.05 m, respectively. For topological relation construction, the collinear distance threshold is set to 0.05 m. The room segmentation results of our method are shown in Table 2 . On both the ISPRS and WUT datasets, our method consistently demonstrates outstanding performance, with all evaluation metrics exceeding 0.94. The exceptional performance of our framework on both the ISPRS and WUT datasets underscores its high stability. The precision of WUT2 is lower than of the other test scenes due to the presence of numerous noisy points in WUT2. Table 2. Evaluation results of the room segmentation.
A comparison is then conducted between our method and three other advanced methods: morphological segmentation (Mor), distance transformation-based segmentation (Dist), and Voronoi graph-based segmentation (Vor) [ 43 ]. Fig. 21 shows the qualitative comparison results of Mor, Dist, Vor and our method on TUB1 for room segmentation. In the results of Mor, Dist, and Vor, under-segmentations are observed, as indicated by the black circles in Fig. 21 . In contrast, the initial segmentation results of our proposed method effectively segment each individual room, although there is an over-segmentation in the long corridor (red circle in Fig. 21 ), and the room boundaries lack sufficient smoothness. Subsequently, the over-segmentation is eliminated and the room boundaries are smoothed, as evidenced by the optimized results (Ours-fin in Fig. 21 ). The quantitative comparison of the performance between our method and the three aforementioned methods is shown in Fig. 22 . Across all four evaluation metrics, our method consistently demonstrates superior performance over the other methods by considerable margins on all seven test scenes. The final results obtained using our method exhibit an average precision of 0.98, recall of 0.95, F1 score of 0.97 and IoU of 0.98. The higher precision and recall values indicate that the rooms segmented using our method are more accurate and comprehensive. Furthermore, the optimization strategy proposed in Section 3.1.4 significantly improves the recall of room segmentation, especially on the ISPRS dataset, as evidenced by the recall metric in Fig. 22 . The long corridors in the ISPRS dataset are easily to be over-segmented into multiple rooms, while the optimization effectively merges them back into single rooms.
#### 4.2.2. Model reconstruction
The experimental settings for model reconstruction are as follows. For the detection of openings, the value of α is set to 0.001 m by default for the α-shape algorithm. Additionally, doors and windows are identified if they meet the following criteria: (1) the 2D area of a door or a window typically exceeds 2m 2 , (2) a door must be positioned on the floor, and its height must be >2 m, (3) a window is at least 0.5 m above the floor, and its height is smaller in comparison to a door. The RetinaNet model for detecting sockets and switches is pretrained on the MS COCO dataset [ 39 ], and fine-tuned with 314 collected panorama images containing 3228 sockets and 1838 switches. The training parameters include 2 images per batch, a base learning rate of 0.005, a momentum of 0.9, a weight decay of 0.0005, and 20 epochs. The evaluation of geometric accuracy between the reconstructed model and input point cloud data is presented in Fig. 23 . To visualize the geometric accuracy , the distance between the reconstructed model and input point cloud data is divided into six ranges marked with different colours: [0, 3 mm] in green, (3 mm, 5 mm] in light green, (5 mm, 8 mm] in yellow, (8 mm, 10 mm] in orange, (10 mm, 12 mm] in red, and (12 mm, 15 mm] in dark red. The seven test scenes are colour-coded according to the six ranges, as shown in the upper graphs in Fig. 23 , and the distributions of distances are depicted as histograms in the lower graphs in Fig. 23 . As evidenced by Fig. 23 , the majority of points are within the first four ranges, indicating that the models reconstructed by our method are extremely close (<10 mm) to the input points. The RMSE values are 0.014 m, 0.017 m, 0.012 m, 0.008 m, 0.010 m, 0.007 m and 0.008 m respectively, which demonstrates that the accuracy of the reconstructed models can be up to the millimetre level. In the context of ISPRS and WUT datasets, the accuracy of reconstructed models is influenced by the data collection methods and quality. The ISPRS dataset, obtained through a mobile laser scanning system , exhibits lower accuracy compared to the WUT dataset, which utilizes a more precise laser scanning system. Additionally, the accuracy of models reconstructed from WUT2 is adversely affected by the presence of numerous noisy points, leading to a decrease in comparison to the other three WUT test scenes. Moreover, the precision and recall of in-wall components are evaluated in this study. Table 3 shows the evaluation of the precision rate and recall rate of each component. The recall of doors in TUB1 and TUB3 is low, as some doors were not open during data collection and thus were not detected. Some closed windows in TUB1 and TUB3 were not detected due to the same reason. Several electrical panels were not detected in WUT3 and WUT4 due to the occlusion by other objects in the panorama images. Additionally, the RMSE of in-wall components for the seven scenes are 0.026 m, 0.021 m, 0.019 m, 0.013 m, 0.015 m, 0.02 m and 0.017 m. Table 3. Precision, recall andRMSEof reconstructed in-wall components.
As shown in Fig. 24 , our method is further compared with three advanced methods, Tran and Khoshelham [ 26 ], Bassier and Vergauwen [ 14 ] and Ochmann et al. [ 6 ], using three metrics, completeness M CP , correctness M CR and median distance M D [ 44 ]. The buffer size for completeness and correctness, and the cut-off distance for median distance are both set to 10 cm. Our reconstructed BIM outperforms the other methods in terms of completeness ( Fig. 24 (a)), correctness ( Fig. 24 (b)) and median distance ( Fig. 24 (c)). This superior performance may be attributed to the geometry regularization and topological relation constraints. The higher completeness and correctness indicate that our model is more detailed and more precise. The smaller median distance suggests that our reconstructed BIM is much closer to the actual indoor scenes compared to the BIM reconstructed by the other three methods. Although the completeness of our proposed method in TUB1 (0.91) is slightly lower than that of Ochmann et al. (2019) [ 6 ] (0.93), this difference is due to the closed doors and windows in TUB1 that were not detected. Additionally, the lower completeness of TUB2 + TUB3 compared to TUB1 in our results is because that our method cannot handle stair components. The larger median distance of TUB2 + TUB3 compared to TUB1 in our results might be caused by the slanted ceilings in TUB2 + TUB3. Besides the quantitative evaluation , our method is also compared to Fang et al. [ 45 ] through visual inspection. Fig. 25 shows the comparison results between our method and Fang et al. [ 45 ] on TUB1, including the semantically segmented point cloud, the BIM generated by Fang et al. [ 45 ] and ours, and the ground truth. Compared to Fang et al. [ 45 ], our method is capable of reconstructing detailed BIMs including in-wall components such as windows and doors (purple and yellow circled areas in Fig. 25 ). Furthermore, our reconstructed BIM exhibits higher completeness than Fang et al. [ 45 ] owing to a more precise room segmentation as shown in the red, pink, black and cyan circles in Fig. 25 . The comparison results demonstrate the superior performance of our method in terms of both completeness and correctness. The final reconstructed BIMs of the seven test scenes are illustrated in Fig. 26 . The columns, from left to right, display the semantically segmented point clouds, segmented rooms, reconstructed BIMs, and ground truth BIMs. The rows, from top to bottom, present the results of scenes TUB1, TUB2, TUB3, WUT1, WUT2, WUT3 and WUT4. All reconstructed BIMs closely resemble their corresponding ground truths, reflecting their high accuracy. Additionally, our method demonstrates remarkable stability in the detailed BIM reconstruction for both building frames and components embedded in walls. Even when there are noisy points, our method is still able to reconstruct decent indoor BIMs. Another noteworthy aspect is that the elements of our reconstructed BIMs, such as walls, beams, and doors, are topologically enclosed and maintain close semantic and geometric relationships to the corresponding point clouds. This showcases their high consistency in semantic, geometric and topological relations. However, the reconstruction results for some data can still be improved to match the ground truth model. For instance, closed doors and windows are unable to be reconstructed in TUB1 to TUB3. Additionally, the stair components of TUB2, TUB3, and WUT2 cannot be handled by our method as well.
### 4.3. Discussion and limitations
In this section, the modules constituting our framework are discussed and the limitations of our method are introduced.
#### 4.3.1. Discussion
Compared to previously proposed method, our reconstructed as-built BIM achieves millimetre-level accuracy with high completeness and correctness. This can be attributed to the combination of the designed room segmentation, topological relation construction, and parametric BIM descriptor. These modules collectively form the prior knowledge that infers indoor BIMs from original point clouds. The prior knowledge ensures the consistency between reconstructed BIMs and real-world building indoor scenes in terms of geometry, semantics and topology, thereby providing a theoretical reference for detailed BIM reconstruction.
##### 4.3.1.1. Room segmentation optimization
In the room segmentation module, our optimization strategy significantly improves the initial room segmentation, as demonstrated in our experimental result ( Section 4.2.1 ). The initial room segmentation is prone to over-segmentation probably due to cluttered points, especially in indoor scenes with long corridors. Our designed optimization solves this problem by constructing an energy function based on generic prior knowledge obtained from daily observations, and minimizing it to encourage the merging of over-segmented room spaces. The significant improvement in performance validates the effectiveness of our room segmentation module.
##### 4.3.1.2. Topological relation construction
The topological relation construction greatly improves the accuracy of our model. Our designed structural rules in the topological relation construction are analogous to the Manhattan hypothesis in terms of orthogonal and parallel relations, with the addition of collinear relations to merge fragmented centrelines caused by occlusions. Besides, while the Manhattan hypothesis only imposes geometric constraints ensuring geometric consistency between the model and the indoor environment to a certain extent, our structural rules include three types of constraints: geometry, semantics, and topology. Geometric rules help to strictly restrict the spatial position of building structures, such as walls. Topological rules maintain the strict connectivity between each different building structure. Semantic rules ensure that each room model align with the room segmentation results. These three aspects guarantee that our model is highly consistent with the indoor environment, which is indispensable for achieving millimetre-level accuracy. Following topological relation constraints, the proposed method is capable of obtaining detailed and precise reconstructed BIMs with strictly enclosed building structures. The obtained BIMs can further be used to refine room segmentation, as illustrated in Fig. 27 . The original room segmentation result of WUT2 from Section 3.1.4 , illustrated in Fig. 27 (a) is refined in alignment with BIM after feedback, as shown in Fig. 27 (b). Within the black cropped area of Fig. 27 , the minor errors in the red circles in Fig. 27 (a) are rectified as in the red circles in Fig. 27 (b). In addition, a quantitative comparison of room segmentation before and after feedback from reconstructed BIM is presented in Fig. 28 . The four metrics consistently indicate that the performance of room segmentation has improved in accordance with our reconstructed BIMs.
#### 4.3.2. Limitations
The method proposed in this study relies on a series of rules and constraints for reconstructing the BIM model, which limits its applicability to various indoor environments. For example, the method assumes that floors and ceilings are flat, so it cannot reliably reconstruct slanted ceilings as the wall model is no longer a simple cube. To address this issue, an angle threshold (e.g., >15°) can be used to filter out sloping ceilings after the semantic segmentation stage, with trapezoidal walls under sloping ceilings being processed separately in the main frame reconstruction stage . Another limitation of the method is that it enforces the Manhattan assumption on the relationship between walls and beams, which may result in failure when dealing with curved or inclined walls. To overcome this challenge, it is proposed to loosen the geometric constraints appropriately, such as allowing the angle between walls to be a multiple of 15°, or detecting and fitting curved walls in advance based on the curvature of the point cloud patch. Furthermore, as the method is knowledge-driven, there are inevitably empirical parameters in the framework, such as the size parameters used for defining windows or doors, which may limit the application of the method. For solving this problem, the parameters are carefully selected from analysing a large quantity of projects, followed by making the parameter settings adjustable from the graphical user interface of the system. Lastly, the method cannot reconstruct pillars in the indoor environment, as they are small planes or curved surfaces that exceed the handling ability of the method.
## 5. Conclusions
This paper presents an automatic reconstruction framework for generating detailed BIM of complex indoor environments. Point clouds are segmented into different rooms by wall-beam line constraints and an optimization strategy . Topological relations between wall-beam lines are established to preserve consistency between realistic scenes and line models. Ultimately, building frames and in-wall components are reconstructed to accomplish indoor BIMs. Experimental results show the exceptional performance of our room segmentation and model reconstruction. The room segmentation accuracy and completeness are 0.98 and 0.88, respectively. The average accuracy of the obtained model reaches the millimetre level. In terms of correctness, completeness and median distance, our method outperforms other methods and represents the state-of-the-art. The experimental analysis validates the importance of each designed module in our framework. Our work holds great practical value in the field of architecture, engineering, construction and operation industry. Despite the remarkable performance of our framework, it still faces certain limitations. For instance, it is challenging for our work to reconstruct slanted ceilings, curved walls and building pillars due to the restricted structural rules. In our future work, we will focus on the BIM reconstruction of buildings with irregular shape elements, such as slanted ceilings and curved walls. For instance, our future work will combine generic rules with binary space partitioning techniques to deal with BIM reconstruction of irregular indoor scenes. In terms of future applications, our research can be applied to the quality control of building construction by comparing reconstructed BIMs of different temporals.
## Declaration of Competing Interest
We declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work, there is no professional or other personal interest of any nature or kind in any product, service and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled, “Knowledge-driven Inference for Automatic Reconstruction of Indoor Detailed As-built BIMs from Laser Scanning Data”.
## Acknowledgement
This research was funded by the National Natural Science Foundation of China (Grant No. 61901311 ). Recommended articles
## Data availability
Data will be made available on request.
## References
- [1]C. Gourguechon, H. Macher, T. LandesAutomation of as-built BIM creation from point cloud: an overview of research works focused on indoor environmentInt. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci., 43 (2022), pp. 193-200,10.5194/isprs-archives-XLIII-B2-2022-193-2022View in ScopusGoogle Scholar
- [2]Y.K. Juan, N.P. HsingBIM-based approach to simulate building adaptive performance and life cycle costs for an open building designAppl. Sci., 7 (8) (2017), p. 837,10.3390/app7080837View in ScopusGoogle Scholar
- [4]G. Pintore, C. Mura, F. Ganovelli, L. Fuentes-Perez, R. Pajarola, E. GobbettiState-of-the-art in automatic 3D reconstruction of structured indoor environmentsComput. Graph. Forum, 39 (2) (2020), pp. 667-699,10.1111/cgf.14021View in ScopusGoogle Scholar
- [5]Q. Wang, Y. Tan, Z. MeiComputational methods of acquisition and processing of 3D point cloud data for construction applicationsArch. Comput. Methods Eng., 27 (2020), pp. 479-499,10.1007/s11831-019-09320-4View in ScopusGoogle Scholar
- [8]Y. Cui, Q. Li, B. Yang, W. Xiao, C. Chen, Z. DongAutomatic 3-D reconstruction of indoor environment with mobile laser scanning point cloudsIEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12 (8) (2019), pp. 3117-3130,10.1109/JSTARS.2019.2918937View in ScopusGoogle Scholar
- [15]E. Grilli, F. Menna, F. RemondinoA review of point clouds segmentation and classification algorithmsInt. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci., 42 (2017), p. 339,10.5194/isprs-archives-XLII-2-W3-339-2017View in ScopusGoogle Scholar
- [16]R. Bormann, J. Hampp, M. HägeleNew brooms sweep clean-an autonomous robotic cleaning assistant for professional office cleaningProceedings of the IEEE International Conference on Robotics and Automation (2015), pp. 4470-4477,10.1109/ICRA.2015.7139818View in ScopusGoogle Scholar
- [17]L. Magri, A. FusielloReconstruction of interior walls from point cloud data with min-hashed J-linkageProceedings of the International Conference on 3D Vision (2018), pp. 131-139,10.1109/3DV.2018.00025View in ScopusGoogle Scholar
- [18]A. Elseicy, S. Nikoohemat, M. Peter, S.O. ElberinkSpace subdivision of indoor mobile laser scanning data based on the scanner trajectoryRemote Sens., 10 (11) (2018), p. 1815,10.3390/rs10111815View in ScopusGoogle Scholar
- [19]R. Ambruş, S. Claici, A. WendtAutomatic room segmentation from unstructured 3-D data of indoor environmentsIEEE Robot. Autom. Lett., 2 (2) (2017), pp. 749-756,10.1109/LRA.2017.2651939View in ScopusGoogle Scholar
- [20]E. Valero, D.D. Mohanty, M. Ceklarz, B. Tao, F. Bosché, G.I. Giannakis, S. Fenz, K. Katsigarakis, G.N. Lilis, D. RovasAn integrated scan-to-bim approach for buildings energy performance evaluation and retrofittingProceedings of the 38th International Symposium on Automation and Robotics in Construction (2021), pp. 204-211,10.22260/ISARC2021/0030View in ScopusGoogle Scholar
- [21]Z. Kang, J. Yang, Z. Yang, S. ChengA review of techniques for 3d reconstruction of indoor environmentsISPRS Int. J. Geo Inf., 9 (5) (2020), p. 330,10.3390/ijgi9050330Google Scholar
- [22]M. Previtali, L. Díaz-Vilariño, M. ScaioniIndoor building reconstruction from occluded point clouds using graph-cut and ray-tracingAppl. Sci., 8 (9) (2018), p. 1529,10.3390/app8091529View in ScopusGoogle Scholar
- [23]W. Shi, W. Ahmed, N. Li, W. Fan, H. Xiang, M. WangSemantic geometric modelling of unstructured indoor point cloudISPRS Int. J. Geo Inf., 8 (1) (2018), p. 9,10.3390/ijgi8010009View in ScopusGoogle Scholar
- [25]Y. Cai, L. FanAn efficient approach to automatic construction of 3D watertight geometry of buildings using point cloudsRemote Sens., 13 (10) (2021), p. 1947,10.3390/rs13101947View in ScopusGoogle Scholar
- [26]G. Lim, N. DohAutomatic reconstruction of multi-level indoor spaces from point cloud and trajectorySensors, 21 (10) (2021), p. 3493,10.3390/s21103493View in ScopusGoogle Scholar
- [27]H. Tran, K. KhoshelhamProcedural reconstruction of 3D indoor models from lidar data using reversible jump Markov chain Monte CarloRemote Sens., 12 (5) (2020), p. 838,10.3390/rs12050838View in ScopusGoogle Scholar
- [29]J. Du, D. Chen, R. Wang, J. Peethambaran, P.T. Mathiopoulos, L. Xie, T. YunA novel framework for 2.5-D building contouring from large-scale residential scenesIEEE Trans. Geosci. Remote Sens., 57 (6) (2019), pp. 4121-4145,10.1109/TGRS.2019.2901539View in ScopusGoogle Scholar
- [30]G. Pintore, E. Almansa, M. Agus, E. GobbettiDeep3dlayout: 3d reconstruction of an indoor layout from a spherical panoramic imageACM Trans. Graph., 40 (6) (2021), pp. 1-12,10.1145/3478513.3480480Google Scholar
- [32]Z. Pan, J. Hou, L. YuOptimization algorithm for high precision RGB-D dense point cloud 3D reconstruction in indoor unbounded extension areaMeas. Sci. Technol., 33 (5) (2022), Article 055402,10.1088/1361-6501/ac505bView in ScopusGoogle Scholar
- [33]H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, X. ZhouNeural 3d scene reconstruction with the manhattan-world assumptionProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2022), pp. 5511-5520,10.1109/CVPR52688.2022.00543View in ScopusGoogle Scholar
- [34]S. Tang, Y. Zhang, Y. Li, Z. Yuan, Y. Wang, X. Zhang, X. Li, Y. Zhang, R. Guo, W. WangFast and automatic reconstruction of semantically rich 3D indoor maps from low-quality RGB-D sequencesSensors, 19 (3) (2019), p. 533,10.3390/s19030533View in ScopusGoogle Scholar
- [35]G. Vosselman, B.G. Gorte, G. Sithole, T. RabbaniRecognising structure in laser scanner point cloudsInt. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci., 46 (8) (2004), pp. 33-38https://www.isprs.org/proceedings/XXXVI/8-W2/VOSSELMAN.pdfGoogle Scholar
- [36]Y. Boykov, V. KolmogorovAn experimental comparison of min-cut/max-flow algorithms for energy minimization in visionIEEE Trans. Pattern Anal. Mach. Intell., 26 (9) (2004), pp. 1124-1137,10.1109/TPAMI.2004.60View in ScopusGoogle Scholar
- [37]H. Edelsbrunner, E.P. MückeThree-dimensional alpha shapesACM Trans. Graph., 13 (1) (1994), pp. 43-72,10.1145/174462.156635View in ScopusGoogle Scholar
- [38]T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. DollárFocal loss for dense object detectionProceedings of the IEEE International Conference on Computer Vision (2017), pp. 2980-2988,10.1109/ICCV.2017.324View in ScopusGoogle Scholar
- [39]T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, C.L. ZitnickMicrosoft coco: common objects in contextProceedings of the European Conference on Computer Vision (2014), pp. 740-755,10.1007/978-3-319-10602-1_48View in ScopusGoogle Scholar
- [40]G. Bradski, A. KaehlerLearning OpenCV: Computer Vision with the OpenCV LibraryO'Reilly Media, Inc. (2008)ISBN: 9780596516130Google Scholar
- [41]A. Fabri, S. PionCGAL: the computational geometry algorithms libraryProceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (2009), pp. 538-539,10.1145/1653771.1653865View in ScopusGoogle Scholar
- [42]K. Khoshelham, L.D. Vilariño, M. Peter, Z. Kang, D. AcharyaThe ISPRS benchmark on indoor modellingInt. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci., 42 (2) (2017), p. W7,10.5194/isprs-archives-XLII-2-W7-367-2017Google Scholar
- [43]R. Bormann, F. Jordan, W. Li, J. Hampp, M. HägeleRoom segmentation: Survey, implementation, and analysisProceedings of the IEEE International Conference on Robotics and Automation (2016), pp. 1019-1026,10.1109/ICRA.2016.7487234View in ScopusGoogle Scholar
- Dataset and benchmark for as-built BIM reconstruction from real-world point cloud2025, Automation in ConstructionShow abstractAs-built BIM reconstruction plays a significant role in urban renewal and building digitization but currently faces challenges of low efficiency. Scan-to-BIM aims to improve reconstruction efficiency but lacks domain-specific, large-scale datasets and accurate, multi-dimensional benchmark metrics. These deficiencies further impede the evaluation and training of scan-to-BIM methods. To address these challenges, this paper proposes BIMNet, an IFC-based large-scale point cloud to BIM dataset, and a set of metrics that reflect the quality and issues of reconstructed models from both geometric and topological perspectives. Experiments demonstrate that BIMNet enhances the evaluation and training of scan-to-BIM methods during the critical processes of reconstruction and segmentation. This research contributes to the data foundation and metric system for deep-learning based scan-to-BIM methods. In the future, BIMNet will not only facilitate the development of scan-to-BIM but also contribute to the advancement of smart cities and AI-driven technologies beyond scan-to-BIM.
- Development of immersive bridge digital twin platform to facilitate bridge damage assessment and asset model updates2025, Computers in IndustryCitation Excerpt :Following the Scan-to-BIM methodology, the steps of constructing and updating the BIM reality model of the bridge are described below. Step 1: Scanning: The first step of the Scan-to-BIM procedure involves the precise surveying of the bridge using a specialized laser scanner as described in Section 4.4 (Xiong et al., 2023). The laser scan emits laser beams and accurately captures the reflected signals, enabling the accurate determination of distances and spatial coordinates of surface points.Show abstractConventional infrastructure asset management practices have heavily relied on static data collection and suffered from decision lags. Though advanced Structural Health Monitoring (SHM) systems were extensively explored based on multi-functional sensor deployment, asset model updating has not been achieved to facilitate timely and effective decision-making of infrastructure managers due to a lack of system integration. To address this challenge, this study develops the Immersive Bridge Digital Twin Platform (IBDTP) to allow infrastructure managers to automate the SHM processes of bridges and engage them in immersive decision-making processes based on Scan-to-BIM and Augmented Reality (AR) technologies. A novel 3D game engine is proposed as part of IBDTP and was tested using a single-span concrete arch bridge located in Poland. Results show that the measurement data collected and presented in IBDTP improves the infrastructure managers' accessibility to major damage data of the bridge to plan for future interventions. The functions of the IBDTP can be potentially scaled for different types of bridges and critical infrastructure, substantially improving the traditional SHM in terms of data management and 3D structural visualization.
- Automated data-driven method for creating digital building models from dense point clouds and images through semantic segmentation and parametric model fitting2024, Advanced Engineering InformaticsCitation Excerpt :In [14], the authors proposed a supervised region-growing method for segmenting unstructured point clouds using geometric features like surface roughness and curvature. In [15], the authors proposed a Knowledge-driven method that first segments the point cloud into five classes (including ceilings, walls, floors, beams, and clutter) using a surface-growing algorithm. The wall-beam center lines are then extracted to partition the building layout space into individual rooms.Show abstractThis paper proposes an automated method for creating semantic digital building models using dense point clouds and images. The method employs a hybrid bottom-up, top-down approach, integrating artificial intelligence capabilities in scene understanding with domain engineering knowledge to overcome challenges in indoor 3D reconstruction. The pre-trained PointTransformer semantic segmentation model extracts thirteen building objects, where the wall and ceiling segments are utilized in a 3D space parsing algorithm. The parameterized floor plan map is then generated using a data-driven approach, enabling the creation of an extruded volumetric digital model. Additionally, the YOLOV8object detection network recognizes doors and windows in images derived from projected points of the wall instances. The validation results for six building datasets with different layouts showcase the effectiveness of the proposed model reconstruction algorithm, with a mean error of about 7 cm between the parameters of elements in digital reference models and reconstructed models. This highlights AI’s potential in automating the creation of digital models for the real world.
- Indoor functional subspace division from point clouds based on graph neural network2024, International Journal of Applied Earth Observation and GeoinformationCitation Excerpt :How to divide a story space into multiple single rooms is a research hotspot. The commonly used methods are mainly plane-based (Jung et al., 2018; Wu et al., 2021; Cheng et al., 2021; Fang et al.,2021), volume-based (Cui et al., 2019; Ochmann et al., 2019; Nikoohemat et al., 2020; Xiong et al., 2023), and deep learning-based (Liu et al., 2018; Wang et al., 2018; Tang et al.,2022). Jung et al. (2018) and Cheng et al. (2021) both projected raw data into two dimensions and transformed it into a binary image to extract line primitives.Show abstractIndoor scenes are closely related to human life and contain rich geometric and semantic information. Dividing indoor spaces from data is crucial for multiple applications, such as navigation and digital twins. However, achieving this task is challenging. Traditional indoor space division methods only represent buildings, floors, and rooms to some extent, lacking semantic descriptions of indoor elements and their spatial arrangements. To divide an indoor space more finely, a novel indoor space subdivision method is proposed in this paper. Our method leverages the flexible space subdivision framework (FSS) to categorize indoor space into free navigation subspace, object subspace, and functional subspace. To define functional subspaces, we present a taxonomy for the spatial layout patterns of common indoor elements (like tables, chairs, ceilings, walls, floors, etc.). Then, we introduce scene graphs to represent indoor 3D scenes, where each node represents an indoor element and each edge encodes the spatial relationship between the elements. Finally, a node classification network is proposed to segment indoor scene into subspaces and predicts their (semantic) functions. We select 9 buildings of Matterport3D and 6 areas in S3DIS and merge them to form our dataset for training and testing our method. Experiments yield good results with up to 90.42% accuracy and 85.28% F1-scores in overall space subdivision. Moreover, compared with the various graph node classification networks, our method has achieved the best performance in indoor space subdivisions.
- 3D reconstruction of building interiors based on scan-to-BIM and generative design for as-built building2024, Engineering Construction and Architectural Management
- Development of an immersive digital twin framework to support infrastructure management: a case study of bridge asset health monitoring2024, Proceedings of the International Symposium on Automation and Robotics in Construction
View all citing articles on Scopus View Abstract © 2023 Elsevier B.V. All rights reserved.


## References
