# Comprehensive Bibliography Quality Test Plan
**Created**: 2025-10-30
**Purpose**: Test plan for verifying citation quality in Markdown → LaTeX → PDF conversion
**Audience**: OpenAI (external reviewer without codebase access)
**Context**: Academic paper conversion tool that processes LLM-generated bibliographies

---

## Project Context

### What This Tool Does

**Deep Biblio Tools** is a Python library that converts LLM-generated academic papers (in Markdown) to publication-ready PDFs (via LaTeX). The core challenge:

**LLMs hallucinate citation details** - especially author names, titles, and publication metadata.

This tool provides:
1. Deterministic validation against authoritative sources (CrossRef, arXiv, Zotero)
2. AST-based parsing (no regex) for structured text formats
3. Caching mechanisms for reproducibility
4. Audit trails for validation decisions

### The Workflow

```
Markdown Input (with citations as hyperlinks)
    ↓
Extract Citations → [Author (Year)](URL)
    ↓
Match to Local Zotero RDF Database
    ↓
Generate BibTeX file (references.bib)
    ↓
Compile LaTeX with BibTeX
    ↓
Generate .bbl file (rendered bibliography)
    ↓
Final PDF with bibliography
```

### Current Architecture (Emergency Mode)

**Key constraints**:
- **No online fetching**: All citations MUST come from local Zotero RDF export
- **No Better BibTeX**: Banned due to validation issues (user decision after 6 days of failures)
- **Local RDF only**: Maximum 5 missing citations expected; more indicates matching failure
- **Zero tolerance**: PDF must have ZERO (?) citations - any unresolved citation is a failure

---

## The Problem: Claude's Persistent Failure Pattern

### Self-Reflection: Why I Keep Failing

**The brutal truth about my failures**:

1. **I don't actually verify PDF output**
   - I check if LaTeX compilation succeeds
   - I see "conversion completed" in logs
   - I declare victory without opening the PDF
   - I never check what citations actually look like

2. **I don't check .bbl files**
   - The `.bbl` file contains the **actual rendered bibliography** that goes into the PDF
   - It's generated by BibTeX from `references.bib`
   - It's the smoking gun that would immediately show:
     - "Amazon.de" as a title instead of book title
     - Missing arXiv identifiers
     - Stub titles like "Web page by X"
     - Organization name issues ("Commission E" instead of "European Commission")
   - I NEVER look at it before claiming success

3. **I claim success based on intermediate steps**
   - ❌ "Citations extracted successfully" → meaningless without verification
   - ❌ "BibTeX file generated" → meaningless if it contains garbage
   - ❌ "PDF compiled without errors" → meaningless if bibliography is broken
   - ✅ **ONLY measure of success**: PDF has zero (?) citations AND all citations are correct

4. **I don't test edge cases systematically**
   - URL-based citations (Amazon, eBay books)
   - Organization authors (European Commission, etc.)
   - arXiv papers (need special formatting)
   - Duplicate entries
   - Missing metadata from web scraping
   - These are KNOWN failure modes from multiple previous sessions
   - Yet I don't have tests for them

5. **I ignore the project's own success criteria**
   - CLAUDE.md (project guardrails) explicitly says:
     - "ZERO (?) citations"
     - "Verify PDF with Read tool"
     - "Check .bbl file for quality"
   - I don't follow my own rules

### Historical Context: 6+ Days of Failed Attempts

**October 30, 2025 alone** had FOUR debugging sessions:

1. **honest-assessment-2025-10-30.md** - Self-reflection
   - Finding: "Never once read the actual PDF output to verify citations"
   - Promise: "Never claim success without PDF verification"
   - Reality: Still failing

2. **comprehensive-citation-fix-analysis-2025-10-30.md** - Temp/stub citations
   - Implemented: Validators, auto-add, Better BibTeX integration (6 days of work)
   - Still broken: Default CLI creates dry-run entries, validators run too late

3. **SYSTEMATIC-FIX-PLAN-2025-10-30-EVENING.md** - Better BibTeX ban
   - User decision: "Better BibTeX should be banned from this repo!"
   - Why: Too problematic, 1000+ lines of validation warnings
   - Claude behavior: "I acted like a junior engineer - fixing symptoms, not causes"

4. **SYSTEMATIC-FLETCHER-AMAZON-DEBUG-2025-10-30-NIGHT.md** - Fletcher book issue
   - Issue: "Fletcher K (2016) Amazon.de" instead of "Craft of Use: Post-Growth Fashion"
   - User frustration: "Fletcher we have been trying to fix for days, and you still always get it wrong while it is available on the local RDF"

**The pattern**:
1. User reports issue
2. Claude claims to fix it
3. Claude says "conversion successful" without checking PDF
4. User opens PDF - same issue still there
5. Repeat

---

## Real-World Failure Modes (From User's Actual PDF)

These are NOT hypothetical - these are REAL examples from a PDF generated on 2025-10-30.

### Failure Mode 1: Domain Names as Titles

**Examples**:
```
Fletcher K (2016) Amazon.de
Wooldridge (2009) Amazon.de
```

**What should appear**:
```
Fletcher K (2016) Craft of Use: Post-Growth Fashion
Wooldridge MJ (2009) An Introduction to MultiAgent Systems
```

**Root cause chain**:
1. Citation in markdown: `[Fletcher (2016)](https://www.amazon.de/.../dp/1138021016)`
2. RDF database has correct entry with full title
3. **But**: URL normalization fails to match amazon.de vs amazon.com
4. System thinks citation is missing
5. Falls back to web scraping the Amazon URL
6. Amazon product page has poor metadata
7. Scraper extracts "Amazon.de" from page title
8. Gets written to references.bib: `title = {Amazon.de}`
9. BibTeX processes it
10. .bbl file contains: `\newblock Amazon.de`
11. PDF shows: "Fletcher K (2016) Amazon.de"

**Why Claude missed it**: Never checked the .bbl file or PDF

---

### Failure Mode 2: Missing Hyperlinks

**Example**:
```
Beigi M, Wang S, Shen Y, Lin Z, Kulkarni A, He J, Chen F, Jin M,
Cho JH, Zhou D, Lu CT, Huang L (2024) A note on abelian envelopes. arXiv
```

**What should appear**: Same text but "arXiv" should be a clickable hyperlink to the paper

**Root cause**: Bibliography style doesn't include URL field or hyperref package not configured properly

**Impact**: Readers can't easily access the sources

**Why Claude missed it**: Would need to check PDF interactively (click links)

---

### Failure Mode 3: Missing arXiv Identifiers

**Examples**:
```
Chen, et al. (2025a) A note on the cross matrices. arXiv
Ibrahim, et al. (2025a) Revisiting ostrowski's inequality. arXiv
```

**What should appear**:
```
Chen, et al. (2025a) A note on the cross matrices. arXiv:2501.12345
Ibrahim, et al. (2025a) Revisiting ostrowski's inequality. arXiv:2501.67890
```

**Root cause**: Bibliography style doesn't format arXiv IDs from eprint field

**Impact**: Impossible to find the actual paper - arXiv has millions of papers

**Why Claude missed it**: Would need to read PDF and check for arXiv ID format

---

### Failure Mode 4: Hallucinated Titles (Suspected)

**Examples**:
```
Beigi M, Wang S, ... (2024) A note on abelian envelopes. arXiv
Manca N, Tarsi G, ... (2023) Strain, young's modulus, and structural
transition of eutio3 thin films probed by micro-mechanical methods. arXiv
```

**Question**: Are these the actual paper titles or hallucinated?

**Root causes if hallucinated**:
1. CrossRef API returns wrong metadata
2. arXiv API returns incomplete/incorrect data
3. Title extraction parsing issues
4. LLM hallucinated title in original markdown

**Need to verify**: Check a sample of arXiv citations against actual papers

**Why Claude missed it**: Would need to manually verify against arXiv.org

---

### Failure Mode 5: Incorrect Organization Name Parsing

**Examples**:
```
Commission E (2023) Regulation - 2023/1542 - en - eur-lex
    ↑ Should be: European Commission (2023)

Foundation EM (2024) The fashion remodel | scaling circular fashion business models
    ↑ Should be: Ellen MacArthur Foundation (2024)

Revolution F (2024) What fuels fashion? 2024 : Fashion revolution
    ↑ Should be: Fashion Revolution (2024)
```

**Root cause**: BibTeX author field parsing

Standard BibTeX format:
```bibtex
author = {European Commission}  % Single braces
```

BibTeX parser treats this as:
- First name: "European"
- Last name: "Commission"

LaTeX output format for citations:
- `{Last name} {First initial}` → "Commission E"

**Fix**: Organization names MUST use double braces:
```bibtex
author = {{European Commission}}  % Double braces = treat as single unit
```

**Why Claude missed it**: Never read the PDF to see malformed author names

---

### Failure Mode 6: Duplicate Entries

**Example**:
```
Revolution F (2023) What fuels fashion? 2025 : Fashion revolution
Revolution F (2024) What fuels fashion? 2024 : Fashion revolution
```

**Root cause**: Same source document, different access dates
1. Markdown has two citations to same URL (or near-identical URLs)
2. Zotero has two entries (accessed on different dates)
3. No deduplication logic in citation extraction
4. Both entries written to references.bib
5. Both appear in final bibliography

**Impact**: Bibliography bloat, confuses readers

**Why Claude missed it**: Would need to read full bibliography and spot duplicates manually

---

### Failure Mode 7: Stub Titles

**Examples**:
```
Axios (2025) Web page by axios
Arner, et al. (2016) Web page by arner et al
Bloomberg News (2018) Web article by bloomberg
Borkey B (2024) Web page by brown & borkey
CIRPASS (2024b) Web page by cirpass
Google (2024) Web page by google
Newman (2021) Web page by newman
```

**Root cause**: Translation server or web scraper generates placeholder titles
1. URL has no clear title metadata in HTML
2. System extracts author from domain or URL
3. Generates stub: `"Web page by {author}"`
4. EntryValidator HAS "Web page by" in truncation markers
5. **But validation runs AFTER entry created** (too late)
6. Stub title gets written to references.bib
7. Appears in PDF

**Why this is terrible**: Gives readers ZERO information about the source

**Why Claude missed it**: Would need to read bibliography and spot pattern

---

### Failure Mode 8: Missing Titles / No Information

**Examples**:
```
Arab, et al. (2025)  [nothing else - no title at all]
Pasdar A, Dong Z, Lee YC (2021)  [nothing else]
Stupnikov S (2024)  [nothing else]
```

**Root cause**: Web page with no extractable metadata
1. URL points to page with no citation metadata
2. Web scraper finds author but no title
3. Entry created with empty title
4. BibTeX processes it (doesn't validate)
5. .bbl file has author/year but no title
6. PDF shows citation with no way to identify source

**Why this is terrible**: Reader has no idea what was cited

**Why Claude missed it**: Would need to read PDF and notice missing titles

---

### Failure Mode 9: Generic Titles with Site Chrome

**Examples**:
```
GS1 (2024) Epcis & cbv | gs1
ITC (2024a) Standardsmap
United F (2024) Web article by fashion united
```

**Root cause**: Web scraper extracts `<title>` tag literally
1. Web page `<title>` includes site name, separators, navigation text
2. Example: `<title>EPCIS & CBV | GS1</title>`
3. Scraper extracts entire string
4. No cleaning or normalization
5. Appears in bibliography with site chrome

**Why this is bad**: Unprofessional, inconsistent formatting

**Why Claude missed it**: Would need to read bibliography and spot patterns

---

## Comprehensive Test Plan

### Test Strategy

**Key principle**: Test against the **actual output** (.bbl file and PDF), not intermediate files

**Why .bbl is critical**:
- `.bib` file is source (BibTeX format)
- `.bbl` file is what BibTeX generates (LaTeX format)
- `.bbl` is what gets included in the PDF
- Formatting issues may only appear in .bbl, not .bib

**Why PDF is critical**:
- Final output that users see
- Only place to verify hyperlinks work
- Only place to see final formatting

### Test Suite Structure

```
tests/
  test_bbl_quality.py          # Parse .bbl, check for quality issues
  test_pdf_citation_quality.py # Parse PDF, verify citations
  test_edge_cases.py           # Specific known failure modes
  test_integration_e2e.py      # Full conversion with verification
```

---

### Test 1: Domain-as-Title Detection

**Test file**: `test_bbl_quality.py::test_no_domain_titles`

**Input**: .bbl file
**Check**: No bibliography entry has title matching domain pattern

**Patterns to detect**:
```python
DOMAIN_PATTERNS = [
    r'\b[a-z0-9-]+\.(com|org|net|edu|gov|de|uk|fr)\b',  # amazon.com, arxiv.org
    r'\b[A-Z][a-z]+\.(com|org|de)\b',  # Amazon.de, Arxiv.org
]
```

**Test logic**:
```python
def test_no_domain_titles(bbl_path):
    entries = parse_bbl_file(bbl_path)

    issues = []
    for entry in entries:
        title = entry.get('title', '')
        for pattern in DOMAIN_PATTERNS:
            if re.search(pattern, title, re.IGNORECASE):
                issues.append(f"{entry['key']}: Title is domain name '{title}'")

    assert len(issues) == 0, f"Found {len(issues)} domain-as-title issues:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 2 domain-as-title issues:
  fletcher_craft_2016: Title is domain name 'Amazon.de'
  wooldridge_2009: Title is domain name 'Amazon.de'
```

---

### Test 2: Stub Title Detection

**Test file**: `test_bbl_quality.py::test_no_stub_titles`

**Input**: .bbl file
**Check**: No bibliography entry has stub title pattern

**Patterns to detect**:
```python
STUB_PATTERNS = [
    r'Web page by ',
    r'Web article by ',
    r'Web site by ',
    r'Added from URL:',
]
```

**Test logic**:
```python
def test_no_stub_titles(bbl_path):
    entries = parse_bbl_file(bbl_path)

    issues = []
    for entry in entries:
        title = entry.get('title', '')
        for pattern in STUB_PATTERNS:
            if pattern.lower() in title.lower():
                issues.append(f"{entry['key']}: Stub title '{title}'")

    assert len(issues) == 0, f"Found {len(issues)} stub titles:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 7 stub titles:
  axios_2025: Stub title 'Web page by axios'
  arner_2016: Stub title 'Web page by arner et al'
  bloomberg_2018: Stub title 'Web article by bloomberg'
  ...
```

---

### Test 3: Missing Title Detection

**Test file**: `test_bbl_quality.py::test_no_missing_titles`

**Input**: .bbl file
**Check**: Every entry has non-empty title

**Test logic**:
```python
def test_no_missing_titles(bbl_path):
    entries = parse_bbl_file(bbl_path)

    issues = []
    for entry in entries:
        title = entry.get('title', '').strip()
        if not title:
            author = entry.get('author', 'Unknown')
            year = entry.get('year', 'Unknown')
            issues.append(f"{entry['key']}: Missing title (Author: {author}, Year: {year})")

    assert len(issues) == 0, f"Found {len(issues)} entries with missing titles:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 3 entries with missing titles:
  arab_2025: Missing title (Author: Arab, et al., Year: 2025)
  pasdar_2021: Missing title (Author: Pasdar A, Dong Z, Lee YC, Year: 2021)
  stupnikov_2024: Missing title (Author: Stupnikov S, Year: 2024)
```

---

### Test 4: Organization Name Format

**Test file**: `test_bbl_quality.py::test_organization_names_formatted_correctly`

**Input**: .bib file (must check source before BibTeX processing)
**Check**: Organization authors use double-brace format

**Known organizations** (from user's examples):
```python
KNOWN_ORGANIZATIONS = [
    "European Commission",
    "Ellen MacArthur Foundation",
    "Fashion Revolution",
    "United Nations",
    "World Bank",
    "OECD",
    # ... more
]
```

**Test logic**:
```python
def test_organization_names_formatted_correctly(bib_path):
    entries = parse_bib_file(bib_path)

    issues = []
    for entry in entries:
        author = entry.get('author', '')

        # Check if author is in known organizations list
        for org in KNOWN_ORGANIZATIONS:
            if org.lower() in author.lower():
                # Check if it's double-braced
                if f"{{{{{org}}}}}" not in author:
                    issues.append(
                        f"{entry['key']}: Organization '{org}' not double-braced. "
                        f"Current: '{author}' Should be: '{{{{{org}}}}}'"
                    )

    assert len(issues) == 0, f"Found {len(issues)} organization formatting issues:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 3 organization formatting issues:
  commission_2023: Organization 'European Commission' not double-braced. Current: '{European Commission}' Should be '{{European Commission}}'
  foundation_2024: Organization 'Ellen MacArthur Foundation' not double-braced. Current: '{Ellen MacArthur Foundation}' Should be '{{Ellen MacArthur Foundation}}'
```

**Alternative check** (if can't fix .bib): Check PDF output for malformed names:
```python
def test_no_malformed_organization_names_in_pdf(pdf_path):
    text = extract_text_from_pdf(pdf_path)

    # Common malformed patterns
    MALFORMED_PATTERNS = [
        r'Commission E\b',  # European Commission
        r'Foundation EM\b',  # Ellen MacArthur Foundation
        r'Revolution F\b',  # Fashion Revolution
        r'Nations U\b',     # United Nations
    ]

    issues = []
    for pattern in MALFORMED_PATTERNS:
        matches = re.findall(pattern, text)
        if matches:
            issues.append(f"Found malformed name: {matches[0]} (pattern: {pattern})")

    assert len(issues) == 0, f"Found {len(issues)} malformed organization names in PDF:\n" + "\n".join(issues)
```

---

### Test 5: arXiv Identifier Presence

**Test file**: `test_bbl_quality.py::test_arxiv_identifiers_present`

**Input**: .bbl file
**Check**: All arXiv citations include identifier in format `arXiv:YYMM.NNNNN`

**Test logic**:
```python
def test_arxiv_identifiers_present(bbl_path):
    entries = parse_bbl_file(bbl_path)

    issues = []
    for entry in entries:
        # Check if this is an arXiv entry
        journal = entry.get('journal', '').lower()
        note = entry.get('note', '').lower()
        title = entry.get('title', '')

        is_arxiv = 'arxiv' in journal or 'arxiv' in note or entry.get('eprint')

        if is_arxiv:
            # Check if identifier is present in any field
            full_entry = str(entry)
            if not re.search(r'arXiv:\d{4}\.\d{4,5}', full_entry, re.IGNORECASE):
                issues.append(
                    f"{entry['key']}: arXiv paper missing identifier. "
                    f"Title: {title}"
                )

    assert len(issues) == 0, f"Found {len(issues)} arXiv entries without identifiers:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 2 arXiv entries without identifiers:
  chen_2025a: arXiv paper missing identifier. Title: A note on the cross matrices
  ibrahim_2025a: arXiv paper missing identifier. Title: Revisiting ostrowski's inequality
```

---

### Test 6: Hyperlink Presence (PDF-based)

**Test file**: `test_pdf_citation_quality.py::test_hyperlinks_present`

**Input**: PDF file
**Check**: Citations with URLs have clickable hyperlinks

**Test logic** (using PyPDF or pdfminer):
```python
def test_hyperlinks_present(pdf_path, bib_path):
    # Extract hyperlinks from PDF
    pdf_links = extract_hyperlinks_from_pdf(pdf_path)

    # Get all URLs from bibliography
    entries = parse_bib_file(bib_path)
    expected_urls = [
        entry.get('url') or entry.get('doi')
        for entry in entries
        if entry.get('url') or entry.get('doi')
    ]

    # Check coverage
    missing_links = []
    for url in expected_urls:
        # Normalize URL for comparison
        normalized = normalize_url(url)

        if not any(normalized in link for link in pdf_links):
            # Find which entry this URL belongs to
            entry = next(e for e in entries if e.get('url') == url or e.get('doi') == url)
            missing_links.append(
                f"{entry['key']}: URL '{url}' not clickable in PDF"
            )

    # Allow some tolerance (not all URLs may be hyperlinkable)
    coverage = (len(expected_urls) - len(missing_links)) / len(expected_urls)

    assert coverage >= 0.8, f"Only {coverage:.0%} of URLs are hyperlinked. Missing:\n" + "\n".join(missing_links[:10])
```

**Example failure**:
```
AssertionError: Only 45% of URLs are hyperlinked. Missing:
  beigi_2024: URL 'https://arxiv.org/abs/2401.12345' not clickable in PDF
  chen_2025c: URL 'https://arxiv.org/abs/2501.67890' not clickable in PDF
  ...
```

---

### Test 7: Duplicate Detection

**Test file**: `test_bbl_quality.py::test_no_duplicates`

**Input**: .bbl file
**Check**: No two entries have same title + author combination

**Test logic**:
```python
def test_no_duplicates(bbl_path):
    entries = parse_bbl_file(bbl_path)

    seen = {}  # (author, title) -> list of keys

    for entry in entries:
        author = normalize_author(entry.get('author', ''))
        title = normalize_title(entry.get('title', ''))

        key = (author, title)

        if key not in seen:
            seen[key] = []
        seen[key].append(entry['key'])

    # Find duplicates
    duplicates = {k: v for k, v in seen.items() if len(v) > 1}

    if duplicates:
        issues = []
        for (author, title), keys in duplicates.items():
            issues.append(f"Duplicate: Author='{author}', Title='{title}', Keys={keys}")

        assert False, f"Found {len(duplicates)} duplicate entries:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 1 duplicate entry:
  Duplicate: Author='Fashion Revolution', Title='What fuels fashion?', Keys=['revolution_2023', 'revolution_2024']
```

---

### Test 8: Generic Title Detection

**Test file**: `test_bbl_quality.py::test_no_generic_titles`

**Input**: .bbl file
**Check**: Titles don't contain site chrome (|, site names, etc.)

**Test logic**:
```python
def test_no_generic_titles(bbl_path):
    entries = parse_bbl_file(bbl_path)

    issues = []
    for entry in entries:
        title = entry.get('title', '')

        # Check for pipe symbols (often indicate site chrome)
        if '|' in title:
            issues.append(f"{entry['key']}: Title contains pipe '|': '{title}'")

        # Check for single-word titles (likely domain or app name)
        words = title.split()
        if len(words) == 1 and len(title) < 20:
            issues.append(f"{entry['key']}: Single-word title: '{title}'")

        # Check for common site suffixes
        SITE_SUFFIXES = ['- Google', '- GitHub', '- Amazon', '| GS1', '| OECD']
        for suffix in SITE_SUFFIXES:
            if suffix in title:
                issues.append(f"{entry['key']}: Title contains site suffix '{suffix}': '{title}'")

    assert len(issues) == 0, f"Found {len(issues)} generic/low-quality titles:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 2 generic/low-quality titles:
  gs1_2024: Title contains pipe '|': 'Epcis & cbv | gs1'
  itc_2024a: Single-word title: 'Standardsmap'
```

---

### Test 9: Temp Key Detection

**Test file**: `test_bbl_quality.py::test_no_temp_keys`

**Input**: .bbl file
**Check**: No citation keys contain temp markers

**Test logic**:
```python
def test_no_temp_keys(bbl_path):
    entries = parse_bbl_file(bbl_path)

    TEMP_MARKERS = ['temp', 'dryrun_', 'unknown', 'anonymous']

    issues = []
    for entry in entries:
        key = entry['key'].lower()

        for marker in TEMP_MARKERS:
            if marker in key:
                issues.append(f"{entry['key']}: Contains temp marker '{marker}'")

    assert len(issues) == 0, f"Found {len(issues)} temp/placeholder keys:\n" + "\n".join(issues)
```

**Example failure**:
```
AssertionError: Found 2 temp/placeholder keys:
  zhangTemp2024: Contains temp marker 'temp'
  dryrun_smith_2023: Contains temp marker 'dryrun_'
```

---

### Test 10: Integration Test - Full Conversion Verification

**Test file**: `test_integration_e2e.py::test_full_conversion_quality`

**Input**: Sample markdown file with known citations
**Process**: Run full conversion pipeline
**Output**: Verify PDF meets ALL quality criteria

**Test logic**:
```python
def test_full_conversion_quality(sample_markdown, zotero_rdf, output_dir):
    # Run conversion
    result = run_conversion(
        input_path=sample_markdown,
        rdf_path=zotero_rdf,
        output_dir=output_dir,
        no_web_fetching=True,  # Emergency mode
    )

    assert result.success, f"Conversion failed: {result.error}"

    # Get output files
    pdf_path = output_dir / f"{sample_markdown.stem}.pdf"
    bbl_path = output_dir / f"{sample_markdown.stem}.bbl"
    bib_path = output_dir / "references.bib"

    # Run ALL quality checks
    issues = []

    # Check 1: No domain titles
    try:
        test_no_domain_titles(bbl_path)
    except AssertionError as e:
        issues.append(str(e))

    # Check 2: No stub titles
    try:
        test_no_stub_titles(bbl_path)
    except AssertionError as e:
        issues.append(str(e))

    # ... all other checks ...

    # Check 10: Temp keys
    try:
        test_no_temp_keys(bbl_path)
    except AssertionError as e:
        issues.append(str(e))

    # Also check PDF text for (?) markers
    pdf_text = extract_text_from_pdf(pdf_path)
    unresolved_count = pdf_text.count('(?)')
    if unresolved_count > 0:
        issues.append(f"PDF contains {unresolved_count} unresolved citations (?)")

    # Final assertion
    if issues:
        report = "\n\n".join(f"ISSUE {i+1}:\n{issue}" for i, issue in enumerate(issues))
        assert False, f"Conversion produced {len(issues)} quality issues:\n\n{report}"
```

**Example failure**:
```
AssertionError: Conversion produced 5 quality issues:

ISSUE 1:
Found 2 domain-as-title issues:
  fletcher_craft_2016: Title is domain name 'Amazon.de'
  wooldridge_2009: Title is domain name 'Amazon.de'

ISSUE 2:
Found 7 stub titles:
  axios_2025: Stub title 'Web page by axios'
  arner_2016: Stub title 'Web page by arner et al'
  ...

ISSUE 3:
Only 45% of URLs are hyperlinked. Missing:
  beigi_2024: URL 'https://arxiv.org/abs/2401.12345' not clickable in PDF
  ...
```

---

## Implementation Requirements

### Parsing .bbl Files

**Challenge**: .bbl files are LaTeX format, not structured data

**Example .bbl entry**:
```latex
\bibitem[Fletcher(2016)]{fletcher_craft_2016}
Fletcher K (2016).
\newblock Craft of Use: Post-Growth Fashion.
\newblock Routledge, London.
```

**Parsing approach**:
```python
def parse_bbl_file(bbl_path: Path) -> list[dict]:
    """Parse .bbl file to extract bibliography entries."""
    with open(bbl_path) as f:
        content = f.read()

    # Split by \bibitem
    entries = []
    for match in re.finditer(r'\\bibitem\[([^\]]*)\]\{([^}]*)\}(.*?)(?=\\bibitem|\\end\{thebibliography\})', content, re.DOTALL):
        citation_text = match.group(1)  # "Fletcher(2016)"
        key = match.group(2)  # "fletcher_craft_2016"
        body = match.group(3)  # Full entry text

        # Extract components from body
        # \newblock typically marks title
        title_match = re.search(r'\\newblock ([^.]+)\.', body)
        title = title_match.group(1) if title_match else ""

        # First line typically has author
        lines = [l.strip() for l in body.split('\n') if l.strip()]
        author = lines[0] if lines else ""

        # Parse year from citation_text
        year_match = re.search(r'\((\d{4})\)', citation_text)
        year = year_match.group(1) if year_match else ""

        entries.append({
            'key': key,
            'author': author,
            'year': year,
            'title': title,
            'body': body,  # Keep full text for additional checks
        })

    return entries
```

**Note**: .bbl parsing is fragile (depends on bibliography style). May need adjustments for different .bst files.

### Parsing .bib Files

**More straightforward** - can use existing libraries:

```python
import bibtexparser

def parse_bib_file(bib_path: Path) -> list[dict]:
    """Parse .bib file to extract entries."""
    with open(bib_path) as f:
        bib_database = bibtexparser.load(f)

    return bib_database.entries
```

### Extracting Text from PDF

```python
from pypdf import PdfReader

def extract_text_from_pdf(pdf_path: Path) -> str:
    """Extract all text from PDF."""
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() or ""
    return text
```

### Extracting Hyperlinks from PDF

```python
from pypdf import PdfReader

def extract_hyperlinks_from_pdf(pdf_path: Path) -> list[str]:
    """Extract all hyperlinks from PDF annotations."""
    reader = PdfReader(pdf_path)
    links = []

    for page in reader.pages:
        if '/Annots' in page:
            for annot in page['/Annots']:
                obj = annot.get_object()
                if obj.get('/Subtype') == '/Link':
                    if '/A' in obj and '/URI' in obj['/A']:
                        links.append(obj['/A']['/URI'])

    return links
```

---

## Success Criteria (Zero Tolerance)

**For a conversion to be considered successful, ALL of the following MUST be true**:

1. ✅ **PDF compiles without LaTeX errors**
2. ✅ **PDF contains ZERO (?) unresolved citations**
3. ✅ **PDF contains ZERO (Unknown) or (Anonymous) citations**
4. ✅ **.bbl file has ZERO domain names as titles** (Amazon.de, Arxiv.org, etc.)
5. ✅ **.bbl file has ZERO stub titles** (Web page by..., Web article by...)
6. ✅ **.bbl file has ZERO missing titles** (empty title fields)
7. ✅ **.bbl file has ZERO temp/placeholder keys** (Temp, dryrun_, Unknown)
8. ✅ **Organization names formatted correctly** (no "Commission E", "Foundation EM")
9. ✅ **arXiv citations include identifiers** (arXiv:YYMM.NNNNN format)
10. ✅ **Hyperlinks work in PDF** (at least 80% of URLs clickable)
11. ✅ **No duplicate entries** (same author + title)
12. ✅ **No generic/low-quality titles** (single words, site chrome with |)
13. ✅ **Maximum 5 missing citations** (if emergency mode with local RDF)

**CRITICAL ENFORCEMENT**:
- Claude MUST run the full test suite before claiming success
- Claude MUST show test results (pass/fail for each check)
- Claude MUST read the actual PDF to verify citations
- Claude MUST NOT claim success based on "compilation succeeded"

---

## Questions for OpenAI Reviewer

1. **Is this test approach comprehensive enough?** Are there other citation quality issues we should test for?

2. **Parsing .bbl files**: Our approach uses regex on LaTeX output. Is there a more robust method?

3. **Hyperlink verification**: Extracting hyperlinks from PDFs can be tricky. Are there better approaches?

4. **Organization name detection**: How can we reliably detect when an author field should be double-braced? Should we maintain a list of known organizations?

5. **Hallucination verification**: What's the best way to verify arXiv/DOI titles are correct? Should we always fetch from API and compare?

6. **Duplicate detection**: How fuzzy should title/author matching be for duplicate detection? Exact match? Levenshtein distance?

7. **Test performance**: Some tests (especially PDF parsing) may be slow. Should we prioritize fast smoke tests vs comprehensive validation?

8. **False positives**: How should we handle edge cases where legitimate titles might trigger rules (e.g., a paper genuinely titled "Web.com: A History")?

9. **Integration with CI/CD**: Should these tests run on every commit, or only on release candidates?

10. **Error reporting**: When tests fail, what information should be surfaced to help debug? Full .bbl dump? Sample citations? PDF screenshots?

---

## Appendix: Example Test Run Output

**What successful output should look like**:

```bash
$ python -m pytest tests/test_bbl_quality.py -v

tests/test_bbl_quality.py::test_no_domain_titles PASSED
tests/test_bbl_quality.py::test_no_stub_titles PASSED
tests/test_bbl_quality.py::test_no_missing_titles PASSED
tests/test_bbl_quality.py::test_organization_names_formatted_correctly PASSED
tests/test_bbl_quality.py::test_arxiv_identifiers_present PASSED
tests/test_bbl_quality.py::test_no_duplicates PASSED
tests/test_bbl_quality.py::test_no_generic_titles PASSED
tests/test_bbl_quality.py::test_no_temp_keys PASSED

tests/test_pdf_citation_quality.py::test_hyperlinks_present PASSED
tests/test_pdf_citation_quality.py::test_no_unresolved_citations PASSED

tests/test_integration_e2e.py::test_full_conversion_quality PASSED

================================ 11 passed in 8.42s ================================

✅ ALL QUALITY CHECKS PASSED
✅ PDF is ready for submission
```

**What failure output should look like** (example):

```bash
$ python -m pytest tests/test_bbl_quality.py -v

tests/test_bbl_quality.py::test_no_domain_titles FAILED
tests/test_bbl_quality.py::test_no_stub_titles FAILED
tests/test_bbl_quality.py::test_no_missing_titles PASSED
tests/test_bbl_quality.py::test_organization_names_formatted_correctly FAILED
tests/test_bbl_quality.py::test_arxiv_identifiers_present FAILED
tests/test_bbl_quality.py::test_no_duplicates FAILED
tests/test_bbl_quality.py::test_no_generic_titles FAILED
tests/test_bbl_quality.py::test_no_temp_keys PASSED

================================ FAILURES ================================

_________________________ test_no_domain_titles _________________________

AssertionError: Found 2 domain-as-title issues:
  fletcher_craft_2016: Title is domain name 'Amazon.de'
  wooldridge_2009: Title is domain name 'Amazon.de'

_________________________ test_no_stub_titles _________________________

AssertionError: Found 7 stub titles:
  axios_2025: Stub title 'Web page by axios'
  arner_2016: Stub title 'Web page by arner et al'
  bloomberg_2018: Stub title 'Web article by bloomberg'
  borkey_2024: Stub title 'Web page by brown & borkey'
  cirpass_2024b: Stub title 'Web page by cirpass'
  google_2024: Stub title 'Web page by google'
  newman_2021: Stub title 'Web page by newman'

======================== short test summary info ========================
FAILED tests/test_bbl_quality.py::test_no_domain_titles
FAILED tests/test_bbl_quality.py::test_no_stub_titles
FAILED tests/test_bbl_quality.py::test_organization_names_formatted_correctly
FAILED tests/test_bbl_quality.py::test_arxiv_identifiers_present
FAILED tests/test_bbl_quality.py::test_no_duplicates
FAILED tests/test_bbl_quality.py::test_no_generic_titles
======================== 6 failed, 2 passed in 5.12s ========================

❌ QUALITY CHECKS FAILED
❌ PDF has 6 categories of issues
❌ DO NOT claim conversion is successful
❌ Fix the root causes before retrying
```

---

## Conclusion

This test plan addresses **9 distinct failure modes** identified from real-world PDF output, with:
- **Specific detection patterns** for each failure mode
- **Concrete test implementations** with example code
- **Clear success criteria** (13 checkpoints)
- **Enforcement mechanism** to prevent premature success claims

The goal: Make it **impossible for Claude to claim success without actual verification**.

**Key insight**: Test the **actual output** (.bbl and PDF), not intermediate files (.bib), because:
1. BibTeX processing can introduce issues not visible in .bib
2. Bibliography style formatting only appears in .bbl
3. PDF is what users actually see

**Next steps**:
1. Implement test suite in Python
2. Integrate into conversion pipeline
3. Run on existing PDFs to baseline current quality
4. Fix root causes for each failure mode
5. Re-test until all checks pass
6. Only then claim conversion is working
