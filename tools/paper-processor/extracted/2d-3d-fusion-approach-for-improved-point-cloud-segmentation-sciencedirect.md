# 2D–3D fusion approach for improved point cloud segmentation

**Authors:** AbstractSemantic segmentation of point clouds withdeep learning(DL) has shown significant potential. However, existing DL algorithms struggle with accurately segmenting categories with fewer instances or similar shapes. To address this issue, this paper proposes a 2D–3D fusion approach (Point-YOLO) to improving semantic segmentation accuracy of point clouds. The proposed method captures images from virtual cameras within point clouds and conducts image semantic segmentation with YOLO. Then, the image segmentation results are fused with point cloud segmentation results obtained from point-based DL methods (e.g., PointNet, PointNet++) for improved point cloud segmentation. The Point-YOLO approach improved mean class Accuracy by 14.52 % on the S3DIS dataset and 26.49 % on the underground dataset compared to PointNet++. The mean Intersection over Union for minority categories such as doors, windows, andair ductsimproved by 37.65 %, 19.35 %, and 75.16 %, respectively. The proposed method also performed well for state-of-the-art algorithms such as PointNext and PointVector.

**Published in:** Automation in Construction, 2025, Vol. 177, pp. 106336

**DOI:** [10.1016/j.autcon.2025.106336](https://doi.org/10.1016/j.autcon.2025.106336)

## Abstract

Highlights•A 2D–3D fusion method (Point-YOLO) is proposed for point cloud semantic segmentation.•YOLO-based image results are fused with point cloud results for enhanced accuracy.•Mean class accuracy is improved by 14.52 % and 26.49 % for two different datasets.•IoUs of minority categories, such as doors, windows, and air ducts, increased significantly.•Compatible with state-of-the-artdeep learningmethods such as PointNext and PointVector.

### Outline

3. Proposed Point-YOLO method
3. Proposed Point-YOLO method
4. Experimental settings
4. Experimental settings
5. Experimental results
5. Experimental results
CRediT authorship contribution statement
CRediT authorship contribution statement
Declaration of competing interest
Declaration of competing interest
Appendix A. Visual results of segmentation performance for PointNet++ and PointNet++-YOLO
Appendix A. Visual results of segmentation performance for PointNet++ and PointNet++-YOLO

### Cited by (2)

Figures (13)Show 7 more figures

### Figures (13)

Tables (14)Table 1Table 2Table 3Table 4Table 5Table 6Show all tables

### Tables (14)

Automation in ConstructionVolume 177,September 2025, 106336
Automation in ConstructionVolume 177,September 2025, 106336

### Automation in Construction

Automation in Construction
Automation in Construction
Volume 177,September 2025, 106336

## 2D–3D fusion approach for improved point cloud segmentation

2D–3D fusion approach for improved point cloud segmentation
Author links open overlay panelHongzheYuea,QianWanga,MingyuZhangb,YutongXuec,LiangLudShow moreOutlineAdd to MendeleyShareCite
Author links open overlay panelHongzheYuea,QianWanga,MingyuZhangb,YutongXuec,LiangLud
Author links open overlay panelHongzheYuea,QianWanga,MingyuZhangb,YutongXuec,LiangLud
Author links open overlay panelHongzheYuea,QianWanga,MingyuZhangb,YutongXuec,LiangLud
Author links open overlay panel
OutlineAdd to MendeleyShareCite
https://doi.org/10.1016/j.autcon.2025.106336Get rights and content
https://doi.org/10.1016/j.autcon.2025.106336
https://doi.org/10.1016/j.autcon.2025.106336
Get rights and content
Get rights and content
Highlights•A 2D–3D fusion method (Point-YOLO) is proposed for point cloud semantic segmentation.•YOLO-based image results are fused with point cloud results for enhanced accuracy.•Mean class accuracy is improved by 14.52 % and 26.49 % for two different datasets.•IoUs of minority categories, such as doors, windows, and air ducts, increased significantly.•Compatible with state-of-the-artdeep learningmethods such as PointNext and PointVector.AbstractSemantic segmentation of point clouds withdeep learning(DL) has shown significant potential. However, existing DL algorithms struggle with accurately segmenting categories with fewer instances or similar shapes. To address this issue, this paper proposes a 2D–3D fusion approach (Point-YOLO) to improving semantic segmentation accuracy of point clouds. The proposed method captures images from virtual cameras within point clouds and conducts image semantic segmentation with YOLO. Then, the image segmentation results are fused with point cloud segmentation results obtained from point-based DL methods (e.g., PointNet, PointNet++) for improved point cloud segmentation. The Point-YOLO approach improved mean class Accuracy by 14.52 % on the S3DIS dataset and 26.49 % on the underground dataset compared to PointNet++. The mean Intersection over Union for minority categories such as doors, windows, andair ductsimproved by 37.65 %, 19.35 %, and 75.16 %, respectively. The proposed method also performed well for state-of-the-art algorithms such as PointNext and PointVector.
Highlights•A 2D–3D fusion method (Point-YOLO) is proposed for point cloud semantic segmentation.•YOLO-based image results are fused with point cloud results for enhanced accuracy.•Mean class accuracy is improved by 14.52 % and 26.49 % for two different datasets.•IoUs of minority categories, such as doors, windows, and air ducts, increased significantly.•Compatible with state-of-the-artdeep learningmethods such as PointNext and PointVector.

### Highlights

•A 2D–3D fusion method (Point-YOLO) is proposed for point cloud semantic segmentation.•YOLO-based image results are fused with point cloud results for enhanced accuracy.•Mean class accuracy is improved by 14.52 % and 26.49 % for two different datasets.•IoUs of minority categories, such as doors, windows, and air ducts, increased significantly.•Compatible with state-of-the-artdeep learningmethods such as PointNext and PointVector.
•A 2D–3D fusion method (Point-YOLO) is proposed for point cloud semantic segmentation.•YOLO-based image results are fused with point cloud results for enhanced accuracy.•Mean class accuracy is improved by 14.52 % and 26.49 % for two different datasets.•IoUs of minority categories, such as doors, windows, and air ducts, increased significantly.•Compatible with state-of-the-artdeep learningmethods such as PointNext and PointVector.
A 2D–3D fusion method (Point-YOLO) is proposed for point cloud semantic segmentation.
A 2D–3D fusion method (Point-YOLO) is proposed for point cloud semantic segmentation.
YOLO-based image results are fused with point cloud results for enhanced accuracy.
YOLO-based image results are fused with point cloud results for enhanced accuracy.
Mean class accuracy is improved by 14.52 % and 26.49 % for two different datasets.
Mean class accuracy is improved by 14.52 % and 26.49 % for two different datasets.
IoUs of minority categories, such as doors, windows, and air ducts, increased significantly.
IoUs of minority categories, such as doors, windows, and air ducts, increased significantly.
Compatible with state-of-the-artdeep learningmethods such as PointNext and PointVector.
Compatible with state-of-the-artdeep learningmethods such as PointNext and PointVector.
AbstractSemantic segmentation of point clouds withdeep learning(DL) has shown significant potential. However, existing DL algorithms struggle with accurately segmenting categories with fewer instances or similar shapes. To address this issue, this paper proposes a 2D–3D fusion approach (Point-YOLO) to improving semantic segmentation accuracy of point clouds. The proposed method captures images from virtual cameras within point clouds and conducts image semantic segmentation with YOLO. Then, the image segmentation results are fused with point cloud segmentation results obtained from point-based DL methods (e.g., PointNet, PointNet++) for improved point cloud segmentation. The Point-YOLO approach improved mean class Accuracy by 14.52 % on the S3DIS dataset and 26.49 % on the underground dataset compared to PointNet++. The mean Intersection over Union for minority categories such as doors, windows, andair ductsimproved by 37.65 %, 19.35 %, and 75.16 %, respectively. The proposed method also performed well for state-of-the-art algorithms such as PointNext and PointVector.

### 1. Introduction

Building Information Modeling(BIM) has been widely adopted in the planning, design, construction, and operation of buildings due to its ability to provide comprehensive semantic and geometric information [[1],[2],[3]]. As-built BIM models accurately represent a building's actual state, capturing its condition either at the time of completion or as it currently exists [4]. This capability significantly enhances the efficiency of building maintenance and monitoring, thus improving operational effectiveness and supporting informed decision-making processes. However, most of the existing buildings were built decades ago and did not have any BIM model available. Even when BIM models are available from the design phase, changes during construction often lead to discrepancies between the models and the as-built structure. This situation necessitates on-site surveys to gather data for reconstructing BIM models, which remains time-consuming and labor-intensive.
Building Information Modeling(BIM) has been widely adopted in the planning, design, construction, and operation of buildings due to its ability to provide comprehensive semantic and geometric information [
In recent years, three-dimensional (3D) laser scanning, especiallyterrestrial laser scanning(TLS), has proven to be an accurate and efficient technology for capturing detailed geometric data of objects' surfaces [5], making it a powerful tool for3D modelreconstruction [5]. However, due to the lack of semantic information in point clouds, the first step in 3D model reconstruction is to conduct semantic segmentation, which assigns semantic labels to various components in 3D point clouds. Nevertheless, semantic segmentation of 3D point clouds is a complex and demanding task. Traditional methods frequently require expert-driven manual segmentation, which is both time-consuming and labor-intensive [6]. Although somemachine learning methodshave been developed to automatically identify certain components within point clouds, these approaches still heavily depend on prior knowledge and exhibit limited generalization and applicability [7].
In recent years, three-dimensional (3D) laser scanning, especiallyterrestrial laser scanning(TLS), has proven to be an accurate and efficient technology for capturing detailed geometric data of objects' surfaces [
], making it a powerful tool for3D modelreconstruction [
]. Although somemachine learning methodshave been developed to automatically identify certain components within point clouds, these approaches still heavily depend on prior knowledge and exhibit limited generalization and applicability [
Recently,deep learning(DL) models have demonstrated exceptional performance across a range of 2D and 3Dcomputer visiontasks, such as point cloud completion, object classification, object detection, and semantic segmentation [[8],[9],[10]]. Unlike traditional methods that heavily rely on manually crafted features, DL algorithms autonomously learn and extract intricate data features from large, labeled training datasets, thereby achieving better performance [6,11,12]. Early DL methods for point clouds were based on voxel or multi-view approaches [13,14], which required high computational costs and suffered from information loss. With the advent of PointNet [15], 3D point cloud data could be directly used as input toneural networksfor object recognition, significantly enhancing the ability to efficiently process and analyze point cloud data. PointNet++ [16] further advanced this by introducing hierarchical feature learning, improving the model's ability to capture localgeometric features. DL algorithms based on PointNet and PointNet++ have been widely applied in indoor building scenes, mechanical, electrical, and plumbing (MEP) systems, as well as bridge structures, facilitating model reconstruction processes [11,12,17,18].
Recently,deep learning(DL) models have demonstrated exceptional performance across a range of 2D and 3Dcomputer visiontasks, such as point cloud completion, object classification, object detection, and semantic segmentation [
(DL) models have demonstrated exceptional performance across a range of 2D and 3Dcomputer visiontasks, such as point cloud completion, object classification, object detection, and semantic segmentation [
], 3D point cloud data could be directly used as input toneural networksfor object recognition, significantly enhancing the ability to efficiently process and analyze point cloud data. PointNet++ [
] further advanced this by introducing hierarchical feature learning, improving the model's ability to capture localgeometric features. DL algorithms based on PointNet and PointNet++ have been widely applied in indoor building scenes, mechanical, electrical, and plumbing (MEP) systems, as well as bridge structures, facilitating model reconstruction processes [
Despite advancements in DL technologies, these DL algorithms still encounter challenges in distinguishing between similar components and recognizing object categories with fewer instances [[19],[20],[21],[22],[23]]. For instance, in the S3DIS dataset, components from smaller categories, such as doors and windows, are often misclassified as larger categories like walls due to similar structural features [17]. In MEP scenes, I-beams are frequently confused with similar components such as rectangular beams due to the limited number of I-beam examples [11]. Thesemisclassificationssignificantly affect the accuracy of point cloud semantic segmentation.
]. Thesemisclassificationssignificantly affect the accuracy of point cloud semantic segmentation.
In recent years, image-based DL techniques have demonstrated significant potential in object recognition and semantic segmentation [24,25]. Compared to point clouds, which require complex parameter adjustments to extract intricate features between points, image recognition can more effectively identify objects using edge and contour features. Additionally, DL models rely on large datasets for training, andimage dataare generally more accessible and easier to annotate compared to point clouds. By integrating the strengths of image recognition into point cloud semantic segmentation, 2D–3D fusion techniques can be employed to improve the accuracy of point cloud segmentation.
]. Compared to point clouds, which require complex parameter adjustments to extract intricate features between points, image recognition can more effectively identify objects using edge and contour features. Additionally, DL models rely on large datasets for training, andimage dataare generally more accessible and easier to annotate compared to point clouds. By integrating the strengths of image recognition into point cloud semantic segmentation, 2D–3D fusion techniques can be employed to improve the accuracy of point cloud segmentation.
Currently, the application of 2D–3D fusion techniques in the construction industry remains in its early stages. Some researchers have attempted to integrate point cloud component recognition results with image-based damage detection results to enable health monitoring of buildings and infrastructure [26,27]. However, these studies use images and point clouds for two separate, independent tasks rather than truly performingdata fusion. In contrast, some scholars employed 2D–3D fusion techniques to enhance the accuracy of semantic segmentation in construction scenarios. For instance, Liang, et al. [28] incorporated material information from images into point clouds to improve the performance of DL-based semantic segmentation for building indoor scenes. Li, et al. [29] developed a data fusion method capable of detecting surface damage on bridges using both 2D images and 3D LiDAR data, which improved the accuracy of damage detection. However, these approaches typically relied on pre-registered 2D–3D data, yet in real-world scenarios, point clouds captured through TLS often lack pre-registered image data. Additionally, public datasets such as Semantic3D [30], Tornoto3D [31], and Paris-Carla-3D [32] contain LiDAR point clouds without associated 2D images. Similarly, photogrammetry-based point clouds (e.g., Campus3D [33], STPLS3D [34]) often lack depth maps.
]. However, these studies use images and point clouds for two separate, independent tasks rather than truly performingdata fusion. In contrast, some scholars employed 2D–3D fusion techniques to enhance the accuracy of semantic segmentation in construction scenarios. For instance, Liang, et al. [
As a result, several gaps remain in current research. First, in practical applications, most real-world point cloud datasets lack pre-registered 2D image data, which presents significant challenges for implementing 2D–3D fusion techniques. Second, while existing research on 2D–3D fusion has primarily focused on autonomous driving [35,36], limited attention has been given to building indoor scenes, leaving the methods for such applications unclear. Third, although techniques like PointNet have substantially improved model reconstruction in building indoor scenes, challenges persist in recognizing less prominent components, such as doors and columns. To address these gaps, this paper poses the following research questions: (1) Can a 2D–3D fusion method be developed that significantly improves semantic segmentation accuracy for point clouds in building indoor scenes? (2) Can such a method be implemented without relying on pre-registered 2D images, enabling image-to-point cloud enhancement without the need for prior alignment?
To address these research questions, this paper proposes a novel 2D–3D fusion approach, named Point-YOLO, which enhances the semantic segmentation of point clouds without requiring pre-registered 2D images. Building on advanced DL algorithms, such as YOLO for object detection and semantic segmentation, this approach introduces a key innovation: using virtual cameras to generate 2D images directly from point cloud scenes. The generated 2D images are processed using YOLO-based algorithms, and the resulting confidence scores from image-based segmentation are then integrated with point cloudsegmentation methods, such as PointNet and PointNet++, to refine and improve the overall accuracy of point cloud segmentation. The proposed Point-YOLO approach is evaluated across various datasets, scenarios, and DL algorithms, demonstrating its broad applicability and effectiveness in enhancing point cloud semantic segmentation.
The rest of this paper is organized as follows.Section 2provides a review of DL-based semantic segmentation of point clouds and images.Section 3introduces the proposed Point-YOLO algorithms and details the methods.Section 4presents the experimental settings.Section 5illustrates and discusses the experimental results, andSection 6concludes the study.

### 2. Related works

This section reviews the related works.Section 2.1discusses DL-based point cloud semantic segmentation,Section 2.2coversDLalgorithms for images, andSection 2.3presents methods for 2D–3Ddata fusion.
coversDLalgorithms for images, and
presents methods for 2D–3Ddata fusion.

#### 2.1. DL-based point cloud semantic segmentation

Early approaches to point cloud semantic segmentation include 2D projection-based methods and voxel-based methods [13,14]. In 2D projection-based methods, point clouds are rendered into 2D images from various camera viewpoints and then segmented in the 2D space. The segmented images are subsequently reprojected onto the 3D point cloud [[37],[38],[39]]. However, information loss often occurs during the conversion between 3D point clouds and 2D images. Voxel-based methods learn point cloud representations using volumetric representations derived from point clouds [40,41]. However, voxel-based methods are computationally demanding and require substantial memory resources.
Point-based methods directly process raw point clouds without converting them into other representations [42]. This approach significantly reduces the information loss and resource demands associated with 2D projection-based and voxel-based methods. The most well-known point-based methods are the PointNet series, which include various models developed based on PointNet and PointNet++. A seminal work in this field is PointNet [43], which employed symmetric functions such as max pooling to handle unordered input sets, ensuring permutation invariance for optimal point cloud processing. Building on PointNet, PointNet++ [16] improved its model performance by capturing local and global features at multiple scales and locations using multiple vanilla PointNet learners, which allowed the model to recognize finer details and perform better on complex datasets. Subsequent models further extended the capabilities of PointNet and PointNet++. For instance, PointNext [44] enhanced PointNet++ by integrating advanced residual connections and dynamic sampling strategies. These innovations addressed the limitations of static sampling and enabled better adaptation to diverse point cloud distributions. PointVector [45] improved point-based methods by introducing vector-based representations, which helped handle varying point densities and spatial relationships, leading to better segmentation performance.
Beyond the PointNet series, other point-based methods have also made significant contributions. For example, DGCNN [46] introduced edge convolution to capture local geometric structures; PointCNN [47] proposed a novel X-conv operator for hierarchical feature learning; RandLA-Net [48] optimized large-scale point cloud segmentation with a lightweight architecture; and Point Transformer [49] and its iteration Point Transformer V3 [50] leveraged self-attention mechanisms for enhanced feature learning and positional encoding.
Despite significant advancements in point-basedDLalgorithms, their application to real-world datasets continues to present challenges.Table 1summarizes the accuracy metrics of various algorithms evaluated on the Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset. Both the computer science and construction domains highlight the ongoing difficulties in achieving high precision with these algorithms.Table 1. Summary of semantic segmentation accuracy on S3DIS in previous literature.DomainRef.AlgorithmYearTraining datasetmAccOAmIoUmIoU or OA of selected categoriesCeilingFloorWallWindowDoorColumnBeamComputer vision[43]PointNet2017S3DIS49.0/41.188.897.369.846.310.83.90.1[47]PointCNN2018S3DIS63.985.957.392.398.279.422.862.117.60.0[51]KPConv2019S3DIS72.8/67.192.897.382.458.069.023.90.0[52]PAConv2021S3DIS73.0/66.694.698.682.458.060.026.40.0[44]PointNeXt2022S3DIS77.590.770.894.298.584.459.374.037.70.0Construction[19]SPG2022S3DIS and synthetic point cloud datasets63.076.646.183.996.466.27.663.927.932.5[21]PointNet++2023S3DIS and synthetic point cloud datasets/82.4442.8188.589.170.44.9/11.631.1[23]RandLA-Net2023S3DIS and synthetic point cloud datasets//41.1<90 %∼85 %∼75 %∼5 %<10 %<3 %/[28]PointNet2024S3DIS with material augmentation51.879.842.088.298.170.637.021.33.60.8PointNet++62.186.954.490.597.774.852.529.24.40.3[53]Improved RandLA-Net2024S3DIS72.587.964.591.997.181.361.947.828.60.0Note: The accuracies of selected categories for reference [19] are OA, while the rest are mIoU. The mIoU values for reference [23] are inferred from a figure in the paper as exact numbers were not provided.
Despite significant advancements in point-basedDLalgorithms, their application to real-world datasets continues to present challenges.Table 1summarizes the accuracy metrics of various algorithms evaluated on the Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset. Both the computer science and construction domains highlight the ongoing difficulties in achieving high precision with these algorithms.
Despite significant advancements in point-basedDLalgorithms, their application to real-world datasets continues to present challenges.
Table 1. Summary of semantic segmentation accuracy on S3DIS in previous literature.DomainRef.AlgorithmYearTraining datasetmAccOAmIoUmIoU or OA of selected categoriesCeilingFloorWallWindowDoorColumnBeamComputer vision[43]PointNet2017S3DIS49.0/41.188.897.369.846.310.83.90.1[47]PointCNN2018S3DIS63.985.957.392.398.279.422.862.117.60.0[51]KPConv2019S3DIS72.8/67.192.897.382.458.069.023.90.0[52]PAConv2021S3DIS73.0/66.694.698.682.458.060.026.40.0[44]PointNeXt2022S3DIS77.590.770.894.298.584.459.374.037.70.0Construction[19]SPG2022S3DIS and synthetic point cloud datasets63.076.646.183.996.466.27.663.927.932.5[21]PointNet++2023S3DIS and synthetic point cloud datasets/82.4442.8188.589.170.44.9/11.631.1[23]RandLA-Net2023S3DIS and synthetic point cloud datasets//41.1<90 %∼85 %∼75 %∼5 %<10 %<3 %/[28]PointNet2024S3DIS with material augmentation51.879.842.088.298.170.637.021.33.60.8PointNet++62.186.954.490.597.774.852.529.24.40.3[53]Improved RandLA-Net2024S3DIS72.587.964.591.997.181.361.947.828.60.0Note: The accuracies of selected categories for reference [19] are OA, while the rest are mIoU. The mIoU values for reference [23] are inferred from a figure in the paper as exact numbers were not provided.
Table 1. Summary of semantic segmentation accuracy on S3DIS in previous literature.
Table 1. Summary of semantic segmentation accuracy on S3DIS in previous literature.
Table 1. Summary of semantic segmentation accuracy on S3DIS in previous literature.
DomainRef.AlgorithmYearTraining datasetmAccOAmIoUmIoU or OA of selected categoriesCeilingFloorWallWindowDoorColumnBeamComputer vision[43]PointNet2017S3DIS49.0/41.188.897.369.846.310.83.90.1[47]PointCNN2018S3DIS63.985.957.392.398.279.422.862.117.60.0[51]KPConv2019S3DIS72.8/67.192.897.382.458.069.023.90.0[52]PAConv2021S3DIS73.0/66.694.698.682.458.060.026.40.0[44]PointNeXt2022S3DIS77.590.770.894.298.584.459.374.037.70.0Construction[19]SPG2022S3DIS and synthetic point cloud datasets63.076.646.183.996.466.27.663.927.932.5[21]PointNet++2023S3DIS and synthetic point cloud datasets/82.4442.8188.589.170.44.9/11.631.1[23]RandLA-Net2023S3DIS and synthetic point cloud datasets//41.1<90 %∼85 %∼75 %∼5 %<10 %<3 %/[28]PointNet2024S3DIS with material augmentation51.879.842.088.298.170.637.021.33.60.8PointNet++62.186.954.490.597.774.852.529.24.40.3[53]Improved RandLA-Net2024S3DIS72.587.964.591.997.181.361.947.828.60.0
Note: The accuracies of selected categories for reference [19] are OA, while the rest are mIoU. The mIoU values for reference [23] are inferred from a figure in the paper as exact numbers were not provided.
Note: The accuracies of selected categories for reference [19] are OA, while the rest are mIoU. The mIoU values for reference [23] are inferred from a figure in the paper as exact numbers were not provided.
In the field ofcomputer vision, DL algorithms have demonstrated commendable performance overall in the S3DIS dataset. These algorithms have achieved over 90 % mIoU for ceilings, over 95 % for floors, and over 70 % for walls. However, accuracy for more complex and less frequent components, such as windows and doors, remains suboptimal. For example, the PointNet model achieved only 46.3 % mIoU for windows and 10.8 % for doors. Even with advanced algorithms like PointNext, the mIoU for windows and doors remained below 60 % and 75 %, respectively. Furthermore, all tested algorithms have struggled to accurately recognize less frequent components such as beams and columns. In fact, for beams, most algorithms had a recognition rate of 0 %. This difficulty underscores the persistent challenges these algorithms face in achieving high precision for underrepresented components.
In addition to the advancements in computer vision, some studies in the construction domain have aimed to enhance semantic segmentation accuracy by leveraging synthetic datasets and incorporating material attributes into point clouds. For instance, Tang, et al. [21] proposed an Improved Hidden Point Removal (IHPR) algorithm for synthetic point clouds. By utilizing PointNet and PointNet++ for semantic segmentation, their method demonstrated a 5 % to 10 % improvement in OA on the S3DIS dataset compared to methods that did not incorporate synthetic point clouds. Liang, et al. [28] improved semantic segmentation by integrating material attributes into the point clouds and tested their approach using the PointNet architecture. Their method showed a 70.87 % improvement in segmenting boards and a 41.06 % improvement for doors. Additionally, several studies have enhanced segmentation model accuracy through algorithmic improvements. For example, Mahmoud, et al. [53] achieved a 64.5 % mIoU on S3DIS by introducing inversedensity importance sampling, strengthening local feature aggregation modules, applying dilated convolutions, and utilizing novel loss functions. Similarly, Li, et al. [54] proposed atransfer learningmethod, AttTransNet, which reduced training time by 80 % and improved segmentation accuracy by more than 20 %.
] achieved a 64.5 % mIoU on S3DIS by introducing inversedensity importance sampling, strengthening local feature aggregation modules, applying dilated convolutions, and utilizing novel loss functions. Similarly, Li, et al. [
] proposed atransfer learningmethod, AttTransNet, which reduced training time by 80 % and improved segmentation accuracy by more than 20 %.
These approaches have shown some improvements in Overall Accuracy (OA) and mIoU compared to baseline models. However, these approaches still struggle with accurately segmenting minority categories such as windows, doors, columns, and beams. In certain cases, the Intersection over Union (IoU) for these components is even lower than that achieved by baseline models in the computer vision domain. Hence, synthetic point clouds and attribute enhancement techniques have not been sufficiently effective in improving semantic segmentation accuracy for these underrepresented components. Moreover, for MEP scenes, similarmisclassificationissues arise with minority categories. For example, Yin, et al. [11] applied PointNet++ to point cloud segmentation in MEP industrial scenes. This approach resulted in 33.77 % of I-beams being misclassified as rectangular beams and 48.65 % of tanks being misclassified as pipes. In tunnel scenes, Zhou, et al. [55] employed ASPCNet for point cloud segmentation. However, 11.17 % of cables were misclassified as segments, and 10.94 % of supports were incorrectly identified.
These approaches have shown some improvements in Overall Accuracy (OA) and mIoU compared to baseline models. However, these approaches still struggle with accurately segmenting minority categories such as windows, doors, columns, and beams. In certain cases, the Intersection over Union (IoU) for these components is even lower than that achieved by baseline models in the computer vision domain. Hence, synthetic point clouds and attribute enhancement techniques have not been sufficiently effective in improving semantic segmentation accuracy for these underrepresented components. Moreover, for MEP scenes, similarmisclassificationissues arise with minority categories. For example, Yin, et al. [
These errors are primarily attributable to 1) the structural similarity between components, such as walls and doors [17], and 2) the insufficient training samples for categories such as windows, doors, I-beams, and cables [22,23]. Although point-based DL algorithms are effective in handling large-scale point clouds, point-based algorithms struggle with accurate recognition of minority categories and structurally similar components. Given the success of DL algorithms in image-based object recognition and semantic segmentation, this paper proposes a 2D–3D fusion approach that leverages image-based semantic segmentation to enhance the performance of point cloud semantic segmentation. This 2D–3D fusion method updates the confidence levels of point cloud segmentation based on 2D recognition results, which helps to improve overall segmentation accuracy.

#### 2.2. DL algorithms for images

In recent years, DL models based on images have been widely applied to object detection and semantic segmentation. These image-based DL models are mainly categorized into two-stage and one-stage detection methods.
Two-stagedetection algorithmsare recognized for their high accuracy in both image segmentation and object detection. These algorithms operate in two phases: initially generating region proposals, followed by refining these proposals through classification andbounding boxregression. A notable example was the Region-basedConvolutional Neural Network(R-CNN) [56]. R-CNN generated region proposals using selective search and then employed a convolutional neural network (CNN) to extract features from each proposal. These features were classified, and bounding boxes were refined to enhance detection accuracy. Building on R-CNN, Mask R-CNN [24] introduced an additional branch that outputted binary masks for each region, enabling pixel-level segmentation. Other developments included Faster R-CNN [57], which introduced a Region Proposal Network (RPN) for more efficient proposal generation, and Cascade R-CNN [58], which applied a multi-stage refinement process to improve detection, especially for objects with varying scales and aspect ratios.
Two-stagedetection algorithmsare recognized for their high accuracy in both image segmentation and object detection. These algorithms operate in two phases: initially generating region proposals, followed by refining these proposals through classification andbounding boxregression. A notable example was the Region-basedConvolutional Neural Network(R-CNN) [
Two-stagedetection algorithmsare recognized for their high accuracy in both image segmentation and object detection. These algorithms operate in two phases: initially generating region proposals, followed by refining these proposals through classification and
regression. A notable example was the Region-basedConvolutional Neural Network(R-CNN) [
In contrast, one-stage detection algorithms streamline the process by eliminating the need for region proposals, enabling end-to-end training and faster inference. The YOLO family exemplifies one-stage detectors, encompassing multiple versions from YOLOv1 to the most recent YOLOv10 [59]. Among these, YOLOv8 has gained particular attention for its balance of speed and accuracy, incorporating features such as improved object localization and better generalization across diverse datasets [60]. Subsequent versions, such as YOLOv9 [61] and YOLOv10 [59], further refined the model architecture by incorporating features such as improved handling of small objects and enhanced detection accuracy in complex environments.
Given the outstanding performance of image-based DL, this paper explores its application to enhance point cloud semantic segmentation. However, two challenges arise when relying solely on image-based semantic segmentation: first, converting multi-view images back into point clouds leads to information loss; second, accurately projecting the entire point cloud scene onto images from multiple views is difficult. To address these challenges, this study retains point cloud-based DL algorithms to directly process point clouds while incorporating image detection methods for critical and less frequent components, such as doors and windows. This integrated approach ensures both comprehensive semantic segmentation of large point cloud scenes and accurate detection of significant but often misclassified components, thereby improving overall segmentation accuracy.

#### 2.3. 2D–3D data fusion

Some studies have successfully transferred techniques from the image domain to the point cloud domain, achieving promising results. For example, Zou, et al. [62] transferred the Hierarchical Geometry Learning (HGL) framework from images to point clouds, improving accuracy on datasets such as SemanticKITTI. Zhao, et al. [49] applied image-based transformers to point clouds, introducing the Point Transformer, which achieved state-of-the-art results at the time. Subsequently, Point Transformer V2 [63]，Point Transformer V3 [50] further refined the transformer mechanism for point clouds, resulting in even better outcomes. Although these algorithms apply image-based ideas to 3D point clouds, they do not constitute true fusion between images and point clouds. In contrast, some 2D–3D fusion strategies involve combining 2D images with 3D point clouds to improve semantic segmentation accuracy. These strategies can be broadly categorized into three main approaches: data-based fusion, result-based fusion, and information-based fusion.
Data-based fusion involves transforming both 2D and 3D data into a unified format that leverages information from both domains. For instance, Wulff, et al. [64] proposed a multi-dimensional Bird's Eye View (BEV)-based grid representation, UGrid-Fused, which projects point clouds onto a 2D plane to facilitate data fusion between point clouds and images. The fused data is then imported into a fully convolutional network (FCN) for improved semantic segmentation. Similarly, Zhou, et al. [65] introduced the StdnDSN model, which fuses point clouds with very-high-spatial-resolution images by projecting them into a unified format. They applied a grey-level co-occurrence matrix and multi-resolution segmentation to segment the data, followed by classification using a CNN. Sun, et al. [35] designed a cross-modal association learning mechanism to use superpixel information from images, improving semantic segmentation of 3D point clouds. This approach combines active labeling and pseudo-label self-correction to enhance the complementarity between images and point clouds, boosting segmentation performance with minimal labeled data.
Result-based fusion segments the 2D and 3D data separately and then combines the results. For example, Gu, et al. [66] developed an inverse-depth-aware fully convolutional neural network (FCNN) for 2D segmentation, which was fused with point cloud-based segmentation using Conditional Random Fields (CRF) to achieve accurate segmentation for roads. Similarly, Narita, et al. [67] proposed PanopticFusion, which combined semantic and instance segmentation results from RGB-D images to enable comprehensive scene understanding.
Information-based fusion extracts information from either 2D images or 3D point clouds to enhance the segmentation results in the other domain. For example, Liang, et al. [28] used DL-based semantic segmentation to predict material information from images and then mapped this information back onto the point clouds, improving point cloud segmentation by incorporating material-specific details.
Despite advancements in 2D–3D fusion methods, many approaches rely on precise alignment between 2D images and 3D point clouds, which is often difficult to achieve in real-world applications. In practice, corresponding 2D images may not always be available for the point clouds. Additionally, many 2D–3D fusion techniques are tailored to specific algorithms and become ineffective when applied to different methods. To overcome these challenges, this paper proposes a generic enhancement method for point cloud semantic segmentation that does not require prior image registration. The method simulates a virtual camera to automatically capture images from the point clouds, which are then used for image-based semantic segmentation. The resulting confidence scores are applied to update the confidence scores of the point cloud segmentation, improving accuracy by combining the strengths of both image and point cloud data.

### 3. Proposed Point-YOLO method

This study proposed the Point-YOLO method to enhance the accuracy of semantic segmentation of point clouds. The overall architecture of Point-YOLO is shown inFig. 1. The method involves three steps. First, point-based DL algorithms such as PointNet or PointNet++ are employed for semantic segmentation of the point cloud. Second, virtual images are generated from the point cloud scene using a simulated camera approach, and a DL algorithm such as YOLO is employed for semantic segmentation on these images. Finally, based on the 2D segmentation results, a 3D voxel representation with detected object labels is reconstructed. This 3D voxel representation is utilized to refine and update the semantic segmentation of the point cloud using the image segmentation results. The details are illustrated in the following subsections. high-res image (483KB) full-size imageFig. 1. Overall architecture of the proposed Point-YOLO method.
This study proposed the Point-YOLO method to enhance the accuracy of semantic segmentation of point clouds. The overall architecture of Point-YOLO is shown inFig. 1. The method involves three steps. First, point-based DL algorithms such as PointNet or PointNet++ are employed for semantic segmentation of the point cloud. Second, virtual images are generated from the point cloud scene using a simulated camera approach, and a DL algorithm such as YOLO is employed for semantic segmentation on these images. Finally, based on the 2D segmentation results, a 3D voxel representation with detected object labels is reconstructed. This 3D voxel representation is utilized to refine and update the semantic segmentation of the point cloud using the image segmentation results. The details are illustrated in the following subsections.
 high-res image (483KB) full-size image
 high-res image (483KB)
 high-res image (483KB)
Download high-res image (483KB)
 full-size image
 full-size image
Download full-size image
Fig. 1. Overall architecture of the proposed Point-YOLO method.
Fig. 1. Overall architecture of the proposed Point-YOLO method.
Fig. 1. Overall architecture of the proposed Point-YOLO method.

#### 3.1. Point cloud semantic segmentation

Despite the rapid advancements indeep neural networkmodels for point cloud segmentation, PointNet and PointNet++ remain foundational models due to their ability to directly apply DL on raw point cloud data, which significantly enhance processing efficiency. Frameworks based on PointNet have been widely applied to tasks such as classification and segmentation. Notably, PointNet++ achieves considerable improvements with enhanced training strategies, delivering performance comparable to state-of-the-art methods without substantial architectural changes [44]. For instance, models such as PointWeb, ResPointNet++, and Assanet, which were built on PointNet and PointNet++, demonstrated exceptional performance upon their introduction [11,68,69]. As a result, this study leverages PointNet and PointNet++ architectures to evaluate the effectiveness of the proposed Point-YOLO method.
Despite the rapid advancements indeep neural networkmodels for point cloud segmentation, PointNet and PointNet++ remain foundational models due to their ability to directly apply DL on raw point cloud data, which significantly enhance processing efficiency. Frameworks based on PointNet have been widely applied to tasks such as classification and segmentation. Notably, PointNet++ achieves considerable improvements with enhanced training strategies, delivering performance comparable to state-of-the-art methods without substantial architectural changes [
PointNet architecture begins with point-wise feature learning, where each point is processed individually using shared multi-layerperceptrons(MLPs) to extract features, thus maintaining the order invariance of point clouds. These features are then aggregated using a symmetric function, typically max pooling, to obtain a global feature representing the entire point cloud. However, PointNet faces challenges in capturing detailed local geometric structures due to its lack of hierarchical feature learning. To overcome this issue, PointNet++ [16] introduces hierarchical learning, downsampling the point cloud using furthest point sampling to select center points and using radius search to form local regions. The overall architecture of PointNet++ is illustrated inFig. 2. Within each region, a small PointNet model is applied to learn features from the center point and its neighbors. This process is then repeated hierarchically, capturing increasingly complex structures at multiple scales.Download:Download high-res image (471KB)Download:Download full-size imageFig. 2. Overall architecture of PointNet++ [16].
PointNet architecture begins with point-wise feature learning, where each point is processed individually using shared multi-layerperceptrons(MLPs) to extract features, thus maintaining the order invariance of point clouds. These features are then aggregated using a symmetric function, typically max pooling, to obtain a global feature representing the entire point cloud. However, PointNet faces challenges in capturing detailed local geometric structures due to its lack of hierarchical feature learning. To overcome this issue, PointNet++ [16] introduces hierarchical learning, downsampling the point cloud using furthest point sampling to select center points and using radius search to form local regions. The overall architecture of PointNet++ is illustrated inFig. 2. Within each region, a small PointNet model is applied to learn features from the center point and its neighbors. This process is then repeated hierarchically, capturing increasingly complex structures at multiple scales.
PointNet architecture begins with point-wise feature learning, where each point is processed individually using shared multi-layerperceptrons(MLPs) to extract features, thus maintaining the order invariance of point clouds. These features are then aggregated using a symmetric function, typically max pooling, to obtain a global feature representing the entire point cloud. However, PointNet faces challenges in capturing detailed local geometric structures due to its lack of hierarchical feature learning. To overcome this issue, PointNet++ [
Download:Download high-res image (471KB)Download:Download full-size image
Download:Download high-res image (471KB)
Download:Download high-res image (471KB)
Download high-res image (471KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 2. Overall architecture of PointNet++ [16].
Fig. 2. Overall architecture of PointNet++ [16].
Fig. 2. Overall architecture of PointNet++ [16].
Based on the benchmark models, this study introduces a novel refinement technique for the final segmentation output. Traditionally, PointNet and PointNet++ assign the class label with the highest probability to each point. This study refines the final output by incorporating additional information from image segmentation results, as detailed inSection 3.3.

#### 3.2. YOLO-based image semantic segmentation

This section introduces YOLO-based image semantic segmentation.Section 3.2.1describes the generation of images from point clouds, whileSection 3.2.2explains image-based object detection and segmentation.

##### 3.2.1. Generation of images from point clouds

This study proposes a method for generating images directly from 3D point clouds, eliminating the need for external 2D–3D data registration. A virtual camera is employed to capture multiple images from the point cloud scene, along with corresponding depth and position information. Specifically, Open3D is used to visualize the entire scene, and images are generated from a virtual camera with various positions and angles. During the generation of each image, depth data is captured, and the camera's intrinsic andextrinsic parametersare recorded to establish the correspondence between the point cloud and the images. A schematic diagram illustrating this process is shown inFig. 3(a).Download:Download high-res image (848KB)Download:Download full-size imageFig. 3. Workflow of 2D–3D fusion for point cloud semantic segmentation. (a) Generation of images from point clouds using a virtual camera. (b) Object detection and segmentation results using YOLOv8. (c) Predicted category masks for detected objects. (d) Image-based voxel reconstruction. (e) Semantic label refinement.
This study proposes a method for generating images directly from 3D point clouds, eliminating the need for external 2D–3D data registration. A virtual camera is employed to capture multiple images from the point cloud scene, along with corresponding depth and position information. Specifically, Open3D is used to visualize the entire scene, and images are generated from a virtual camera with various positions and angles. During the generation of each image, depth data is captured, and the camera's intrinsic andextrinsic parametersare recorded to establish the correspondence between the point cloud and the images. A schematic diagram illustrating this process is shown inFig. 3(a).
This study proposes a method for generating images directly from 3D point clouds, eliminating the need for external 2D–3D data registration. A virtual camera is employed to capture multiple images from the point cloud scene, along with corresponding depth and position information. Specifically, Open3D is used to visualize the entire scene, and images are generated from a virtual camera with various positions and angles. During the generation of each image, depth data is captured, and the camera's intrinsic andextrinsic parametersare recorded to establish the correspondence between the point cloud and the images. A schematic diagram illustrating this process is shown in
Download:Download high-res image (848KB)Download:Download full-size image
Download:Download high-res image (848KB)
Download:Download high-res image (848KB)
Download high-res image (848KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 3. Workflow of 2D–3D fusion for point cloud semantic segmentation. (a) Generation of images from point clouds using a virtual camera. (b) Object detection and segmentation results using YOLOv8. (c) Predicted category masks for detected objects. (d) Image-based voxel reconstruction. (e) Semantic label refinement.
Fig. 3. Workflow of 2D–3D fusion for point cloud semantic segmentation. (a) Generation of images from point clouds using a virtual camera. (b) Object detection and segmentation results using YOLOv8. (c) Predicted category masks for detected objects. (d) Image-based voxel reconstruction. (e) Semantic label refinement.
Fig. 3. Workflow of 2D–3D fusion for point cloud semantic segmentation. (a) Generation of images from point clouds using a virtual camera. (b) Object detection and segmentation results using YOLOv8. (c) Predicted category masks for detected objects. (d) Image-based voxel reconstruction. (e) Semantic label refinement.

##### 3.2.2. Image-based object detection and segmentation

YOLOv8 advances image-based object detection and segmentation, combining speed and accuracy in a single framework. Built on aCNN, YOLOv8 predictsbounding boxesand class probabilities directly, dividing images into grids where each cell identifies objects and confidence scores. For segmentation, YOLOv8 includes a branch that generates pixel-level object masks by refining feature maps from earlier layers. The architecture's “neck” component enhances spatial and contextual information, enabling efficient and precise detection and segmentation.
YOLOv8 advances image-based object detection and segmentation, combining speed and accuracy in a single framework. Built on aCNN, YOLOv8 predicts
To address the challenges posed by point cloud-generated images, which often suffer from sparse and incomplete pixel information, a fine-tuned YOLOv8 model is employed. Initially, this model was trained on the COCO dataset [70], a large and diverse collection of real-world images, ensuring its robustness and generalizability. The model is then fine-tuned on images derived from the S3DIS dataset, specifically tailored for point cloud-based image analysis. Subsequently, the fine-tuned YOLOv8 model is used to predict objects in the images captured from Open3D, as illustrated inFig. 3(b). The predicted category masks are extracted, as shown inFig. 3(c).

#### 3.3. 2D–3D fusion-based point cloud semantic segmentation

This section introduces a method to improve the accuracy of point cloud semantic segmentation by updating confidence levels based on image-based semantic segmentation results. The approach consists of two steps: projecting 2D segmentation onto the 3D point cloud and fusing 2D and 3D segmentation results, as detailed inSection 3.3.1andSection 3.3.2, respectively.

##### 3.3.1. Projection of 2D segmentation to 3D point cloud

In the world coordinate system, given a point cloud (PC), the intrinsic matrix (K) of the camera, and the extrinsic matrix (E), the corresponding image (I) captured by the camera, along with its associated depth map (D), can be obtained. This relationship can be expressed as:(1)ID=fPCKE
For a single image, object detection is performed using the YOLO model, resulting in a masked imageI′that retains only the detected objects. To reconstruct the detected objects in 3D space, theinverse functionf−1is applied. This process reconstructs a new point cloud (PC’), containing only the segmented objects, as defined by the following equation:(2)PC′=f−1I′DKE
that retains only the detected objects. To reconstruct the detected objects in 3D space, theinverse function
An example of this 2D-to-3D projection onto a point cloud is illustrated inFig. 4, which demonstrates the process of extracting a single image from the point cloud, applying YOLO to obtain a segmentation mask, and reprojecting the 2D segmentation onto the 3D point cloud.Download:Download high-res image (598KB)Download:Download full-size imageFig. 4. Projection of 2D segmented objects onto a 3D point cloud.
An example of this 2D-to-3D projection onto a point cloud is illustrated inFig. 4, which demonstrates the process of extracting a single image from the point cloud, applying YOLO to obtain a segmentation mask, and reprojecting the 2D segmentation onto the 3D point cloud.
Download:Download high-res image (598KB)Download:Download full-size image
Download:Download high-res image (598KB)
Download:Download high-res image (598KB)
Download high-res image (598KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 4. Projection of 2D segmented objects onto a 3D point cloud.
Fig. 4. Projection of 2D segmented objects onto a 3D point cloud.
Fig. 4. Projection of 2D segmented objects onto a 3D point cloud.
After generating the point cloud from a single predicted image, a voxel grid is constructed to aggregate points from point clouds generated from different image views. The voxel grid helps resolve conflicts when multiple points from different views fall within the same voxel. In such cases, a majority-voting scheme is applied to assign the voxel's label, ensuring that the category with the highest number of points within that voxel is selected.
As illustrated inFig. 5, there are four pixels from different images (A-D) corresponding to Voxel P or Voxel Q, respectively. For Voxel P, all the four pixels from different images are classified as Class 1. Hence, the category for Voxel P is updated to Class 1. On the other hand, for Voxel Q, two pixels are classified as Class 2, and the other two pixels are classified as Class 1 and 3, respectively. In this case, the category for Voxel Q is updated to the majority class, which is Class 2.Download:Download high-res image (278KB)Download:Download full-size imageFig. 5. Voxel label assignment using majority voting.
As illustrated inFig. 5, there are four pixels from different images (A-D) corresponding to Voxel P or Voxel Q, respectively. For Voxel P, all the four pixels from different images are classified as Class 1. Hence, the category for Voxel P is updated to Class 1. On the other hand, for Voxel Q, two pixels are classified as Class 2, and the other two pixels are classified as Class 1 and 3, respectively. In this case, the category for Voxel Q is updated to the majority class, which is Class 2.
Download:Download high-res image (278KB)Download:Download full-size image
Download:Download high-res image (278KB)
Download:Download high-res image (278KB)
Download high-res image (278KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 5. Voxel label assignment using majority voting.
Fig. 5. Voxel label assignment using majority voting.
Fig. 5. Voxel label assignment using majority voting.

##### 3.3.2. Fusion of 2D and 3D segmentation results

In this section, a confidence-based label refinement algorithm is introduced to enhance the semantic segmentation accuracy of point clouds. The refinement process integrates predictions from two primary sources: (1) point-wise classification probabilities derived from the PointNet family model, and (2) image-based object detection confidence scores obtained from YOLO, which are then projected onto the 3D point cloud. The confidence scores from image-based semantic segmentation are utilized to update the confidence levels in the point cloud semantic segmentation.
For each pointλin the point cloud, the point-wise classification probabilities derived from PointNet family are represented as:(3)PPointNet familyλ=p1λp2λ⋯pnλwherepiλis the predicted probability that pointλbelongs to thei-th class, and the sum of all probabilities is 1∑i=1npiλ=1, wherenis the total number of classes.
(3)PPointNet familyλ=p1λp2λ⋯pnλ
(3)PPointNet familyλ=p1λp2λ⋯pnλ
PPointNet familyλ=p1λp2λ⋯pnλ
PPointNet familyλ=p1λp2λ⋯pnλ
PPointNet familyλ=p1λp2λ⋯pnλ
Similarly, the YOLO predictions, when projected onto the 3D point cloud, provide a confidence score for each predicted object class. For a given pointλin the 3D space that corresponds to a 2D detection by YOLO, the YOLO-based confidencePYOLOλis as follows:(4)PYOLOλ=q1λq2λ⋯qnλwhereqiλis the confidence score that point λ belongs to thei-th class based on YOLO predictions.
For each pointλin the point clouds, the final class confidence is determined by summing the individual confidence values from PointNet family and YOLO for each class:(5)Pcombinedλ=PPointNet familyλ+PYOLOλ
(5)Pcombinedλ=PPointNet familyλ+PYOLOλ
(5)Pcombinedλ=PPointNet familyλ+PYOLOλ
Pcombinedλ=PPointNet familyλ+PYOLOλ
Pcombinedλ=PPointNet familyλ+PYOLOλ
Pcombinedλ=PPointNet familyλ+PYOLOλ
To determine the label of point λ, identify the class with the maximum confidence in bothPPointNet familyλandPcombinedλ:
The class with the highest confidence inPPointNet familyλis:(6)CPointNet familyλ=argmaxPPointNet familyλ
(6)CPointNet familyλ=argmaxPPointNet familyλ
(6)CPointNet familyλ=argmaxPPointNet familyλ
CPointNet familyλ=argmaxPPointNet familyλ
CPointNet familyλ=argmaxPPointNet familyλ
CPointNet familyλ=argmaxPPointNet familyλ
The class with the highest confidence inPcombinedλis(7)Ccombinedλ=argmaxPcombinedλ
(7)Ccombinedλ=argmaxPcombinedλ
(7)Ccombinedλ=argmaxPcombinedλ
Ccombinedλ=argmaxPcombinedλ
Ccombinedλ=argmaxPcombinedλ
Ccombinedλ=argmaxPcombinedλ
The decision to update the semantic label of point λ is based on a comparison of the confidence scores for these two classes:(8)PcombinedλCcombinedλandPPointNet familyλCPointNet familyλ
(8)PcombinedλCcombinedλandPPointNet familyλCPointNet familyλ
(8)PcombinedλCcombinedλandPPointNet familyλCPointNet familyλ
PcombinedλCcombinedλandPPointNet familyλCPointNet familyλ
PcombinedλCcombinedλandPPointNet familyλCPointNet familyλ
PcombinedλCcombinedλandPPointNet familyλCPointNet familyλ
If the confidence of the combined label is greater than that of the original PointNet family label, update the label toCcombinedλ; otherwise, retain the label asCPointNet familyλ:(9)Labelλ=Ccombinedλ,ifPcombinedλCcombinedλ>PPointNet familyλCPointNet familyλCPointNet familyλ,otherwise
(9)Labelλ=Ccombinedλ,ifPcombinedλCcombinedλ>PPointNet familyλCPointNet familyλCPointNet familyλ,otherwise
(9)Labelλ=Ccombinedλ,ifPcombinedλCcombinedλ>PPointNet familyλCPointNet familyλCPointNet familyλ,otherwise
Labelλ=Ccombinedλ,ifPcombinedλCcombinedλ>PPointNet familyλCPointNet familyλCPointNet familyλ,otherwise
Labelλ=Ccombinedλ,ifPcombinedλCcombinedλ>PPointNet familyλCPointNet familyλCPointNet familyλ,otherwise
Labelλ=Ccombinedλ,ifPcombinedλCcombinedλ>PPointNet familyλCPointNet familyλCPointNet familyλ,otherwise
The algorithm for updating point cloud labels is detailed in Algorithm 1.Algorithm 1Point cloud semantic update algorithmInput: A point cloud consisting of points λ, with confidence scores from PointNet family (PPointNet family) and YOLO (PYOLO).Output: Refined semantic labels for each point λ.For each pointλin the point cloud:1.Initialize the combined confidence scores:Pcombined(λ) ← [0, 0, …, 0](An array of size num_classes to store combined confidence scores.)2.For each classi∈ [1, …, num_classes]:▪Pcombined(λ,i) ←PPointNet family(λ,i) +PYOLO(λ,i)3.Determine:▪Find the index of the maximum value inPPointNet familyCPointNet family(λ) ← argmax (PPointNet family(λ))▪Find the index of the maximum value inPcombinedCcombined(λ) ← argmax (Pcombined(λ))4.Update Label if Necessary:IfPcombined(λ,Ccombined(λ)) >PPointNet family(λ,CPointNet family(λ)):▪Updatethe point's label toCcombined(λ).Else:▪Retainthe original labelCPointNet family(λ).
Point cloud semantic update algorithm
Input: A point cloud consisting of points λ, with confidence scores from PointNet family (PPointNet family) and YOLO (PYOLO).
Output: Refined semantic labels for each point λ.
For each pointλin the point cloud:1.Initialize the combined confidence scores:Pcombined(λ) ← [0, 0, …, 0](An array of size num_classes to store combined confidence scores.)2.For each classi∈ [1, …, num_classes]:▪Pcombined(λ,i) ←PPointNet family(λ,i) +PYOLO(λ,i)3.Determine:▪Find the index of the maximum value inPPointNet familyCPointNet family(λ) ← argmax (PPointNet family(λ))▪Find the index of the maximum value inPcombinedCcombined(λ) ← argmax (Pcombined(λ))4.Update Label if Necessary:IfPcombined(λ,Ccombined(λ)) >PPointNet family(λ,CPointNet family(λ)):▪Updatethe point's label toCcombined(λ).Else:▪Retainthe original labelCPointNet family(λ).
Initialize the combined confidence scores:Pcombined(λ) ← [0, 0, …, 0](An array of size num_classes to store combined confidence scores.)
Initialize the combined confidence scores:
Pcombined(λ) ← [0, 0, …, 0]
(An array of size num_classes to store combined confidence scores.)
For each classi∈ [1, …, num_classes]:▪Pcombined(λ,i) ←PPointNet family(λ,i) +PYOLO(λ,i)
For each classi∈ [1, …, num_classes]:▪Pcombined(λ,i) ←PPointNet family(λ,i) +PYOLO(λ,i)
Pcombined(λ,i) ←PPointNet family(λ,i) +PYOLO(λ,i)
Pcombined(λ,i) ←PPointNet family(λ,i) +PYOLO(λ,i)
Determine:▪Find the index of the maximum value inPPointNet familyCPointNet family(λ) ← argmax (PPointNet family(λ))▪Find the index of the maximum value inPcombinedCcombined(λ) ← argmax (Pcombined(λ))
Determine:▪Find the index of the maximum value inPPointNet familyCPointNet family(λ) ← argmax (PPointNet family(λ))▪Find the index of the maximum value inPcombinedCcombined(λ) ← argmax (Pcombined(λ))
Find the index of the maximum value inPPointNet familyCPointNet family(λ) ← argmax (PPointNet family(λ))
Find the index of the maximum value inPPointNet family
CPointNet family(λ) ← argmax (PPointNet family(λ))
Find the index of the maximum value inPcombinedCcombined(λ) ← argmax (Pcombined(λ))
Find the index of the maximum value inPcombined
Ccombined(λ) ← argmax (Pcombined(λ))
Update Label if Necessary:IfPcombined(λ,Ccombined(λ)) >PPointNet family(λ,CPointNet family(λ)):▪Updatethe point's label toCcombined(λ).Else:▪Retainthe original labelCPointNet family(λ).
Update Label if Necessary:
IfPcombined(λ,Ccombined(λ)) >PPointNet family(λ,CPointNet family(λ)):▪Updatethe point's label toCcombined(λ).
Updatethe point's label toCcombined(λ).
Updatethe point's label toCcombined(λ).
Else:▪Retainthe original labelCPointNet family(λ).
Retainthe original labelCPointNet family(λ).
Retainthe original labelCPointNet family(λ).
Fig. 6shows examples of the semantic label updating process. For point 1 from a door, when using PointNet for semantic segmentation, this point could be misclassified as board, as the semantic score for board was the highest (0.303) among all classes. When using the PointNet-YOLO method, YOLO would classify this point as door with a confidence score of 0.65. Hence, summing the semantic score from PointNet and confidence score from YOLO, door would have the highest score (0.284 + 0.65 = 0.934). In this case, this point would be correctly classified as door.Download:Download high-res image (2MB)Download:Download full-size imageFig. 6. Example of semantic information update process using the Point-YOLO method.
Fig. 6shows examples of the semantic label updating process. For point 1 from a door, when using PointNet for semantic segmentation, this point could be misclassified as board, as the semantic score for board was the highest (0.303) among all classes. When using the PointNet-YOLO method, YOLO would classify this point as door with a confidence score of 0.65. Hence, summing the semantic score from PointNet and confidence score from YOLO, door would have the highest score (0.284 + 0.65 = 0.934). In this case, this point would be correctly classified as door.
Download:Download high-res image (2MB)Download:Download full-size image
Download:Download high-res image (2MB)
Download:Download high-res image (2MB)
Download high-res image (2MB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 6. Example of semantic information update process using the Point-YOLO method.
Fig. 6. Example of semantic information update process using the Point-YOLO method.
Fig. 6. Example of semantic information update process using the Point-YOLO method.
For point 2 from a wall, this point is correctly classified as wall by PointNet with the highest semantic score of 0.641. However, YOLO classifies the point as chair with a confidence score of 0.51. In this case, the summed score for chair becomes 0.566 (0.056 + 0.51), still lower than wall (0.641). Therefore, the final classification of this point is still wall.

### 4. Experimental settings

This section presents the experimental settings.Section 4.1introduces the scenes and datasets,Section 4.2describes the algorithms and implementation details, andSection 4.3outlines the evaluation metrics.

#### 4.1. Scenes and datasets

The proposed Point-YOLO method is designed for building indoor scenes and is applicable to both purely architectural interiors and environments where building and MEP components coexist. To evaluate its performance, this study tests the method in two different scenarios: Scene 1, a typical building interior, and Scene 2, an underground garage containing both MEP and building elements. For Scene 1, the S3DIS dataset was used, which consists of six areas (Area 1 to Area 6) and includes 13 semantic categories: ceiling, floor, wall, beam, column, window, door, table, chair, sofa, bookcase, board, and clutter. Scene 2 utilized the Underground Garage Dataset (UGD), which was scanned using a Leica RTC360 scanner in the underground garage of the Humanities Building at Southeast University. The dataset contains point clouds with XYZ coordinates, RGB values, and density information but lacks registeredimage data. The UGD dataset is divided into five areas (Area 1 to Area 5) and includes 10 component categories:air duct,cable tray, ceiling, fire hydrant, floor, lamp, pillar, pipe, wall, and door. Scene segmentation in UGD is particularly challenging due to the combination of large, regular architectural components (e.g., ceilings, floors) and smaller, complex MEP elements.Fig.7presents visualizations of both datasets in Autodesk Recap. The datasets were split into training and test sets. For Scene 1, Areas 1, 2, 3, 4, and 6 were used for training, while 42 office scenes from Area 5 served as the test set. For Scene 2, Areas 1, 2, and 5 were used for training, with Areas 3 and 4 as the test set.Download:Download high-res image (811KB)Download:Download full-size imageFig. 7. Point cloud scenes in Autodesk Recap. (a) Scene 1: S3DIS. (b) Scene 2:UGD.
The proposed Point-YOLO method is designed for building indoor scenes and is applicable to both purely architectural interiors and environments where building and MEP components coexist. To evaluate its performance, this study tests the method in two different scenarios: Scene 1, a typical building interior, and Scene 2, an underground garage containing both MEP and building elements. For Scene 1, the S3DIS dataset was used, which consists of six areas (Area 1 to Area 6) and includes 13 semantic categories: ceiling, floor, wall, beam, column, window, door, table, chair, sofa, bookcase, board, and clutter. Scene 2 utilized the Underground Garage Dataset (UGD), which was scanned using a Leica RTC360 scanner in the underground garage of the Humanities Building at Southeast University. The dataset contains point clouds with XYZ coordinates, RGB values, and density information but lacks registeredimage data. The UGD dataset is divided into five areas (Area 1 to Area 5) and includes 10 component categories:air duct,cable tray, ceiling, fire hydrant, floor, lamp, pillar, pipe, wall, and door. Scene segmentation in UGD is particularly challenging due to the combination of large, regular architectural components (e.g., ceilings, floors) and smaller, complex MEP elements.Fig.7presents visualizations of both datasets in Autodesk Recap. The datasets were split into training and test sets. For Scene 1, Areas 1, 2, 3, 4, and 6 were used for training, while 42 office scenes from Area 5 served as the test set. For Scene 2, Areas 1, 2, and 5 were used for training, with Areas 3 and 4 as the test set.
The proposed Point-YOLO method is designed for building indoor scenes and is applicable to both purely architectural interiors and environments where building and MEP components coexist. To evaluate its performance, this study tests the method in two different scenarios: Scene 1, a typical building interior, and Scene 2, an underground garage containing both MEP and building elements. For Scene 1, the S3DIS dataset was used, which consists of six areas (Area 1 to Area 6) and includes 13 semantic categories: ceiling, floor, wall, beam, column, window, door, table, chair, sofa, bookcase, board, and clutter. Scene 2 utilized the Underground Garage Dataset (UGD), which was scanned using a Leica RTC360 scanner in the underground garage of the Humanities Building at Southeast University. The dataset contains point clouds with XYZ coordinates, RGB values, and density information but lacks registeredimage data. The UGD dataset is divided into five areas (Area 1 to Area 5) and includes 10 component categories:air duct,cable tray, ceiling, fire hydrant, floor, lamp, pillar, pipe, wall, and door. Scene segmentation in UGD is particularly challenging due to the combination of large, regular architectural components (e.g., ceilings, floors) and smaller, complex MEP elements.
The proposed Point-YOLO method is designed for building indoor scenes and is applicable to both purely architectural interiors and environments where building and MEP components coexist. To evaluate its performance, this study tests the method in two different scenarios: Scene 1, a typical building interior, and Scene 2, an underground garage containing both MEP and building elements. For Scene 1, the S3DIS dataset was used, which consists of six areas (Area 1 to Area 6) and includes 13 semantic categories: ceiling, floor, wall, beam, column, window, door, table, chair, sofa, bookcase, board, and clutter. Scene 2 utilized the Underground Garage Dataset (UGD), which was scanned using a Leica RTC360 scanner in the underground garage of the Humanities Building at Southeast University. The dataset contains point clouds with XYZ coordinates, RGB values, and density information but lacks registeredimage data. The UGD dataset is divided into five areas (Area 1 to Area 5) and includes 10 component categories:
,cable tray, ceiling, fire hydrant, floor, lamp, pillar, pipe, wall, and door. Scene segmentation in UGD is particularly challenging due to the combination of large, regular architectural components (e.g., ceilings, floors) and smaller, complex MEP elements.
Download:Download high-res image (811KB)Download:Download full-size image
Download:Download high-res image (811KB)
Download:Download high-res image (811KB)
Download high-res image (811KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 7. Point cloud scenes in Autodesk Recap. (a) Scene 1: S3DIS. (b) Scene 2:UGD.
Fig. 7. Point cloud scenes in Autodesk Recap. (a) Scene 1: S3DIS. (b) Scene 2:UGD.
Fig. 7. Point cloud scenes in Autodesk Recap. (a) Scene 1: S3DIS. (b) Scene 2:UGD.

#### 4.2. Algorithms and implementation

In this study, PointNet and PointNet++ were selected as the baseline algorithms for the experiments. Subsequently, PointNext and PointVector were used as comparative algorithms to further evaluate the performance of the proposed algorithm. Initially, experiments were conducted using DL algorithms, such as PointNet and PointNet++. Subsequently, Point-YOLO algorithms were tested. Specifically, PointNet-YOLO refers to the integration of the YOLO algorithm with the PointNet algorithm, while PointNet++-YOLO refers to the integration of the YOLO algorithm with the PointNet++ algorithm.
For image annotation and training, 415 images were randomly selected from the training set of Scene 1, with one to three images captured from each room. These images were annotated across eight categories: beam, column, window, door, table, chair, sofa, and board. These categories were chosen due to their relative scarcity in the S3DIS dataset, which makes them more susceptible to misclassification as more prevalent categories, such as floor, wall, and ceiling. In Scene 2, a total of 90 images were randomly captured from each area of the training set. These images were annotated with seven categories: air duct, cable tray, fire hydrant, lamp, column, pipe, and door. Similar to Scene 1, these components are less common compared to larger, more frequently represented categories, making them prone to misclassification. Following annotation, the YOLOv8 model was employed for training. Of the annotated images, 90 % were used for training, while the remaining 10 % were reserved for validation. Pre-trained models based on the COCO dataset were utilized as the initial training models.
During image semantic segmentation, for Scene 1, each room in the test set was photographed using a virtual camera. A virtual camera was positioned at the center of the scene, and eight photos were taken at 45-degree intervals to complete a 360-degree sweep. The camera was then moved by 0.5 m in each direction (forward, backward, left, right), capturing an additional 32 photos. This approach resulted in a total of 40 photos per room, effectively covering the entire scene. In Scene 2, due to the larger space, the central point approach proved insufficient. Therefore, manual photography was employed using Open3D. A total of 106 images were captured across the two test areas in Scene 2.
The image training process lasted 200 epochs, while the point cloud training process extended to 100 epochs. All experiments were conducted on a single NVIDIA L40GPU, with training parameters set to themodel's defaultsettings.
The image training process lasted 200 epochs, while the point cloud training process extended to 100 epochs. All experiments were conducted on a single NVIDIA L40GPU, with training parameters set to the

#### 4.3. Evaluation metrics

In this paper, the confusion matrix (CM), OA, Precision, Recall, F1 Score, IoU, and mIoU, and mean class Accuracy (mAcc) were utilized as evaluation metrics to assess the performance of DL algorithms.
CM is a fundamental tool in evaluating the performance of classification models. CM provides a detailed breakdown of the model's predictions compared to the ground-truth labels, consisting of four key components: True Positives (TP), False Positives (FP), True Negatives (TN), andFalse Negatives(FN).
mAcc is another important metric that assesses the model's performance by computing the accuracy for each class individually and then averaging these accuracies. This approach ensures that classes with fewer instances are not overshadowed by more frequent classes. mAcc is defined as:(10)mAcc=1N∑i=1NTPiTPi+FPi+FNiwhereNis the number of classes, andTPi,FPi_andFNiare the true positives, false positives, and false negatives for classi, respectively.
(10)mAcc=1N∑i=1NTPiTPi+FPi+FNi
(10)mAcc=1N∑i=1NTPiTPi+FPi+FNi
mAcc=1N∑i=1NTPiTPi+FPi+FNi
mAcc=1N∑i=1NTPiTPi+FPi+FNi
mAcc=1N∑i=1NTPiTPi+FPi+FNi
OA measures the proportion of correctly classified samples out of the total number of samples and is defined as:(11)OA=TP+TNTP+TN+FP+FN
(11)OA=TP+TNTP+TN+FP+FN
(11)OA=TP+TNTP+TN+FP+FN
Precision is the ratio of correctly predicted positive observations to the total predicted positives:(12)Precision=TPTP+FP
(12)Precision=TPTP+FP
(12)Precision=TPTP+FP
Recall is the ratio of correctly predicted positive observations to all observations in the actual class:(13)Recall=TPTP+FN
F1-Score represents the harmonic mean of Precision and Recall. It considers both false positives and false negatives in its calculation, and is defined as:(14)F1−Score=2×Precision·RecallPrecision+Recall
(14)F1−Score=2×Precision·RecallPrecision+Recall
(14)F1−Score=2×Precision·RecallPrecision+Recall
F1−Score=2×Precision·RecallPrecision+Recall
F1−Score=2×Precision·RecallPrecision+Recall
F1−Score=2×Precision·RecallPrecision+Recall
IoU quantifies the degree of overlap between the predicted segmentation and the ground truth, and is defined as:(15)IoU=TPTP+FP+FN
mIoU is the average IoU across all classes, providing an overall measure of segmentation performance. It is calculated as:(16)mIoU=1N∑i=1NIoUi

### 5. Experimental results

The experiments were organized into four parts. First, the performance of four algorithms including PointNet, PointNet++, PointNet-YOLO, and PointNet++-YOLO was compared for semantic segmentation in Scene 1 and Scene 2, as illustrated in5.1 Results on S3DIS dataset,5.2 Results on the UGD dataset, respectively. Then, the performance of PointNet++ and PointNet++-YOLO with varying sizes of point cloud training samples was assessed in Scene 1, as discussed inSection 5.3. Finally, the effectiveness and generalizability of the proposed algorithms are validated by deploying them on more advanced DL algorithms including PointNext and PointVector, as detailed inSection 5.4.
5.1 Results on S3DIS dataset
5.1 Results on S3DIS dataset
5.2 Results on the UGD dataset
5.2 Results on the UGD dataset

#### 5.1. Results on S3DIS dataset

This section presents the results of testing four DL algorithms on the S3DIS dataset. The comparative results of PointNet and PointNet-YOLO are shown inTable 2, while the comparison between PointNet++ and PointNet++-YOLO is detailed inTable 3.Table 2. Performance comparison between PointNet and PointNet-YOLO on the S3DIS dataset.ModelsmAccOAmIoUIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPointNet37.4063.5428.8174.2296.1646.130012.750.4348.1823.29040.8213.6218.88PointNet-YOLO59.3168.6440.4874.3796.2149.673.348.0155.2038.9949.9480.6229.2042.9658.4719.87Increment21.915.1011.670.150.053.543.348.0142.4538.561.7657.3329.202.1444.850.99Table 3. Performance comparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.ModelsmAccOAmIoUIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPointNet++69.1084.7161.2791.3697.7272.130.0025.8761.7739.7176.6980.6264.2571.0866.7148.63PointNet++-YOLO83.6286.9067.9691.3797.6876.737.3433.2281.1277.3676.4879.8163.7471.3778.2349.01Increment14.522.196.690.01−0.044.607.347.3519.3537.65−0.21−0.81−0.510.2911.520.38
This section presents the results of testing four DL algorithms on the S3DIS dataset. The comparative results of PointNet and PointNet-YOLO are shown inTable 2, while the comparison between PointNet++ and PointNet++-YOLO is detailed inTable 3.
Table 2. Performance comparison between PointNet and PointNet-YOLO on the S3DIS dataset.ModelsmAccOAmIoUIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPointNet37.4063.5428.8174.2296.1646.130012.750.4348.1823.29040.8213.6218.88PointNet-YOLO59.3168.6440.4874.3796.2149.673.348.0155.2038.9949.9480.6229.2042.9658.4719.87Increment21.915.1011.670.150.053.543.348.0142.4538.561.7657.3329.202.1444.850.99
Table 2. Performance comparison between PointNet and PointNet-YOLO on the S3DIS dataset.
Table 2. Performance comparison between PointNet and PointNet-YOLO on the S3DIS dataset.
Table 2. Performance comparison between PointNet and PointNet-YOLO on the S3DIS dataset.
ModelsmAccOAmIoUIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPointNet37.4063.5428.8174.2296.1646.130012.750.4348.1823.29040.8213.6218.88PointNet-YOLO59.3168.6440.4874.3796.2149.673.348.0155.2038.9949.9480.6229.2042.9658.4719.87Increment21.915.1011.670.150.053.543.348.0142.4538.561.7657.3329.202.1444.850.99
Table 3. Performance comparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.ModelsmAccOAmIoUIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPointNet++69.1084.7161.2791.3697.7272.130.0025.8761.7739.7176.6980.6264.2571.0866.7148.63PointNet++-YOLO83.6286.9067.9691.3797.6876.737.3433.2281.1277.3676.4879.8163.7471.3778.2349.01Increment14.522.196.690.01−0.044.607.347.3519.3537.65−0.21−0.81−0.510.2911.520.38
Table 3. Performance comparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.
Table 3. Performance comparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.
Table 3. Performance comparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.
ModelsmAccOAmIoUIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPointNet++69.1084.7161.2791.3697.7272.130.0025.8761.7739.7176.6980.6264.2571.0866.7148.63PointNet++-YOLO83.6286.9067.9691.3797.6876.737.3433.2281.1277.3676.4879.8163.7471.3778.2349.01Increment14.522.196.690.01−0.044.607.347.3519.3537.65−0.21−0.81−0.510.2911.520.38
As depicted inTable 2, PointNet-YOLO had a better performance than PointNet for all the metrics, with mAcc improved by 21.91 %, mIoU improved by 11.67 %, and OA improved by 5.1 %. Although PointNet achieved high mIoU scores for some large, well-defined components such as floor and ceiling, its performance was notably lower for components with fewer instances, such as windows and doors. Consequently, the overall metrics yielded only 28.81 % mIoU and 37.4 % mAcc. In contrast, PointNet-YOLO significantly improved accuracy for window and door, with increases of 42.45 % and 38.56 %, respectively. Additionally, categories such as chair, sofa, bookcase, column, beam, wall, and table also showed improvements.
According toTable 3, PointNet++-YOLO showed an overall improvement of 14.52 % in mAcc, 6.69 % in mIoU, and 2.19 % in OA compared to PointNet++. Notable improvements were observed in categories such as door, window, board, column, and beam, with mIoU increases of 37.65 %, 19.35 %, 11.52 %, 7.35 %, and 7.34 %, respectively. Even categories without image prediction, such as wall, clutter, bookcase, and ceiling, experienced slight improvements. This is likely due to the increased prediction accuracy for image-annotated categories, which reduced misclassification probabilities and improved overall precision and mIoU. Despite these positive results, a slight decrease in accuracy was observed for chairs, tables, and sofas. This reduction may be attributed to PointNet++ already achieving high mIoU values for chairs and tables, while errors in YOLO could arise from misclassifying walls and floor points as chairs, tables, or sofas. For sofas, the decreased accuracy may be related to the limited number of annotated images, which is insufficient for robust training.
In addition to OA and mIoU,Table 4further presents the Precision, Recall, and F1-Score for PointNet++ and PointNet++-YOLO. PointNet++-YOLO outperformed PointNet++ in overall Precision, Recall, and F1-Score. Although PointNet++ exhibited higher Precision for window and door, the increase in Recall for these categories with PointNet++-YOLO was more substantial, leading to overall improved F1-score metrics.Table 4. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on S3DIS Dataset.MetricsAlgorithmsOverallCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPrecisionPointNet++84.9593.1298.2777.010.0058.3091.6980.0685.6594.4190.6987.6975.8768.39PointNet++-YOLO86.8993.3398.3284.187.3761.9087.2179.7484.9392.9690.6488.9883.0969.79RecallPointNet++84.7197.9799.4391.920.0031.7465.4344.0687.9984.6668.7878.9684.6862.72PointNet++-YOLO86.9097.7599.3489.6695.0641.7692.0796.2988.4884.9468.2378.2993.0462.20F1-ScorePointNet++84.2995.4998.8583.810.0041.1076.3756.8486.8189.2778.2383.1080.0365.43PointNet++-YOLO86.6995.4998.8286.8313.6849.8789.5787.2486.6788.7777.8583.3087.7965.78
In addition to OA and mIoU,Table 4further presents the Precision, Recall, and F1-Score for PointNet++ and PointNet++-YOLO. PointNet++-YOLO outperformed PointNet++ in overall Precision, Recall, and F1-Score. Although PointNet++ exhibited higher Precision for window and door, the increase in Recall for these categories with PointNet++-YOLO was more substantial, leading to overall improved F1-score metrics.
Table 4. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on S3DIS Dataset.MetricsAlgorithmsOverallCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPrecisionPointNet++84.9593.1298.2777.010.0058.3091.6980.0685.6594.4190.6987.6975.8768.39PointNet++-YOLO86.8993.3398.3284.187.3761.9087.2179.7484.9392.9690.6488.9883.0969.79RecallPointNet++84.7197.9799.4391.920.0031.7465.4344.0687.9984.6668.7878.9684.6862.72PointNet++-YOLO86.9097.7599.3489.6695.0641.7692.0796.2988.4884.9468.2378.2993.0462.20F1-ScorePointNet++84.2995.4998.8583.810.0041.1076.3756.8486.8189.2778.2383.1080.0365.43PointNet++-YOLO86.6995.4998.8286.8313.6849.8789.5787.2486.6788.7777.8583.3087.7965.78
Table 4. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on S3DIS Dataset.
Table 4. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on S3DIS Dataset.
Table 4. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on S3DIS Dataset.
MetricsAlgorithmsOverallCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPrecisionPointNet++84.9593.1298.2777.010.0058.3091.6980.0685.6594.4190.6987.6975.8768.39PointNet++-YOLO86.8993.3398.3284.187.3761.9087.2179.7484.9392.9690.6488.9883.0969.79RecallPointNet++84.7197.9799.4391.920.0031.7465.4344.0687.9984.6668.7878.9684.6862.72PointNet++-YOLO86.9097.7599.3489.6695.0641.7692.0796.2988.4884.9468.2378.2993.0462.20F1-ScorePointNet++84.2995.4998.8583.810.0041.1076.3756.8486.8189.2778.2383.1080.0365.43PointNet++-YOLO86.6995.4998.8286.8313.6849.8789.5787.2486.6788.7777.8583.3087.7965.78
Moreover, the CM comparison between PointNet++ and PointNet++-YOLO is illustrated inFig. 8. In PointNet++, the recognition rate for beams was 0 %, while in PointNet++-YOLO, 95.06 % of beams were correctly identified. Similarly, in PointNet++, doors and windows were frequently misclassified as walls, whereas in PointNet++-YOLO, over 92 % of windows and over 96 % of doors were accurately identified. All of these improvements demonstrated the effectiveness of the proposed method.Download:Download high-res image (617KB)Download:Download full-size imageFig. 8.CMcomparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.
Moreover, the CM comparison between PointNet++ and PointNet++-YOLO is illustrated inFig. 8. In PointNet++, the recognition rate for beams was 0 %, while in PointNet++-YOLO, 95.06 % of beams were correctly identified. Similarly, in PointNet++, doors and windows were frequently misclassified as walls, whereas in PointNet++-YOLO, over 92 % of windows and over 96 % of doors were accurately identified. All of these improvements demonstrated the effectiveness of the proposed method.
Download:Download high-res image (617KB)Download:Download full-size image
Download:Download high-res image (617KB)
Download:Download high-res image (617KB)
Download high-res image (617KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 8.CMcomparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.
Fig. 8.CMcomparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.
Fig. 8.CMcomparison between PointNet++ and PointNet++-YOLO on the S3DIS dataset.
Fig. 9provides an example of segmentation results from PointNet++ and PointNet++-YOLO. PointNet++ tended to misclassify windows and doors as other categories, while PointNet++-YOLO accurately identified most of the point clouds for these components.Download:Download high-res image (1MB)Download:Download full-size imageFig. 9. Example of segmentation results for PointNet++ and PointNet++-YOLO on S3DIS dataset.
Fig. 9provides an example of segmentation results from PointNet++ and PointNet++-YOLO. PointNet++ tended to misclassify windows and doors as other categories, while PointNet++-YOLO accurately identified most of the point clouds for these components.
Download:Download high-res image (1MB)Download:Download full-size image
Download:Download high-res image (1MB)
Download:Download high-res image (1MB)
Download high-res image (1MB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 9. Example of segmentation results for PointNet++ and PointNet++-YOLO on S3DIS dataset.
Fig. 9. Example of segmentation results for PointNet++ and PointNet++-YOLO on S3DIS dataset.
Fig. 9. Example of segmentation results for PointNet++ and PointNet++-YOLO on S3DIS dataset.
Further visual results are provided in Appendix I, which shows that PointNet++-YOLO significantly improved the recognition accuracy for less common and similar components, such as doors, windows, and columns.

#### 5.2. Results on the UGD dataset

This section presents the results of testing four DL algorithms on the UGD dataset. The comparative results of PointNet and PointNet-YOLO, as well as PointNet++ and PointNet++-YOLO, are shown inTable 5,Table 6.Table 5. Performance comparison between PointNet and PointNet-YOLO on UGD Dataset.ModelsmAccOAmIoUIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNet31.3778.8522.910081.97084.990029.3132.820PointNet-YOLO68.5684.756.9385.1924.883.8361.785.9418.2445.3335.637.6990.94Increment37.195.8534.0285.1924.81.8661.70.9518.2445.336.294.8790.94Table 6. Performance comparison between PointNet++ and PointNet++-YOLO on UGD Dataset.ModelsmAccOAmIoUIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNet++52.9890.9945.059.9234.6392.5535.2598.440079.1270.0330.57PointNet++-YOLO79.4795.0269.7685.0851.9394.3458.8298.4318.2445.3379.1180.2486.02Increment26.494.0324.7175.1617.31.7923.57−0.0118.2445.33−0.0110.2155.45
This section presents the results of testing four DL algorithms on the UGD dataset. The comparative results of PointNet and PointNet-YOLO, as well as PointNet++ and PointNet++-YOLO, are shown inTable 5,Table 6.
Table 5. Performance comparison between PointNet and PointNet-YOLO on UGD Dataset.ModelsmAccOAmIoUIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNet31.3778.8522.910081.97084.990029.3132.820PointNet-YOLO68.5684.756.9385.1924.883.8361.785.9418.2445.3335.637.6990.94Increment37.195.8534.0285.1924.81.8661.70.9518.2445.336.294.8790.94
Table 5. Performance comparison between PointNet and PointNet-YOLO on UGD Dataset.
Table 5. Performance comparison between PointNet and PointNet-YOLO on UGD Dataset.
Table 5. Performance comparison between PointNet and PointNet-YOLO on UGD Dataset.
ModelsmAccOAmIoUIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNet31.3778.8522.910081.97084.990029.3132.820PointNet-YOLO68.5684.756.9385.1924.883.8361.785.9418.2445.3335.637.6990.94Increment37.195.8534.0285.1924.81.8661.70.9518.2445.336.294.8790.94
Table 6. Performance comparison between PointNet++ and PointNet++-YOLO on UGD Dataset.ModelsmAccOAmIoUIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNet++52.9890.9945.059.9234.6392.5535.2598.440079.1270.0330.57PointNet++-YOLO79.4795.0269.7685.0851.9394.3458.8298.4318.2445.3379.1180.2486.02Increment26.494.0324.7175.1617.31.7923.57−0.0118.2445.33−0.0110.2155.45
Table 6. Performance comparison between PointNet++ and PointNet++-YOLO on UGD Dataset.
Table 6. Performance comparison between PointNet++ and PointNet++-YOLO on UGD Dataset.
Table 6. Performance comparison between PointNet++ and PointNet++-YOLO on UGD Dataset.
ModelsmAccOAmIoUIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNet++52.9890.9945.059.9234.6392.5535.2598.440079.1270.0330.57PointNet++-YOLO79.4795.0269.7685.0851.9394.3458.8298.4318.2445.3379.1180.2486.02Increment26.494.0324.7175.1617.31.7923.57−0.0118.2445.33−0.0110.2155.45
As indicated inTable 5, PointNet achieved high mIoU scores for ceiling and floor, with values of 81.97 % and 84.99 %, respectively. However, PointNet struggled with other MEP and building components, such asair ducts,cable trays, fire hydrants, lamps, columns, and doors, where the mIoUs were zero. This highlighted the difficulty of PointNet in recognizing less frequent components. After integrating the YOLO algorithm, there was a significant improvement in accuracy across all components, with mAcc increased by 37.19 % to 68.56 %, mIoU increased by 34.02 % to 56.93 %, and OA increased by 5.85 % to 84.7 %. Notably, the mIoUs for air ducts and doors saw the most substantial increases, rising by 85.19 % and 90.94 %, respectively.
, PointNet achieved high mIoU scores for ceiling and floor, with values of 81.97 % and 84.99 %, respectively. However, PointNet struggled with other MEP and building components, such asair ducts,cable trays, fire hydrants, lamps, columns, and doors, where the mIoUs were zero. This highlighted the difficulty of PointNet in recognizing less frequent components. After integrating the YOLO algorithm, there was a significant improvement in accuracy across all components, with mAcc increased by 37.19 % to 68.56 %, mIoU increased by 34.02 % to 56.93 %, and OA increased by 5.85 % to 84.7 %. Notably, the mIoUs for air ducts and doors saw the most substantial increases, rising by 85.19 % and 90.94 %, respectively.
,cable trays, fire hydrants, lamps, columns, and doors, where the mIoUs were zero. This highlighted the difficulty of PointNet in recognizing less frequent components. After integrating the YOLO algorithm, there was a significant improvement in accuracy across all components, with mAcc increased by 37.19 % to 68.56 %, mIoU increased by 34.02 % to 56.93 %, and OA increased by 5.85 % to 84.7 %. Notably, the mIoUs for air ducts and doors saw the most substantial increases, rising by 85.19 % and 90.94 %, respectively.
Table 6shows that even compared to the PointNet++ algorithm, PointNet++-YOLO achieved significant gains in mAcc, mIoU, and OA, with increases of 26.49 %, 24.71 %, and 4.03 %, respectively. These improvements were observed for both architectural components, such as doors and columns (55.45 % and 45.33 %, respectively), and MEP components, such as air ducts and fire hydrants (75.16 % and 23.57 %, respectively).
Table 7further presents the Precision, Recall, and F1-Score results for PointNet++ and PointNet++-YOLO. The PointNet++-YOLO algorithm achieved approximately 5 % improvement in overall Precision, Recall, and F1-Score compared to PointNet++. Precision and F1-Score were enhanced across all component categories using the PointNet++-YOLO algorithm.Table 7. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on UGD dataset.MetricsAlgorithmsOverallAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPrecisionPointNet++89.5181.7842.1992.8168.5899.510.000.0088.9275.0375.20PointNet++-YOLO94.9288.5471.1596.0070.8199.6536.0263.9990.1791.7387.78RecallPointNet90.9910.1565.8999.7042.0498.930.000.0087.7791.3133.99PointNet++-YOLO95.0295.6265.7898.2177.6598.7826.9960.8686.5786.5097.72F1-ScorePointNet89.0218.0651.4496.1352.1299.220.000.0088.3482.3846.82PointNet+-YOLO94.9491.9468.3697.0974.0799.2130.8662.3988.3489.0492.48
Table 7further presents the Precision, Recall, and F1-Score results for PointNet++ and PointNet++-YOLO. The PointNet++-YOLO algorithm achieved approximately 5 % improvement in overall Precision, Recall, and F1-Score compared to PointNet++. Precision and F1-Score were enhanced across all component categories using the PointNet++-YOLO algorithm.
Table 7. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on UGD dataset.MetricsAlgorithmsOverallAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPrecisionPointNet++89.5181.7842.1992.8168.5899.510.000.0088.9275.0375.20PointNet++-YOLO94.9288.5471.1596.0070.8199.6536.0263.9990.1791.7387.78RecallPointNet90.9910.1565.8999.7042.0498.930.000.0087.7791.3133.99PointNet++-YOLO95.0295.6265.7898.2177.6598.7826.9960.8686.5786.5097.72F1-ScorePointNet89.0218.0651.4496.1352.1299.220.000.0088.3482.3846.82PointNet+-YOLO94.9491.9468.3697.0974.0799.2130.8662.3988.3489.0492.48
Table 7. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on UGD dataset.
Table 7. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on UGD dataset.
Table 7. Precision, Recall, and F1-Score comparison between PointNet++ and PointNet++-YOLO on UGD dataset.
MetricsAlgorithmsOverallAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPrecisionPointNet++89.5181.7842.1992.8168.5899.510.000.0088.9275.0375.20PointNet++-YOLO94.9288.5471.1596.0070.8199.6536.0263.9990.1791.7387.78RecallPointNet90.9910.1565.8999.7042.0498.930.000.0087.7791.3133.99PointNet++-YOLO95.0295.6265.7898.2177.6598.7826.9960.8686.5786.5097.72F1-ScorePointNet89.0218.0651.4496.1352.1299.220.000.0088.3482.3846.82PointNet+-YOLO94.9491.9468.3697.0974.0799.2130.8662.3988.3489.0492.48
Fig. 10illustrates the CMs for PointNet++ and PointNet++-YOLO. In PointNet++, air ducts were frequently misclassified as cable trays or ceilings. Additionally, other components such as fire hydrants, lamps, columns, and doors saw significant improvements in recognition rates, which demonstrated the effectiveness of the PointNet++-YOLO algorithm.Download:Download high-res image (467KB)Download:Download full-size imageFig. 10.CMcomparison between PointNet++ and PointNet++-YOLO onUGDdataset.
Fig. 10illustrates the CMs for PointNet++ and PointNet++-YOLO. In PointNet++, air ducts were frequently misclassified as cable trays or ceilings. Additionally, other components such as fire hydrants, lamps, columns, and doors saw significant improvements in recognition rates, which demonstrated the effectiveness of the PointNet++-YOLO algorithm.
Download:Download high-res image (467KB)Download:Download full-size image
Download:Download high-res image (467KB)
Download:Download high-res image (467KB)
Download high-res image (467KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 10.CMcomparison between PointNet++ and PointNet++-YOLO onUGDdataset.
Fig. 10.CMcomparison between PointNet++ and PointNet++-YOLO onUGDdataset.
Fig. 10.CMcomparison between PointNet++ and PointNet++-YOLO onUGDdataset.
CMcomparison between PointNet++ and PointNet++-YOLO on
Fig. 11provides an example of segmentation results from PointNet++ and PointNet++-YOLO on the UGD dataset. PointNet++ tended to misclassify air ducts as cable trays and columns as walls, while PointNet++-YOLO accurately identified most of the point clouds for these components.Download:Download high-res image (1MB)Download:Download full-size imageFig. 11. Example of segmentation results for PointNet++ and PointNet++-YOLO on UGD dataset.
Fig. 11provides an example of segmentation results from PointNet++ and PointNet++-YOLO on the UGD dataset. PointNet++ tended to misclassify air ducts as cable trays and columns as walls, while PointNet++-YOLO accurately identified most of the point clouds for these components.
Download:Download high-res image (1MB)Download:Download full-size image
Download:Download high-res image (1MB)
Download:Download high-res image (1MB)
Download high-res image (1MB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 11. Example of segmentation results for PointNet++ and PointNet++-YOLO on UGD dataset.
Fig. 11. Example of segmentation results for PointNet++ and PointNet++-YOLO on UGD dataset.
Fig. 11. Example of segmentation results for PointNet++ and PointNet++-YOLO on UGD dataset.

#### 5.3. Results on different training data sizes

This section examines the effectiveness of the proposed algorithm across different sizes of training data in the S3DIS dataset. Training sizes varied from one to five areas, with testing conducted on Area 5. The specific training areas for each data size are detailed inTable 8.Table 8. Experiments with different training data sizes in the S3DIS dataset.Training data sizesTraining areasTesting areaOne areaArea 1Area 5Two areasArea 1, Area 2Three areasArea 1, Area 2, Area 3Four areasArea 1, Area 2, Area 3, Area 4Five areasArea 1, Area 2, Area 3, Area 4, Area 6
This section examines the effectiveness of the proposed algorithm across different sizes of training data in the S3DIS dataset. Training sizes varied from one to five areas, with testing conducted on Area 5. The specific training areas for each data size are detailed inTable 8.
Table 8. Experiments with different training data sizes in the S3DIS dataset.Training data sizesTraining areasTesting areaOne areaArea 1Area 5Two areasArea 1, Area 2Three areasArea 1, Area 2, Area 3Four areasArea 1, Area 2, Area 3, Area 4Five areasArea 1, Area 2, Area 3, Area 4, Area 6
Table 8. Experiments with different training data sizes in the S3DIS dataset.
Table 8. Experiments with different training data sizes in the S3DIS dataset.
Table 8. Experiments with different training data sizes in the S3DIS dataset.
Training data sizesTraining areasTesting areaOne areaArea 1Area 5Two areasArea 1, Area 2Three areasArea 1, Area 2, Area 3Four areasArea 1, Area 2, Area 3, Area 4Five areasArea 1, Area 2, Area 3, Area 4, Area 6
Table 9illustrates the comparative results of the proposed PointNet++-YOLO algorithm across all five data sizes. As shown inTable 9, PointNet++-YOLO significantly enhanced mAcc, mIoU, and OA across all five data sizes. Notably, less frequent categories, such as windows, doors, beams, columns, and boards, exhibited the most substantial improvements in accuracy. This observation further demonstrated the algorithm's ability to enhance semantic segmentation accuracy for less common categories across different data sizes.Table 9. Performance comparison of PointNet++-YOLO across different training data sizes.Training dataAlgorithmmAccOAmIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterFive areasPointNet++69.1084.7161.2791.3697.7272.13025.8761.7739.7176.6980.6264.2571.0866.7148.63PointNet++-YOLO83.6286.9067.9691.3797.6876.737.3433.2281.1277.3676.4879.8163.7471.3778.2349.01Increment14.522.196.690.01−0.044.607.347.3519.3537.65−0.21−0.81−0.510.2911.520.38Four areasPointNet++58.9378.8149.2388.3496.6165.41027.5155.827.2170.1177.05753.2633.9337.71PointNet++-YOLO75.8881.7360.4888.4196.5970.8848.2831.9277.5460.6169.8776.32753.1267.4438.25Increment16.952.9211.250.07−0.025.4748.284.4121.7433.4−0.24−0.730−0.1433.510.54Three areasPointNet++50.9673.1841.0791.8389.3162.9604.6652.42.1160.8952.70.942.5843.7829.72PointNet++-YOLO62.9274.7945.3991.8589.3165.056.7421.6577.942.1360.952.70.942.4948.0330.38Increment11.961.614.320.0202.096.7416.9925.540.020.0100−0.094.250.66Two areasPointNet++56.5479.0048.4689.6495.4266.1108.5367.4416.459.8966.762.1960.3560.3636.86PointNet++-YOLO67.0579.8751.1989.6795.4267.563.8723.6476.9216.6559.8966.762.1960.2865.5637.11Increment10.510.872.730.0301.453.8715.119.480.25000−0.075.20.25One areaPointNet++37.9665.9129.4682.2485.156.660025.370.3945.9930.06035.69021.44PointNet++-YOLO63.8572.0548.9182.8285.6564.8769.519.4176.6733.3747.4731.77035.6765.2123.43Increment25.896.1419.450.580.558.2169.519.4151.332.981.481.710−0.0265.211.99
Table 9illustrates the comparative results of the proposed PointNet++-YOLO algorithm across all five data sizes. As shown inTable 9, PointNet++-YOLO significantly enhanced mAcc, mIoU, and OA across all five data sizes. Notably, less frequent categories, such as windows, doors, beams, columns, and boards, exhibited the most substantial improvements in accuracy. This observation further demonstrated the algorithm's ability to enhance semantic segmentation accuracy for less common categories across different data sizes.
Table 9. Performance comparison of PointNet++-YOLO across different training data sizes.Training dataAlgorithmmAccOAmIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterFive areasPointNet++69.1084.7161.2791.3697.7272.13025.8761.7739.7176.6980.6264.2571.0866.7148.63PointNet++-YOLO83.6286.9067.9691.3797.6876.737.3433.2281.1277.3676.4879.8163.7471.3778.2349.01Increment14.522.196.690.01−0.044.607.347.3519.3537.65−0.21−0.81−0.510.2911.520.38Four areasPointNet++58.9378.8149.2388.3496.6165.41027.5155.827.2170.1177.05753.2633.9337.71PointNet++-YOLO75.8881.7360.4888.4196.5970.8848.2831.9277.5460.6169.8776.32753.1267.4438.25Increment16.952.9211.250.07−0.025.4748.284.4121.7433.4−0.24−0.730−0.1433.510.54Three areasPointNet++50.9673.1841.0791.8389.3162.9604.6652.42.1160.8952.70.942.5843.7829.72PointNet++-YOLO62.9274.7945.3991.8589.3165.056.7421.6577.942.1360.952.70.942.4948.0330.38Increment11.961.614.320.0202.096.7416.9925.540.020.0100−0.094.250.66Two areasPointNet++56.5479.0048.4689.6495.4266.1108.5367.4416.459.8966.762.1960.3560.3636.86PointNet++-YOLO67.0579.8751.1989.6795.4267.563.8723.6476.9216.6559.8966.762.1960.2865.5637.11Increment10.510.872.730.0301.453.8715.119.480.25000−0.075.20.25One areaPointNet++37.9665.9129.4682.2485.156.660025.370.3945.9930.06035.69021.44PointNet++-YOLO63.8572.0548.9182.8285.6564.8769.519.4176.6733.3747.4731.77035.6765.2123.43Increment25.896.1419.450.580.558.2169.519.4151.332.981.481.710−0.0265.211.99
Table 9. Performance comparison of PointNet++-YOLO across different training data sizes.
Table 9. Performance comparison of PointNet++-YOLO across different training data sizes.
Table 9. Performance comparison of PointNet++-YOLO across different training data sizes.
Training dataAlgorithmmAccOAmIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterFive areasPointNet++69.1084.7161.2791.3697.7272.13025.8761.7739.7176.6980.6264.2571.0866.7148.63PointNet++-YOLO83.6286.9067.9691.3797.6876.737.3433.2281.1277.3676.4879.8163.7471.3778.2349.01Increment14.522.196.690.01−0.044.607.347.3519.3537.65−0.21−0.81−0.510.2911.520.38Four areasPointNet++58.9378.8149.2388.3496.6165.41027.5155.827.2170.1177.05753.2633.9337.71PointNet++-YOLO75.8881.7360.4888.4196.5970.8848.2831.9277.5460.6169.8776.32753.1267.4438.25Increment16.952.9211.250.07−0.025.4748.284.4121.7433.4−0.24−0.730−0.1433.510.54Three areasPointNet++50.9673.1841.0791.8389.3162.9604.6652.42.1160.8952.70.942.5843.7829.72PointNet++-YOLO62.9274.7945.3991.8589.3165.056.7421.6577.942.1360.952.70.942.4948.0330.38Increment11.961.614.320.0202.096.7416.9925.540.020.0100−0.094.250.66Two areasPointNet++56.5479.0048.4689.6495.4266.1108.5367.4416.459.8966.762.1960.3560.3636.86PointNet++-YOLO67.0579.8751.1989.6795.4267.563.8723.6476.9216.6559.8966.762.1960.2865.5637.11Increment10.510.872.730.0301.453.8715.119.480.25000−0.075.20.25One areaPointNet++37.9665.9129.4682.2485.156.660025.370.3945.9930.06035.69021.44PointNet++-YOLO63.8572.0548.9182.8285.6564.8769.519.4176.6733.3747.4731.77035.6765.2123.43Increment25.896.1419.450.580.558.2169.519.4151.332.981.481.710−0.0265.211.99
However, the extent of improvement varied with the training size. The most significant gains in mAcc, mIoU, and OA were achieved with the smallest training size (training with only one area), suggesting that the algorithm performed most effectively with relatively smaller datasets. Despite these improvements, the proposed algorithm's performance was still influenced by the inherent limitations of the dataset size itself. When the dataset size was limited, the algorithm's ability to enhance accuracy was also constrained. For instance, even when using four areas with the PointNet++-YOLO algorithm, it remained challenging to achieve the accuracy levels of PointNet++ trained on five areas.

#### 5.4. Results on different algorithms

In this section, the application of the Point-YOLO approach is extended to more advanced point cloud segmentation algorithms, including ASSANet, Point Transformer, PointNext, and PointVector. These four algorithms, introduced between 2021 and 2023, represent the state-of-the-art approaches for semantic segmentation of point clouds. The successful integration of the Point-YOLO approach with these advanced algorithms further validates the generalizability and robustness of the proposed method.
Table 10presents the results of ASSANet-YOLO, Point Transformer-YOLO, PointNext-YOLO, and PointVector-YOLO on the S3DIS dataset. As shown inTable 10, the integration of YOLO with the Point Transformer model resulted in significant improvements, with mAcc increasing by 8.94 % and mIoU by 1.13 %. The integration with the ASSANet model led to even greater improvements, with mAcc increasing by 10.73 % and mIoU by 3.08 %. The integration of YOLO with the PointNext model showed an increase of 9.54 % in mAcc and 1.51 % in mIoU. Similarly, the PointVector-YOLO model demonstrated increases of 9.04 % in mAcc and 1.17 % in mIoU compared to the original PointVector.Table 10. Performance of PointNext-YOLO and PointVector-YOLO on the S3DIS dataset.AlgorithmmAccOAmIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPoint Transformer78.5689.9172.5693.6598.0980.95041.3377.5089.2383.4690.0767.5579.6584.3057.53Point Transformer-YOLO87.5090.1873.6993.6398.0981.876.1545.8883.2289.0182.7688.7766.9779.4484.8857.35Increment8.940.271.13−0.0200.926.154.555.72−0.22−0.7−1.3−0.58−0.210.58−0.18ASSANet72.8087.5666.9894.9198.2877.140.0223.3857.9486.5681.8689.8658.6673.7275.0353.44ASSANet-YOLO83.5388.6370.0694.8798.2879.845.7532.6079.8286.3081.2288.5958.0873.6977.9653.80Increment10.731.073.08−0.0402.705.739.2221.88−0.26−0.64−1.27−0.58−0.032.930.36PointNext76.5686.0268.8694.4597.9272.22035.4576.9283.1781.5088.2966.6771.5676.0850.99PointNext -YOLO86.1086.5570.3794.4397.9273.221.7940.5082.5282.9081.4888.2866.6771.4582.3151.30Increment9.540.531.51−0.0201.991.795.055.6−0.27−0.02−0.010.00−0.116.230.31Pointvector78.3290.5273.0496.6298.0580.27028.3480.1891.7183.8691.4170.2180.1985.9162.80Pointvector -YOLO87.3690.7774.2196.6098.0581.175.7836.3484.3291.3783.1490.0869.5880.0185.6862.63Increment9.040.251.17−0.020.000.905.788.004.14−0.34−0.72−1.33−0.63−0.18−0.23−0.17
Table 10presents the results of ASSANet-YOLO, Point Transformer-YOLO, PointNext-YOLO, and PointVector-YOLO on the S3DIS dataset. As shown inTable 10, the integration of YOLO with the Point Transformer model resulted in significant improvements, with mAcc increasing by 8.94 % and mIoU by 1.13 %. The integration with the ASSANet model led to even greater improvements, with mAcc increasing by 10.73 % and mIoU by 3.08 %. The integration of YOLO with the PointNext model showed an increase of 9.54 % in mAcc and 1.51 % in mIoU. Similarly, the PointVector-YOLO model demonstrated increases of 9.04 % in mAcc and 1.17 % in mIoU compared to the original PointVector.
Table 10. Performance of PointNext-YOLO and PointVector-YOLO on the S3DIS dataset.AlgorithmmAccOAmIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPoint Transformer78.5689.9172.5693.6598.0980.95041.3377.5089.2383.4690.0767.5579.6584.3057.53Point Transformer-YOLO87.5090.1873.6993.6398.0981.876.1545.8883.2289.0182.7688.7766.9779.4484.8857.35Increment8.940.271.13−0.0200.926.154.555.72−0.22−0.7−1.3−0.58−0.210.58−0.18ASSANet72.8087.5666.9894.9198.2877.140.0223.3857.9486.5681.8689.8658.6673.7275.0353.44ASSANet-YOLO83.5388.6370.0694.8798.2879.845.7532.6079.8286.3081.2288.5958.0873.6977.9653.80Increment10.731.073.08−0.0402.705.739.2221.88−0.26−0.64−1.27−0.58−0.032.930.36PointNext76.5686.0268.8694.4597.9272.22035.4576.9283.1781.5088.2966.6771.5676.0850.99PointNext -YOLO86.1086.5570.3794.4397.9273.221.7940.5082.5282.9081.4888.2866.6771.4582.3151.30Increment9.540.531.51−0.0201.991.795.055.6−0.27−0.02−0.010.00−0.116.230.31Pointvector78.3290.5273.0496.6298.0580.27028.3480.1891.7183.8691.4170.2180.1985.9162.80Pointvector -YOLO87.3690.7774.2196.6098.0581.175.7836.3484.3291.3783.1490.0869.5880.0185.6862.63Increment9.040.251.17−0.020.000.905.788.004.14−0.34−0.72−1.33−0.63−0.18−0.23−0.17
Table 10. Performance of PointNext-YOLO and PointVector-YOLO on the S3DIS dataset.
Table 10. Performance of PointNext-YOLO and PointVector-YOLO on the S3DIS dataset.
Table 10. Performance of PointNext-YOLO and PointVector-YOLO on the S3DIS dataset.
AlgorithmmAccOAmIoUCeilingFloorWallBeamColumnWindowDoorTableChairSofaBookcaseBoardClutterPoint Transformer78.5689.9172.5693.6598.0980.95041.3377.5089.2383.4690.0767.5579.6584.3057.53Point Transformer-YOLO87.5090.1873.6993.6398.0981.876.1545.8883.2289.0182.7688.7766.9779.4484.8857.35Increment8.940.271.13−0.0200.926.154.555.72−0.22−0.7−1.3−0.58−0.210.58−0.18ASSANet72.8087.5666.9894.9198.2877.140.0223.3857.9486.5681.8689.8658.6673.7275.0353.44ASSANet-YOLO83.5388.6370.0694.8798.2879.845.7532.6079.8286.3081.2288.5958.0873.6977.9653.80Increment10.731.073.08−0.0402.705.739.2221.88−0.26−0.64−1.27−0.58−0.032.930.36PointNext76.5686.0268.8694.4597.9272.22035.4576.9283.1781.5088.2966.6771.5676.0850.99PointNext -YOLO86.1086.5570.3794.4397.9273.221.7940.5082.5282.9081.4888.2866.6771.4582.3151.30Increment9.540.531.51−0.0201.991.795.055.6−0.27−0.02−0.010.00−0.116.230.31Pointvector78.3290.5273.0496.6298.0580.27028.3480.1891.7183.8691.4170.2180.1985.9162.80Pointvector -YOLO87.3690.7774.2196.6098.0581.175.7836.3484.3291.3783.1490.0869.5880.0185.6862.63Increment9.040.251.17−0.020.000.905.788.004.14−0.34−0.72−1.33−0.63−0.18−0.23−0.17
Table 11presents the results of PointNext-YOLO and PointVector-YOLO on the UGD dataset. As shown inTable 11, the improvements on the UGD dataset were even more pronounced. PointNext-YOLO achieved a significant increase of 34.67 % in mAcc and 32.38 % in mIoU, outperforming the original PointNext model. Similarly, PointVector-YOLO demonstrated improvements of 24.36 % in mAcc and 20.81 % in mIoU compared to the original PointVector. This substantial performance boost on the UGD dataset is likely due to the fact that both PointNext and PointVector were fine-tuned on the S3DIS dataset, while their parameters were not fully optimized for the new dataset. This suggests that the proposed Point-YOLO approach may perform even better on unseen datasets, highlighting its adaptability and potential for broader applications.Table 11. Performance of PointNext-YOLO and PointVector-YOLO on the UGD dataset.AlgorithmmAccOAmIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNext41.6989.2635.79037.8287.99099.190067.3265.60PointNext -YOLO76.3694.4268.1785.1941.1093.0061.799.2018.2445.3368.6678.3890.94Increment34.675.1632.3885.193.285.0161.70.0118.2445.331.3412.7890.94PointVector57.7494.5253.5297.9976.1497.76099.650086.9271.015.76PointVector -YOLO82.1096.5374.3389.3772.7296.6361.7099.4818.2445.3384.6785.0490.09Increment24.362.0120.81−8.62−3.42−1.1361.70−0.1718.2445.33−2.2514.0384.33
Table 11presents the results of PointNext-YOLO and PointVector-YOLO on the UGD dataset. As shown inTable 11, the improvements on the UGD dataset were even more pronounced. PointNext-YOLO achieved a significant increase of 34.67 % in mAcc and 32.38 % in mIoU, outperforming the original PointNext model. Similarly, PointVector-YOLO demonstrated improvements of 24.36 % in mAcc and 20.81 % in mIoU compared to the original PointVector. This substantial performance boost on the UGD dataset is likely due to the fact that both PointNext and PointVector were fine-tuned on the S3DIS dataset, while their parameters were not fully optimized for the new dataset. This suggests that the proposed Point-YOLO approach may perform even better on unseen datasets, highlighting its adaptability and potential for broader applications.
Table 11. Performance of PointNext-YOLO and PointVector-YOLO on the UGD dataset.AlgorithmmAccOAmIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNext41.6989.2635.79037.8287.99099.190067.3265.60PointNext -YOLO76.3694.4268.1785.1941.1093.0061.799.2018.2445.3368.6678.3890.94Increment34.675.1632.3885.193.285.0161.70.0118.2445.331.3412.7890.94PointVector57.7494.5253.5297.9976.1497.76099.650086.9271.015.76PointVector -YOLO82.1096.5374.3389.3772.7296.6361.7099.4818.2445.3384.6785.0490.09Increment24.362.0120.81−8.62−3.42−1.1361.70−0.1718.2445.33−2.2514.0384.33
Table 11. Performance of PointNext-YOLO and PointVector-YOLO on the UGD dataset.
Table 11. Performance of PointNext-YOLO and PointVector-YOLO on the UGD dataset.
Table 11. Performance of PointNext-YOLO and PointVector-YOLO on the UGD dataset.
AlgorithmmAccOAmIoUAir ductCable trayCeilingFire hydrantFloorLampColumnPipeWallDoorPointNext41.6989.2635.79037.8287.99099.190067.3265.60PointNext -YOLO76.3694.4268.1785.1941.1093.0061.799.2018.2445.3368.6678.3890.94Increment34.675.1632.3885.193.285.0161.70.0118.2445.331.3412.7890.94PointVector57.7494.5253.5297.9976.1497.76099.650086.9271.015.76PointVector -YOLO82.1096.5374.3389.3772.7296.6361.7099.4818.2445.3384.6785.0490.09Increment24.362.0120.81−8.62−3.42−1.1361.70−0.1718.2445.33−2.2514.0384.33

#### 5.5. Results on the ScanNet dataset

In this section, this study evaluates the performance of Point-YOLO on the ScanNet dataset. Since ScanNet does not include the “ceiling” category, a new image capture method was adopted by photographing from the ceiling and making predictions based on these images. Examples of the original ScanNet scene and image annotations are shown inFig. 12. Six types of components—bed, chair, sofa, table, door, and window—were labeled and predicted, withrandom testsconducted on 30 rooms. The performance was tested using two algorithms: PointNet++ and PointNext. The IoU for each category on the ScanNet test set is shown inTable 12. From the results, it is evident that Point-YOLO improved the accuracy of all six categories in comparison to the baseline models, further demonstrating its robustness across different algorithms.Download:Download high-res image (214KB)Download:Download full-size imageFig. 12. ScanNet dataset scene and image annotation. (a) Original scene from the ScanNet dataset, (b) Image annotations of the ScanNet scene.Table 12. Performance of Point-YOLO on the ScanNet dataset.AlgorithmBedChairSofaTableDoorWindowPointNet++54.5380.7560.5851.7943.0955.47PointNet++-YOLO58.2185.4564.5458.9046.5456.42Increment3.684.73.967.113.450.95PointNext75.5788.387.3373.7658.5968.8PointNext-YOLO76.2689.6789.2474.3258.7169.61Increment0.691.371.910.560.120.81
In this section, this study evaluates the performance of Point-YOLO on the ScanNet dataset. Since ScanNet does not include the “ceiling” category, a new image capture method was adopted by photographing from the ceiling and making predictions based on these images. Examples of the original ScanNet scene and image annotations are shown inFig. 12. Six types of components—bed, chair, sofa, table, door, and window—were labeled and predicted, withrandom testsconducted on 30 rooms. The performance was tested using two algorithms: PointNet++ and PointNext. The IoU for each category on the ScanNet test set is shown inTable 12. From the results, it is evident that Point-YOLO improved the accuracy of all six categories in comparison to the baseline models, further demonstrating its robustness across different algorithms.
. Six types of components—bed, chair, sofa, table, door, and window—were labeled and predicted, withrandom testsconducted on 30 rooms. The performance was tested using two algorithms: PointNet++ and PointNext. The IoU for each category on the ScanNet test set is shown in
Download:Download high-res image (214KB)Download:Download full-size image
Download:Download high-res image (214KB)
Download:Download high-res image (214KB)
Download high-res image (214KB)
Download:Download full-size image
Download:Download full-size image
Download full-size image
Fig. 12. ScanNet dataset scene and image annotation. (a) Original scene from the ScanNet dataset, (b) Image annotations of the ScanNet scene.
Fig. 12. ScanNet dataset scene and image annotation. (a) Original scene from the ScanNet dataset, (b) Image annotations of the ScanNet scene.
Fig. 12. ScanNet dataset scene and image annotation. (a) Original scene from the ScanNet dataset, (b) Image annotations of the ScanNet scene.
Table 12. Performance of Point-YOLO on the ScanNet dataset.AlgorithmBedChairSofaTableDoorWindowPointNet++54.5380.7560.5851.7943.0955.47PointNet++-YOLO58.2185.4564.5458.9046.5456.42Increment3.684.73.967.113.450.95PointNext75.5788.387.3373.7658.5968.8PointNext-YOLO76.2689.6789.2474.3258.7169.61Increment0.691.371.910.560.120.81
Table 12. Performance of Point-YOLO on the ScanNet dataset.
Table 12. Performance of Point-YOLO on the ScanNet dataset.
Table 12. Performance of Point-YOLO on the ScanNet dataset.
AlgorithmBedChairSofaTableDoorWindowPointNet++54.5380.7560.5851.7943.0955.47PointNet++-YOLO58.2185.4564.5458.9046.5456.42Increment3.684.73.967.113.450.95PointNext75.5788.387.3373.7658.5968.8PointNext-YOLO76.2689.6789.2474.3258.7169.61Increment0.691.371.910.560.120.81

### 6. Discussion

This section discusses the proposed Point-YOLO method.Section 6.1analyzes runtime performance and memory usage,Section 6.2addresses data acquisition time,Section 6.3presents a performance and efficiency analysis of Point-YOLO, andSection 6.4discusses the limitations and future directions.

#### 6.1. Runtime performance and memory usage

In this study, point cloud semantic segmentation and image inference were performed using a singleGPU(NVIDIA L40 with 48GB of GPU memory). The training time for the point cloud model is 10 h and 52 min, while the image model training time is 30 min. The inference time for processing 42 rooms is 30 min, with an average of 43 s per room. On the UGD dataset, which is denser than the S3DIS dataset due to its LiDAR scans, training and testing times are slightly longer. The point cloud model training time is 11 h and 19 min, the image model training time is 20 min, and the inference time is 1 h and 48 min. The memory usage during training and inference varies depending on the complexity of the dataset and model architecture. For the point cloud model, peak memory usage during training reaches approximately 24GB of GPU memory, while inference typically requires around 1GB. The image model uses less memory, with peak usage around 6GB during training. Overall, the proposed model meets the practical demands ofengineering applications.
(NVIDIA L40 with 48GB of GPU memory). The training time for the point cloud model is 10 h and 52 min, while the image model training time is 30 min. The inference time for processing 42 rooms is 30 min, with an average of 43 s per room. On the UGD dataset, which is denser than the S3DIS dataset due to its LiDAR scans, training and testing times are slightly longer. The point cloud model training time is 11 h and 19 min, the image model training time is 20 min, and the inference time is 1 h and 48 min. The memory usage during training and inference varies depending on the complexity of the dataset and model architecture. For the point cloud model, peak memory usage during training reaches approximately 24GB of GPU memory, while inference typically requires around 1GB. The image model uses less memory, with peak usage around 6GB during training. Overall, the proposed model meets the practical demands ofengineering applications.

#### 6.2. Discussion on data acquisition time

The proposed Point-YOLO method significantly reduces both training and testing time compared to conventional image-point cloud fusion approaches.Table 13summarizes the equipment requirements and estimated processing time for Point-YOLO and traditional methods.Table 13. Comparison of equipment requirements and processing time.MethodEquipment requirementsEstimated timeTraditional image-point cloud fusion methodLaser scanner, camera, registration software, and algorithms- Data acquisition: 5 min/room - Registration: 5 min/room - Manual adjustment: 3 min/room - Total: 18 min/roomProposed Point-YOLO methodOnly a laser scannerDirect image generation from the point cloud: <1 min/roomTime savings–17 min/room (94.4 % reduction)
The proposed Point-YOLO method significantly reduces both training and testing time compared to conventional image-point cloud fusion approaches.Table 13summarizes the equipment requirements and estimated processing time for Point-YOLO and traditional methods.
Table 13. Comparison of equipment requirements and processing time.MethodEquipment requirementsEstimated timeTraditional image-point cloud fusion methodLaser scanner, camera, registration software, and algorithms- Data acquisition: 5 min/room - Registration: 5 min/room - Manual adjustment: 3 min/room - Total: 18 min/roomProposed Point-YOLO methodOnly a laser scannerDirect image generation from the point cloud: <1 min/roomTime savings–17 min/room (94.4 % reduction)
Table 13. Comparison of equipment requirements and processing time.
Table 13. Comparison of equipment requirements and processing time.
Table 13. Comparison of equipment requirements and processing time.
MethodEquipment requirementsEstimated timeTraditional image-point cloud fusion methodLaser scanner, camera, registration software, and algorithms- Data acquisition: 5 min/room - Registration: 5 min/room - Manual adjustment: 3 min/room - Total: 18 min/roomProposed Point-YOLO methodOnly a laser scannerDirect image generation from the point cloud: <1 min/roomTime savings–17 min/room (94.4 % reduction)
Conventional methods rely on pre-registeredlaser scannersand cameras, which introduce additional time costs due to extra data acquisition, registration, and manual adjustments. Specifically, cameras must be deployed alongside laser scanners during data collection, requiring approximately 5 min per room based on the S3DIS dataset. Once the point cloud and images are obtained, an additional 5 min per room is needed for modality alignment using specialized registration software or algorithms. Furthermore, registration errors often necessitate manual adjustments, adding another 3 min per room. In total, conventional methods require an additional 18 min per room.
In contrast, the Point-YOLO method eliminates the need for pre-registered sensors, registration software, and manual adjustments by directly generating 2D images from point clouds using virtual cameras. The only additional processing involved is extracting images from the point cloud, which takes less than 1 min per room. As a result, Point-YOLO saves approximately 17 min per room, achieving a 94.4 % reduction in processing time compared to conventional methods.

#### 6.3. Performance and efficiency analysis of Point-YOLO

Another key advantage of Point-YOLO is its ability to achieve high accuracy with a lightweight model, leading to significant reductions in both training and inference time.Table 14presents a comparative analysis of PointNet++-YOLO against conventional methods, categorized into two comparison groups (CG-A and CG-B). In CG-A, the S3DIS dataset is used to compare PointNet++-YOLO with PointNext. PointNext has a significantly larger parameter count (7.1 M), while PointNet++-YOLO adopts a more compact design (1.0 M parameters) by integrating Point-YOLO. This reduction in model size leads to a 52.6 % decrease in FLOPs, significantly improving throughput (TP), from 115 to 186. Despite its smaller size, PointNet++-YOLO achieves comparable performance in OA and mIoU, while achieving a noticeably higher mAcc than PointNext. This demonstrates that Point-YOLO enables higher processing efficiency without compromising accuracy.Table 14. Performance comparison of Point-YOLO with conventional methods.CGDatasetMethodmAccOAmIoUHardware#ParamsFLOPsTPAS3DISPointNet++-YOLO83.6283.6283.62CPU-compatible1.0 M7.2G186PointNext76.5686.0268.86Requires GPU7.1 M15.2G115BUGDPointNext++-YOLO76.3694.4268.17Requires GPU7.1 M15.2G115PointVector57.7494.5253.52Requires GPU24.1 M58.5G40
Another key advantage of Point-YOLO is its ability to achieve high accuracy with a lightweight model, leading to significant reductions in both training and inference time.Table 14presents a comparative analysis of PointNet++-YOLO against conventional methods, categorized into two comparison groups (CG-A and CG-B). In CG-A, the S3DIS dataset is used to compare PointNet++-YOLO with PointNext. PointNext has a significantly larger parameter count (7.1 M), while PointNet++-YOLO adopts a more compact design (1.0 M parameters) by integrating Point-YOLO. This reduction in model size leads to a 52.6 % decrease in FLOPs, significantly improving throughput (TP), from 115 to 186. Despite its smaller size, PointNet++-YOLO achieves comparable performance in OA and mIoU, while achieving a noticeably higher mAcc than PointNext. This demonstrates that Point-YOLO enables higher processing efficiency without compromising accuracy.
Table 14. Performance comparison of Point-YOLO with conventional methods.CGDatasetMethodmAccOAmIoUHardware#ParamsFLOPsTPAS3DISPointNet++-YOLO83.6283.6283.62CPU-compatible1.0 M7.2G186PointNext76.5686.0268.86Requires GPU7.1 M15.2G115BUGDPointNext++-YOLO76.3694.4268.17Requires GPU7.1 M15.2G115PointVector57.7494.5253.52Requires GPU24.1 M58.5G40
Table 14. Performance comparison of Point-YOLO with conventional methods.
Table 14. Performance comparison of Point-YOLO with conventional methods.
Table 14. Performance comparison of Point-YOLO with conventional methods.
CGDatasetMethodmAccOAmIoUHardware#ParamsFLOPsTPAS3DISPointNet++-YOLO83.6283.6283.62CPU-compatible1.0 M7.2G186PointNext76.5686.0268.86Requires GPU7.1 M15.2G115BUGDPointNext++-YOLO76.3694.4268.17Requires GPU7.1 M15.2G115PointVector57.7494.5253.52Requires GPU24.1 M58.5G40
In CG-B, the UGD dataset is used to compare PointNext++-YOLO with PointVector. PointNext++-YOLO, leveraging Point-YOLO, maintains lower computational costs and achieves faster inference speeds, while also outperforming PointVector in terms of accuracy. This demonstrates that Point-YOLO effectively reduces both training and testing time without sacrificing performance.
The reduction in model complexity through the use of Point-YOLO directly leads to substantial time savings. Specifically, the reduced parameter count and computational requirements result in approximately 38 % less training time and a 40–60 % reduction in inference time. By adopting Point-YOLO, the model not only lowers computational resource usage but also enhances processing speed, offering a balanced trade-off between performance, efficiency, and accuracy.

#### 6.4. Limitations and future directions

Despite the Point-YOLO method achieving significant improvements in OA, mAcc, and IoU, it enhances accuracy for most classes. In particular, the method performs well for minority components, such as windows and doors, which are often confused with larger categories such as walls. This demonstrates the method's effective recognition of these less frequent components. However, accuracy is not consistently improved across all categories. Specifically, the method does not show consistent enhancement in IoU for more disordered 3D objects, such as chairs and pipes. One reason for this limitation is that the 3D morphology of these objects is more complex compared to regular, vertically-oriented components like doors and windows. Additionally, the limited number of training samples for these categories, such as the pipes in the UGD dataset (with only 90 image samples), may not provide sufficient representation for learning their intricate features, which impacts the model's performance. Furthermore, as a one-stage algorithm, YOLO is optimized for speed rather than achieving the highest precision. Future research could explore hybrid approaches, combining point cloud segmentation with two-stage image segmentation models such as Mask R-CNN to improve the detection of complex 3D objects such as pipes. Increasing the number of training samples would also help improve image recognition accuracy. By integrating these techniques, it is expected that the performance of DL for image and point cloud fusion can be further enhanced.

### 7. Conclusions

This paper presented a 2D–3D fusion approach, the Point-YOLO, designed to enhance the accuracy of point cloud semantic segmentation, particularly for categories with limited instances or similar shapes. Point-YOLO eliminates the need for pre-registration with 2D images by using virtual cameras to capture images within the point cloud scene. YOLOv8 is then used for object detection and segmentation, and the predicted confidence scores are applied to update the point cloud's semantic segmentation confidence in point-based DL methods. The experimental results validate the effectiveness of the proposed approach. Specifically, on the S3DIS and UGD datasets, the proposed Point-YOLO method significantly improved mAcc and mIoU by 14.52 % and 6.69 % on S3DIS, and by 26.49 % and 24.71 % on UGD, respectively, compared to using PointNet++. Notably, for minority components such as doors, windows, and air ducts, the mIoU increased by 37.65 %, 19.35 %, and 75.16 %, respectively. The method also showed substantial improvements when applied to advanced algorithms such as PointNext and PointVector, further demonstrating its versatility and effectiveness.
To address practical applications and inspire further research, the discussion has highlighted several important aspects. Point-YOLO achieves a favorable balance between accuracy and computational efficiency, making it suitable forengineering applicationswith limited resources. By eliminating the need for complex registration processes, Point-YOLO significantly reduces data acquisition time, offering a 94.4 % time saving per scene compared to traditional methods. Additionally, the method shows strong performance even with a lightweight model architecture, making it accessible for real-time or resource-constrained environments. However, challenges remain for irregular or less-represented categories, such as chairs and pipes. Future work may explore integrating two-stage segmentation frameworks (e.g., Mask R-CNN) or expanding dataset diversity to improve recognition for complex 3D structures. These directions point to the continued potential of Point-YOLO as a foundation for more robust and generalizable 2D–3D fusion frameworks.

### CRediT authorship contribution statement

Hongzhe Yue:Writing – original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Data curation.Qian Wang:Writing – review & editing, Supervision, Software, Project administration, Funding acquisition, Formal analysis, Conceptualization.Mingyu Zhang:Writing – review & editing, Software, Methodology, Formal analysis.Yutong Xue:Writing – original draft, Methodology, Formal analysis.Liang Lu:Writing – review & editing, Formal analysis.

### Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

### Acknowledgments

This work was supported by theNational Key R&D Program of China(No.2023YFC3804300),Start-up Research Fund of Southeast University(No.RF1028623126),Science and Technology Planning Project of Jiangsu Province of China(No.BZ2024058),SEU Innovation Enhancement Plan for Doctoral Students(No.CXJH_SEU 25101) andPostgraduate Research&Practice Innovation Program of Jiangsu Province(No.KYCX25_0497).
National Key R&D Program of China
Start-up Research Fund of Southeast University
Science and Technology Planning Project of Jiangsu Province of China
SEU Innovation Enhancement Plan for Doctoral Students
Postgraduate Research&Practice Innovation Program of Jiangsu Province
Appendix A. Visual results of segmentation performance for PointNet++ and PointNet++-YOLO high-res image (1MB) full-size image high-res image (2MB) full-size image

### Appendix A. Visual results of segmentation performance for PointNet++ and PointNet++-YOLO

high-res image (1MB) full-size image high-res image (2MB) full-size image
 high-res image (1MB) full-size image high-res image (2MB) full-size image
 high-res image (1MB) full-size image
 high-res image (1MB)
 high-res image (1MB)
Download high-res image (1MB)
 full-size image
 full-size image
Download full-size image
 high-res image (2MB) full-size image
 high-res image (2MB)
 high-res image (2MB)
Download high-res image (2MB)
 full-size image
 full-size image
Download full-size image
Data availabilityData generated or analyzed during the study are available from the corresponding author by request.

### Data availability

Data generated or analyzed during the study are available from the corresponding author by request.
References[1]X.H. Gao, P. Pishdad-BozorgiBIM-enabled facilities operation and maintenance: a reviewAdv. Eng. Inform., 39 (2019), pp. 227-247,10.1016/j.aei.2019.01.005View PDFView articleView in ScopusGoogle Scholar[2]X.F. Yin, H.X. Liu, Y. Chen, M. Al-HusseinBuilding information modelling for off-site construction: review and future directionsAutomat. Construct., 101 (2019), pp. 72-91,10.1016/j.autcon.2019.01.010View PDFView articleView in ScopusGoogle Scholar[3]H. Yue, Q. Wang, Z. Zhao, S. Lai, G. HuangInteractions between BIM and robotics: towards intelligent construction engineering and managementComput. Ind., 169 (2025), Article 104299,10.1016/j.compind.2025.104299View PDFView articleView in ScopusGoogle Scholar[4]X.H. Xiong, A. Adan, B. Akinci, D. HuberAutomatic creation of semantically rich 3D building models from laser scanner dataAutomat. Construct., 31 (2013), pp. 325-337,10.1016/j.autcon.2012.10.006View PDFView articleView in ScopusGoogle Scholar[5]Q. Wang, M.K. KimApplications of 3D point cloud data in the construction industry: a fifteen-year review from 2004 to 2018Adv. Eng. Inform., 39 (2019), pp. 306-319,10.1016/j.aei.2019.02.007View PDFView articleGoogle Scholar[6]Z. Zhang, A. Ji, K. Wang, L. ZhangUnrollingNet: an attention-based deep learning approach for the segmentation of large-scale point clouds of tunnelsAutomat. Construct., 142 (2022), Article 104456,10.1016/j.autcon.2022.104456View PDFView articleView in ScopusGoogle Scholar[7]K. Mirzaei, M. Arashpour, E. Asadi, H. Masoumi, Y. Bai, A. Behnood3D point cloud data processing with machine learning for construction and infrastructure applications: a comprehensive reviewAdv. Eng. Inform., 51 (2022), Article 101501,10.1016/j.aei.2021.101501View PDFView articleView in ScopusGoogle Scholar[8]Y. LeCun, Y. Bengio, G. HintonDeep learningNature, 521 (2015), pp. 436-444,10.1038/nature14539View in ScopusGoogle Scholar[9]T.D. Akinosho, L.O. Oyedele, M. Bilal, A.O. Ajayi, M.D. Delgado, O.O. Akinade, A.A. AhmedDeep learning in the construction industry: a review of present status and future innovationsJ. Build. Eng., 32 (2020),10.1016/j.jobe.2020.101827Google Scholar[10]H. Yue, Q. Wang, Y. Yan, G. HuangDeep learning-based point cloud completion for MEP componentsAutomat. Construct., 175 (2025), Article 106218,10.1016/j.autcon.2025.106218View PDFView articleView in ScopusGoogle Scholar[11]C. Yin, B. Wang, V.J. Gan, M. Wang, J.C. ChengAutomated semantic segmentation of industrial point clouds using ResPointNet++Automat. Construct., 130 (2021), Article 103874,10.1016/j.autcon.2021.103874View PDFView articleView in ScopusGoogle Scholar[12]H. Yue, Q. Wang, H. Zhao, N. Zeng, Y. TanDeep learning applications for point clouds in the construction industryAutomat. Construct., 168 (2024), Article 105769,10.1016/j.autcon.2024.105769View PDFView articleView in ScopusGoogle Scholar[13]Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, J. Xiao3d shapenets: a deep representation for volumetric shapesProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2015), pp. 1912-1920,10.1109/CVPR.2015.7298801View in ScopusGoogle Scholar[14]H. Su, S. Maji, E. Kalogerakis, E. Learned-MillerMulti-view convolutional neural networks for 3d shape recognitionProceedings of the IEEE International Conference on Computer Vision (2015), pp. 945-953,10.1109/ICCV.2015.114View in ScopusGoogle Scholar[15]C.R. Qi, H. Su, K.C. Mo, L.J. Guibas, IeeePointNet: deep learning on point sets for 3D classification and segmentation30th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Honolulu, HI (2017), pp. 77-85,10.1109/cvpr.2017.16View in ScopusGoogle Scholar[16]C.R. Qi, L. Yi, H. Su, L.J. GuibasPointnet++: deep hierarchical feature learning on point sets in a metric spaceAdv. Neural Inform. Process. Syst., 30 (2017)arXiv:1706.02413Google Scholar[17]S. Tang, X. Li, X. Zheng, B. Wu, W. Wang, Y. ZhangBIM generation from 3D point clouds by combining 3D deep learning and improved morphological approachAutomat. Construct., 141 (2022), Article 104422,10.1016/j.autcon.2022.104422View PDFView articleView in ScopusGoogle Scholar[18]D. Lamas, A. Justo, M. Soilán, B. RiveiroAutomated production of synthetic point clouds of truss bridges for semantic and instance segmentation using deep learning modelsAutomat. Construct., 158 (2024), Article 105176,10.1016/j.autcon.2023.105176View PDFView articleView in ScopusGoogle Scholar[19]R.M. Zhai, J.G. Zou, Y.F. He, L.Y. MengBIM-driven data augmentation method for semantic segmentation in superpoint-based deep learning networkAutomat. Construct., 140 (2022), Article 104373,10.1016/j.autcon.2022.104373View PDFView articleView in ScopusGoogle Scholar[20]S. Kim, K. Jeong, T. Hong, J. Lee, J. LeeDeep learning-based automated generation of material data with object-space relationships for scan to BIMJ. Manag. Eng., 39 (2023),10.1061/jmenea.Meeng-5143Google Scholar[21]S.J. Tang, H.S. Huang, Y.J. Zhang, M.M. Yao, X.M. Li, L.F. Xie, W.X. WangSkeleton-guided generation of synthetic noisy point clouds from as-built BIM to improve indoor scene understandingAutomat. Construct., 156 (2023), Article 105076,10.1016/j.autcon.2023.105076View PDFView articleView in ScopusGoogle Scholar[22]I. Armeni, S. Sax, A.R. Zamir, S. SavareseJoint 2d-3d-semantic data for indoor scene understandingJ. arXiv preprint (2017),10.48550/arXiv.1702.01105arXiv:.01105Google Scholar[23]H.X. Zhang, Z. ZouQuality assurance for building components through point cloud segmentation leveraging synthetic dataAutom. Constr., 155 (2023), Article 105045,10.1016/j.autcon.2023.105045View PDFView articleView in ScopusGoogle Scholar[24]K. He, G. Gkioxari, P. Dollár, R. GirshickMask R-CNN2017 IEEE International Conference on Computer Vision (ICCV) (2017), pp. 2980-2988,10.1109/ICCV.2017.322View in ScopusGoogle Scholar[25]G. Huang, D. Li, S.T. Ng, L. Wang, Y. ZhangVision-based personal thermal comfort modeling under facial occlusion scenariosEnerg. Buildings, 335 (2025), Article 115566,10.1016/j.enbuild.2025.115566View PDFView articleView in ScopusGoogle Scholar[26]K. Jang, S. Park, H. Jung, H. Yoo, Y.-K. AnDeep learning-based 3D digital damage map of vertical-type tunnels using unmanned fusion data scanningAutomat. Construct., 162 (2024), Article 105397,10.1016/j.autcon.2024.105397View PDFView articleView in ScopusGoogle Scholar[27]J.-L. Xiao, J.-S. Fan, Y.-F. Liu, B.-L. Li, J.-G. NieRegion of interest (ROI) extraction and crack detection for UAV-based bridge inspection using point cloud segmentation and 3D-to-2D projectionAutomat. Construct., 158 (2024), Article 105226,10.1016/j.autcon.2023.105226View PDFView articleView in ScopusGoogle Scholar[28]H.H. Liang, J.K.W. Yeoh, D.K.H. ChuaMaterial augmented semantic segmentation of point clouds for building elementsComput. Aided Civ. Inf. Eng., 39 (2024), pp. 2312-2329,10.1111/mice.13198View in ScopusGoogle Scholar[29]H.Z. Li, Y.L. Chen, J. Liu, C.T. Che, Z.Y. Meng, H. ZhuHigh-resolution model reconstruction and bridge damage detection based on data fusion of unmanned aerial vehicles light detection and ranging data imageryComput. Aided Civ. Inf. Eng., 39 (2024), pp. 1197-1217,10.1111/mice.13133View in ScopusGoogle Scholar[30]T. Hackel, N. Savinov, L. Ladicky, J.D. Wegner, K. Schindler, M. PollefeysNEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARKISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-1/W1, ("SEMANTIC3D.NET: A) (2017), pp. 91-98http://dx.doi.org/10.5194/isprs-annals-IV-1-W1-91-2017View in ScopusGoogle Scholar[31]W. Tan, N. Qin, L. Ma, Y. Li, J. Du, G. Cai, K. Yang, J. LiToronto-3D: a large-scale mobile LiDAR dataset for semantic segmentation of urban roadways2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2020), pp. 797-806,10.1109/CVPRW50498.2020.00109View in ScopusGoogle Scholar[32]J.-E. Deschaud, D. Duque, J.P. Richa, S. Velasco-Forero, B. Marcotegui, F. GouletteParis-CARLA-3D: a real and synthetic outdoor point cloud dataset for challenging tasks in 3D mappingRemote Sens. (Basel), 13 (2021), p. 4713,10.3390/rs13224713View in ScopusGoogle Scholar[33]X. Li, C. Li, Z. Tong, A. Lim, J. Yuan, Y. Wu, J. Tang, R. HuangCampus3d: a photogrammetry point cloud benchmark for hierarchical understanding of outdoor sceneProceedings of the 28th ACM International Conference on Multimedia (2020), pp. 238-246arXiv:2008.04968v1CrossrefGoogle Scholar[34]Chen M, Hu Q, Yu Z, et al. Stpls3d: A large-scale synthetic and real aerial photogrammetry 3d point cloud dataset[J]. arXiv preprint arXiv:2203.09065, 2022.Google Scholar[35]T. Sun, Z. Zhang, X. Tan, Y. Qu, Y. XieImage understands point cloud: weakly supervised 3D semantic segmentation via association learningIEEE Trans. Image Process., 33 (2024), pp. 1838-1852,10.1109/TIP.2024.3372449View in ScopusGoogle Scholar[36]C.R. Qi, W. Liu, C. Wu, H. Su, L.J. GuibasFrustum PointNets for 3D object detection from RGB-D data2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), pp. 918-927,10.1109/CVPR.2018.00102View in ScopusGoogle Scholar[37]Z. Xiang, A. Rashidi, G. OuIntegrating inverse photogrammetry and a deep learning–based point cloud segmentation approach for automated generation of BIM modelsJ. Constr. Eng. Manag., 149 (2023), Article 04023074,10.1061/JCEMD4.COENG-13020View in ScopusGoogle Scholar[38]Z. Zhang, A. Ji, L. Zhang, Y. Xu, Q. ZhouDeep learning for large-scale point cloud segmentation in tunnels considering causal inferenceAutomat. Construct., 152 (2023), Article 104915,10.1016/j.autcon.2023.104915View PDFView articleView in ScopusGoogle Scholar[39]B. Wang, Q. Wang, J.C. Cheng, C. Song, C. YinVision-assisted BIM reconstruction from 3D LiDAR point clouds for MEP scenesAutomat. Construct., 133 (2022), Article 103997,10.1016/j.autcon.2021.103997View PDFView articleView in ScopusGoogle Scholar[40]A. Ji, A.W.Z. Chew, X. Xue, L. ZhangAn encoder-decoder deep learning method for multi-class object segmentation from 3D tunnel point cloudsAutomat. Construct., 137 (2022), Article 104187,10.1016/j.autcon.2022.104187View PDFView articleView in ScopusGoogle Scholar[41]S. Li, H.J. LiRegional-to-local point-voxel transformer for large-scale indoor 3D point cloud semantic segmentationRemote Sens. (Basel), 15 (2023),10.3390/rs15194832Google Scholar[42]H. Hao, Y. Jincheng, Y. Ling, C. Gengyuan, Z. Sumin, Z. HuanAn improved PointNet++ point cloud segmentation model applied to automatic measurement method of pig body sizeComput. Electron. Agric., 205 (2023), Article 107560,10.1016/j.compag.2022.107560View PDFView articleView in ScopusGoogle Scholar[43]C.R. Qi, H. Su, K. Mo, L.J. GuibasPointnet: deep learning on point sets for 3d classification and segmentationProc. IEEE Conf. Comput. Vis. Pattern Recognit. (2017), pp. 652-660,10.1109/CVPR.2017.16Google Scholar[44]G. Qian, Y. Li, H. Peng, J. Mai, H. Hammoud, M. Elhoseiny, B. GhanemPointnext: revisiting pointnet++ with improved training and scaling strategiesAdv. Neural Inf. Proces. Syst., 35 (2022), pp. 23192-23204arXiv:2206.04670Google Scholar[45]X. Deng, W. Zhang, Q. Ding, X. ZhangPointVector: a vector representation in point cloud analysisProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023), pp. 9455-9465,10.1109/CVPR52729.2023.00912View in ScopusGoogle Scholar[46]Y. Wang, Y.B. Sun, Z.W. Liu, S.E. Sarma, M.M. Bronstein, J.M. SolomonDynamic graph CNN for learning on point cloudsACM Trans. Graph., 38 (2019),10.1145/3326362Google Scholar[47]Y. Li, R. Bu, M. Sun, W. Wu, X. Di, B. ChenPointcnn: convolution on x-transformed pointsAdv. Neural Inf. Proces. Syst., 31 (2018)arXiv:1801.07791Google Scholar[48]Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. MarkhamRandla-net: efficient semantic segmentation of large-scale point cloudsProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020), pp. 11108-11117,10.1109/CVPR42600.2020.01112Google Scholar[49]H. Zhao, L. Jiang, J. Jia, P.H. Torr, V. KoltunPoint transformerProceedings of the IEEE/CVF International Conference on Computer Vision (2021), pp. 16259-16268,10.1109/ICCV48922.2021.01595Google Scholar[50]X. Wu, L. Jiang, P.S. Wang, Z. Liu, X. Liu, Y. Qiao, W. Ouyang, T. He, H. ZhaoPoint Transformer V3: Simpler, Faster, Strongerhttp://dx.doi.org/10.1109/CVPR52733.2024.00463Google Scholar[51]H. Thomas, C.R. Qi, J.E. Deschaud, B. Marcotegui, F. Goulette, L.J. Guibas, IeeeKPConv: flexible and deformable convolution for point cloudsIEEE/CVF International Conference on Computer Vision (ICCV)Seoul, South Korea (2019), pp. 6420-6429,10.1109/iccv.2019.00651Google Scholar[52]M. Xu, R. Ding, H. Zhao, X. QiPAConv: position adaptive convolution with dynamic kernel assembling on point clouds2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021), pp. 3172-3181,10.1109/CVPR46437.2021.00319View in ScopusGoogle Scholar[53]M. Mahmoud, W. Chen, Y. Yang, Y.X. LiAutomated BIM generation for large-scale indoor complex environments based on deep learningAutomat. Construct., 162 (2024), Article 105376,10.1016/j.autcon.2024.105376View PDFView articleView in ScopusGoogle Scholar[54]L. Li, J. Chen, X. Su, H. Han, C. FanDeep learning network for indoor point cloud semantic segmentation with transferabilityAutomat. Construct., 168 (2024), Article 105806,10.1016/j.autcon.2024.105806View PDFView articleGoogle Scholar[55]Y.X. Zhou, A.K. Ji, L.M. Zhang, X.L. XueAttention-enhanced sampling point cloud network (ASPCNet) for efficient 3D tunnel semantic segmentationAutomat. Construct., 146 (2023),10.1016/j.autcon.2022.104667Google Scholar[56]R. Girshick, J. Donahue, T. Darrell, J. MalikRegion-based convolutional networks for accurate object detection and segmentationIEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 142-158,10.1109/TPAMI.2015.2437384View in ScopusGoogle Scholar[57]S. Ren, K. He, R. Girshick, J. SunFaster R-CNN: towards real-time object detection with region proposal networksIEEE Trans. Pattern Anal. Mach. Intell., 39 (2017), pp. 1137-1149,10.1109/TPAMI.2016.2577031Google Scholar[58]Z. Cai, N. VasconcelosCascade R-CNN: delving into high quality object detection2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), pp. 6154-6162,10.1109/CVPR.2018.00644View in ScopusGoogle Scholar[59]A Wang, H Chen, L Liu, Yolov10,et al.Real-time end-to-end object detection[J]Advances in Neural Information Processing Systems, 37 (2024), pp. 107984-108011Google Scholar[60]R. Varghese, M. SYOLOv8: a novel object detection algorithm with enhanced performance and robustness2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS) (2024), pp. 1-6,10.1109/ADICS58448.2024.10533619Google Scholar[61]Wang, CY., Yeh, IH., Mark Liao, HY. (2025). YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information. In: Leonardis, A., Ricci, E., Roth, S., Russakovsky, O., Sattler, T., Varol, G. (eds) Computer Vision – ECCV 2024. ECCV 2024. Lecture Notes in Computer Science, vol 15089. Springer, Cham.https://doi.org/10.1007/978-3-031-72751-1_1.Google Scholar[62]T. Zou, S. Qu, Z. Li, A. Knoll, L. He, G. Chen, C. JiangHGL: hierarchical geometry learning for test-time adaptation in 3D point cloud segmentationA. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, G. Varol (Eds.), Computer Vision – ECCV 2024, Springer Nature Switzerland, Cham (2025), pp. 19-36,10.1007/978-3-031-73001-6_2View in ScopusGoogle Scholar[63]Wu, X., Lao, Y., Jiang, L., Liu, X., and Zhao, H. (2022). "Point transformer V2: grouped vector attention and partition-based pooling."Proceedings of the 36th International Conference on Neural Information Processing Systems, Curran Associates Inc., New Orleans, LA, USA, Article 2415.Google Scholar[64]F. Wulff, B. Schäufele, O. Sawade, D. Becker, B. Henke, I. RaduschEarly fusion of camera and Lidar for robust road detection based on U-net FCN2018 IEEE Intelligent Vehicles Symposium (IV) (2018), pp. 1426-1431,10.1109/IVS.2018.8500549View in ScopusGoogle Scholar[65]K.Q. Zhou, D.P. Ming, X.W. Lv, J. Fang, M. WangCNN-based land cover classification combining stratified segmentation and fusion of point cloud and very high-spatial resolution remote sensing image dataRemote Sens. (Basel), 11 (2019),10.3390/rs11172065Google Scholar[66]S. Gu, T. Lu, Y. Zhang, J.M. Alvarez, J. Yang, H. Kong3-D LiDAR + monocular camera: an inverse-depth-induced fusion framework for urban road detectionIEEE Trans. Intel. Vehicles, 3 (2018), pp. 351-360,10.1109/TIV.2018.2843170View in ScopusGoogle Scholar[67]G. Narita, T. Seno, T. Ishikawa, Y. KajiPanopticFusion: online volumetric semantic mapping at the level of stuff and things2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (2019), pp. 4205-4212,10.1109/IROS40897.2019.8967890View in ScopusGoogle Scholar[68]H. Zhao, L. Jiang, C.W. Fu, J. JiaPointWeb: enhancing local neighborhood features for point cloud processing2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019), pp. 5560-5568,10.1109/CVPR.2019.00571View in ScopusGoogle Scholar[69]G. Qian, H. Hammoud, G. Li, A. Thabet, B. GhanemAssanet: An anisotropic separable set abstraction for efficient point cloud representation learningJ. Adv. Neural Inform. Process. Syst., 34 (2021), pp. 28119-28130,10.48550/arXiv.2110.10538View in ScopusGoogle Scholar[70]T.Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, C.L. ZitnickMicrosoft COCO: common objects in context13th European Conference on Computer Vision (ECCV) Zurich, Switzerland (2014), pp. 740-755,10.1007/978-3-319-10602-1_48View in ScopusGoogle Scholar

#### Enhancing semantic segmentation of MEP scenes with deep learning and BIM-generated synthetic point clouds

Enhancing semantic segmentation of MEP scenes with deep learning and BIM-generated synthetic point clouds
Enhancing semantic segmentation of MEP scenes with deep learning and BIM-generated synthetic point clouds
2025, Advanced Engineering Informatics
Semantic segmentation of point clouds with deep learning (DL) heavily relies on large datasets for training. However, there is a significant scarcity of datasets for MEP scenes. To address this gap, this study proposes the ray-based laser scanning and intersection algorithm (RBLSIA) for automatically generating synthetic point clouds for MEP from BIM. Totally 26 groups of comparison experiments were conducted to investigate the semantic segmentation performance with different algorithms, different generation approaches for synthetic point clouds, and different training datasets. The results show that: 1) the mean Intersection over Union with synthetic point clouds produced by the RBLSIA method is on average 3.32% higher than the uniform sampling method; 2) supplementing real point clouds with synthetic ones can improve Overall Accuracy by an average of 7.70%; 3) using synthetic point clouds to replace real ones can reduce required time for preparing training data by over 90% while achieving similar performance.
Semantic segmentation of point clouds with deep learning (DL) heavily relies on large datasets for training. However, there is a significant scarcity of datasets for MEP scenes. To address this gap, this study proposes the ray-based laser scanning and intersection algorithm (RBLSIA) for automatically generating synthetic point clouds for MEP from BIM. Totally 26 groups of comparison experiments were conducted to investigate the semantic segmentation performance with different algorithms, different generation approaches for synthetic point clouds, and different training datasets. The results show that: 1) the mean Intersection over Union with synthetic point clouds produced by the RBLSIA method is on average 3.32% higher than the uniform sampling method; 2) supplementing real point clouds with synthetic ones can improve Overall Accuracy by an average of 7.70%; 3) using synthetic point clouds to replace real ones can reduce required time for preparing training data by over 90% while achieving similar performance.
Semantic segmentation of point clouds with deep learning (DL) heavily relies on large datasets for training. However, there is a significant scarcity of datasets for MEP scenes. To address this gap, this study proposes the ray-based laser scanning and intersection algorithm (RBLSIA) for automatically generating synthetic point clouds for MEP from BIM. Totally 26 groups of comparison experiments were conducted to investigate the semantic segmentation performance with different algorithms, different generation approaches for synthetic point clouds, and different training datasets. The results show that: 1) the mean Intersection over Union with synthetic point clouds produced by the RBLSIA method is on average 3.32% higher than the uniform sampling method; 2) supplementing real point clouds with synthetic ones can improve Overall Accuracy by an average of 7.70%; 3) using synthetic point clouds to replace real ones can reduce required time for preparing training data by over 90% while achieving similar performance.
Semantic segmentation of point clouds with deep learning (DL) heavily relies on large datasets for training. However, there is a significant scarcity of datasets for MEP scenes. To address this gap, this study proposes the ray-based laser scanning and intersection algorithm (RBLSIA) for automatically generating synthetic point clouds for MEP from BIM. Totally 26 groups of comparison experiments were conducted to investigate the semantic segmentation performance with different algorithms, different generation approaches for synthetic point clouds, and different training datasets. The results show that: 1) the mean Intersection over Union with synthetic point clouds produced by the RBLSIA method is on average 3.32% higher than the uniform sampling method; 2) supplementing real point clouds with synthetic ones can improve Overall Accuracy by an average of 7.70%; 3) using synthetic point clouds to replace real ones can reduce required time for preparing training data by over 90% while achieving similar performance.
A point cloud dataset and deep learning method for semantic segmentation of underground garages2025, Computer Aided Civil and Infrastructure Engineering

#### A point cloud dataset and deep learning method for semantic segmentation of underground garages

A point cloud dataset and deep learning method for semantic segmentation of underground garages
A point cloud dataset and deep learning method for semantic segmentation of underground garages
2025, Computer Aided Civil and Infrastructure Engineering
© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.
© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.
Component-based BIM-semantic web integration for enhanced robotic visual perceptionAutomation in Construction, Volume 177, 2025, Article 106270AiyuZhu, …,HongZhangPredicting the performance of slurry TBM through marine deposits using machine learning modelsAutomation in Construction, Volume 177, 2025, Article 106308SahilWani, …,Ramesh KannanKandasamiGenerative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalanceAutomation in Construction, Volume 177, 2025, Article 106287EmmanouilPanagiotou, …,EiriniNtoutsiView PDFSmart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in constructionAutomation in Construction, Volume 178, 2025, Article 106431MohamedElrifaee,TarekZayedUnified residential floor plan generation with multimodal inputsAutomation in Construction, Volume 178, 2025, Article 106408PengyuZeng, …,ShuaiLuCost-effective optimization system for automated asphalt pavement maintenanceAutomation in Construction, Volume 177, 2025, Article 106333ZhouZidong, …,LiuWeizhengShow 3 more articlesCitationsCitation Indexes1CapturesMendeley Readers3View details
Component-based BIM-semantic web integration for enhanced robotic visual perceptionAutomation in Construction, Volume 177, 2025, Article 106270AiyuZhu, …,HongZhangPredicting the performance of slurry TBM through marine deposits using machine learning modelsAutomation in Construction, Volume 177, 2025, Article 106308SahilWani, …,Ramesh KannanKandasamiGenerative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalanceAutomation in Construction, Volume 177, 2025, Article 106287EmmanouilPanagiotou, …,EiriniNtoutsiView PDFSmart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in constructionAutomation in Construction, Volume 178, 2025, Article 106431MohamedElrifaee,TarekZayedUnified residential floor plan generation with multimodal inputsAutomation in Construction, Volume 178, 2025, Article 106408PengyuZeng, …,ShuaiLuCost-effective optimization system for automated asphalt pavement maintenanceAutomation in Construction, Volume 177, 2025, Article 106333ZhouZidong, …,LiuWeizhengShow 3 more articles
Component-based BIM-semantic web integration for enhanced robotic visual perceptionAutomation in Construction, Volume 177, 2025, Article 106270AiyuZhu, …,HongZhangPredicting the performance of slurry TBM through marine deposits using machine learning modelsAutomation in Construction, Volume 177, 2025, Article 106308SahilWani, …,Ramesh KannanKandasamiGenerative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalanceAutomation in Construction, Volume 177, 2025, Article 106287EmmanouilPanagiotou, …,EiriniNtoutsiView PDFSmart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in constructionAutomation in Construction, Volume 178, 2025, Article 106431MohamedElrifaee,TarekZayedUnified residential floor plan generation with multimodal inputsAutomation in Construction, Volume 178, 2025, Article 106408PengyuZeng, …,ShuaiLuCost-effective optimization system for automated asphalt pavement maintenanceAutomation in Construction, Volume 177, 2025, Article 106333ZhouZidong, …,LiuWeizheng
Component-based BIM-semantic web integration for enhanced robotic visual perceptionAutomation in Construction, Volume 177, 2025, Article 106270AiyuZhu, …,HongZhang

#### Component-based BIM-semantic web integration for enhanced robotic visual perception

Component-based BIM-semantic web integration for enhanced robotic visual perception
Component-based BIM-semantic web integration for enhanced robotic visual perception
Component-based BIM-semantic web integration for enhanced robotic visual perception
Automation in Construction, Volume 177, 2025, Article 106270
Automation in Construction, Volume 177, 2025, Article 106270
Predicting the performance of slurry TBM through marine deposits using machine learning modelsAutomation in Construction, Volume 177, 2025, Article 106308SahilWani, …,Ramesh KannanKandasami

#### Predicting the performance of slurry TBM through marine deposits using machine learning models

Predicting the performance of slurry TBM through marine deposits using machine learning models
Predicting the performance of slurry TBM through marine deposits using machine learning models
Predicting the performance of slurry TBM through marine deposits using machine learning models
Automation in Construction, Volume 177, 2025, Article 106308
Automation in Construction, Volume 177, 2025, Article 106308
SahilWani, …,Ramesh KannanKandasami
Generative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalanceAutomation in Construction, Volume 177, 2025, Article 106287EmmanouilPanagiotou, …,EiriniNtoutsi

#### Generative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalance

Generative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalance
Generative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalance
Generative AI-augmented offshore jacket design: Integrated approach for mixed tabular data generation under scarcity and imbalance
Automation in Construction, Volume 177, 2025, Article 106287
Automation in Construction, Volume 177, 2025, Article 106287
EmmanouilPanagiotou, …,EiriniNtoutsi
Smart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in constructionAutomation in Construction, Volume 178, 2025, Article 106431MohamedElrifaee,TarekZayed

#### Smart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in construction

Smart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in construction
Smart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in construction
Smart IoT-BIM framework with modified zonal safety analysis (ZSA) for real-time safety monitoring in construction
Automation in Construction, Volume 178, 2025, Article 106431
Automation in Construction, Volume 178, 2025, Article 106431
MohamedElrifaee,TarekZayed
Unified residential floor plan generation with multimodal inputsAutomation in Construction, Volume 178, 2025, Article 106408PengyuZeng, …,ShuaiLu

#### Unified residential floor plan generation with multimodal inputs

Unified residential floor plan generation with multimodal inputs
Unified residential floor plan generation with multimodal inputs
Unified residential floor plan generation with multimodal inputs
Automation in Construction, Volume 178, 2025, Article 106408
Automation in Construction, Volume 178, 2025, Article 106408
PengyuZeng, …,ShuaiLu
Cost-effective optimization system for automated asphalt pavement maintenanceAutomation in Construction, Volume 177, 2025, Article 106333ZhouZidong, …,LiuWeizheng

#### Cost-effective optimization system for automated asphalt pavement maintenance

Cost-effective optimization system for automated asphalt pavement maintenance
Cost-effective optimization system for automated asphalt pavement maintenance
Cost-effective optimization system for automated asphalt pavement maintenance
Automation in Construction, Volume 177, 2025, Article 106333
Automation in Construction, Volume 177, 2025, Article 106333
ZhouZidong, …,LiuWeizheng
CitationsCitation Indexes1CapturesMendeley Readers3View details
CitationsCitation Indexes1CapturesMendeley Readers3View details

#### Citations

#### Captures

Cookie Preference CenterWe use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see ourCookie Policyand the list ofGoogle Ad-Tech Vendors.You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.Allow allManage Consent PreferencesStrictly Necessary CookiesAlways activeThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.Cookie Details List‎Performance CookiesPerformance CookiesThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.Cookie Details List‎Contextual Advertising CookiesContextual Advertising CookiesThese cookies are used for properly showing banner advertisements on our site and associated functions such as limiting the number of times ads are shown to each user.Cookie Details List‎Back ButtonCookie ListSearch IconFilter IconClearcheckbox labellabelApplyCancelConsentLeg.Interestcheckbox labellabelcheckbox labellabelcheckbox labellabelConfirm my choices
Cookie Preference CenterWe use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see ourCookie Policyand the list ofGoogle Ad-Tech Vendors.You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.Allow allManage Consent PreferencesStrictly Necessary CookiesAlways activeThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.Cookie Details List‎Performance CookiesPerformance CookiesThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.Cookie Details List‎Contextual Advertising CookiesContextual Advertising CookiesThese cookies are used for properly showing banner advertisements on our site and associated functions such as limiting the number of times ads are shown to each user.Cookie Details List‎Back ButtonCookie ListSearch IconFilter IconClearcheckbox labellabelApplyCancelConsentLeg.Interestcheckbox labellabelcheckbox labellabelcheckbox labellabelConfirm my choices
Cookie Preference CenterWe use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see ourCookie Policyand the list ofGoogle Ad-Tech Vendors.You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.Allow allManage Consent PreferencesStrictly Necessary CookiesAlways activeThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.Cookie Details List‎Performance CookiesPerformance CookiesThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.Cookie Details List‎Contextual Advertising CookiesContextual Advertising CookiesThese cookies are used for properly showing banner advertisements on our site and associated functions such as limiting the number of times ads are shown to each user.Cookie Details List‎Back ButtonCookie ListSearch IconFilter IconClearcheckbox labellabelApplyCancelConsentLeg.Interestcheckbox labellabelcheckbox labellabelcheckbox labellabelConfirm my choices
Cookie Preference CenterWe use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see ourCookie Policyand the list ofGoogle Ad-Tech Vendors.You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.Allow allManage Consent PreferencesStrictly Necessary CookiesAlways activeThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.Cookie Details List‎Performance CookiesPerformance CookiesThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.Cookie Details List‎Contextual Advertising CookiesContextual Advertising CookiesThese cookies are used for properly showing banner advertisements on our site and associated functions such as limiting the number of times ads are shown to each user.Cookie Details List‎

### Cookie Preference Center

We use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see ourCookie Policyand the list ofGoogle Ad-Tech Vendors.You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.

#### Manage Consent Preferences

Strictly Necessary CookiesAlways activeThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.Cookie Details List‎
Strictly Necessary CookiesAlways active

##### Strictly Necessary Cookies

These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.Cookie Details List‎
These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.
Performance CookiesPerformance CookiesThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.Cookie Details List‎
Performance CookiesPerformance Cookies

##### Performance Cookies

These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.Cookie Details List‎
These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.
Contextual Advertising CookiesContextual Advertising CookiesThese cookies are used for properly showing banner advertisements on our site and associated functions such as limiting the number of times ads are shown to each user.Cookie Details List‎
Contextual Advertising CookiesContextual Advertising Cookies

##### Contextual Advertising Cookies

Contextual Advertising Cookies
Contextual Advertising Cookies
These cookies are used for properly showing banner advertisements on our site and associated functions such as limiting the number of times ads are shown to each user.Cookie Details List‎
These cookies are used for properly showing banner advertisements on our site and associated functions such as limiting the number of times ads are shown to each user.
Back ButtonCookie ListSearch IconFilter IconClearcheckbox labellabelApplyCancel
Back ButtonCookie List

#### Cookie List

Search IconFilter IconClearcheckbox labellabelApplyCancel
Clearcheckbox labellabelApplyCancel
checkbox labellabelApplyCancel
ConsentLeg.Interestcheckbox labellabelcheckbox labellabelcheckbox labellabel
ConsentLeg.Interestcheckbox labellabelcheckbox labellabelcheckbox labellabel
checkbox labellabelcheckbox labellabelcheckbox labellabel

## References

1. 3D point cloud data processing with machine learning for construction and infrastructure applications: a comprehensive review
2. 3d shapenets: a deep representation for volumetric shapes
3. 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Honolulu, HI (2017), pp. 77-85,10.1109/cvpr.2017.16
4. 2017 IEEE International Conference on Computer Vision (ICCV) (2017), pp. 2980-2988,10.1109/ICCV.2017.322
5. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2020), pp. 797-806,10.1109/CVPRW50498.2020.00109
6. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), pp. 918-927,10.1109/CVPR.2018.00102
7. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021), pp. 3172-3181,10.1109/CVPR46437.2021.00319
8. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), pp. 6154-6162,10.1109/CVPR.2018.00644
9. 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS) (2024), pp. 1-6,10.1109/ADICS58448.2024.10533619
10. 2018 IEEE Intelligent Vehicles Symposium (IV) (2018), pp. 1426-1431,10.1109/IVS.2018.8500549
11. 3-D LiDAR + monocular camera: an inverse-depth-induced fusion framework for urban road detection
12. 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (2019), pp. 4205-4212,10.1109/IROS40897.2019.8967890
13. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019), pp. 5560-5568,10.1109/CVPR.2019.00571
14. 13th European Conference on Computer Vision (ECCV) Zurich, Switzerland (2014), pp. 740-755,10.1007/978-3-319-10602-1_48
